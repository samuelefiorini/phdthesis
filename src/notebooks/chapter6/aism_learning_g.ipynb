{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning `g`\n",
    "\n",
    "Once $f:X\\rightarrow Y$ is learned, let's learn $g:X_t \\rightarrow X_{t+1}$. We are gonna use three models:\n",
    "\n",
    "1. Multi-task Elastic-Net\n",
    "2. Nuclear-Norm minimization\n",
    "3. Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle as pkl\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from fancyimpute import KNN\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.linear_model import MultiTaskElasticNet\n",
    "from minimal.estimators import NNMRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# import distributed.joblib\n",
    "# from sklearn.externals.joblib import parallel_backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading training and test data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1946, 145)\n",
      "(1946, 34)\n",
      "(714, 145)\n",
      "(714, 34)\n"
     ]
    }
   ],
   "source": [
    "data_tr = pd.read_csv('../../data/AISM/vvr_dataset/vvr_training_data.csv', header=0, index_col=0)\n",
    "labels_tr = pd.read_csv('../../data/AISM/vvr_dataset/vvr_training_labels.csv', header=0, index_col=0)\n",
    "print(data_tr.shape)\n",
    "print(labels_tr.shape)\n",
    "\n",
    "data_ts = pd.read_csv('../../data/AISM/vvr_dataset/vvr_test_data.csv', header=0, index_col=0)\n",
    "labels_ts = pd.read_csv('../../data/AISM/vvr_dataset/vvr_test_labels.csv', header=0, index_col=0)\n",
    "print(data_ts.shape)\n",
    "print(labels_ts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing row 1/1946 with 0 missing, elapsed time: 2.504\n",
      "Imputing row 101/1946 with 8 missing, elapsed time: 2.521\n",
      "Imputing row 201/1946 with 2 missing, elapsed time: 2.525\n",
      "Imputing row 301/1946 with 3 missing, elapsed time: 2.529\n",
      "Imputing row 401/1946 with 2 missing, elapsed time: 2.534\n",
      "Imputing row 501/1946 with 0 missing, elapsed time: 2.537\n",
      "Imputing row 601/1946 with 0 missing, elapsed time: 2.541\n",
      "Imputing row 701/1946 with 1 missing, elapsed time: 2.544\n",
      "Imputing row 801/1946 with 0 missing, elapsed time: 2.546\n",
      "Imputing row 901/1946 with 2 missing, elapsed time: 2.550\n",
      "Imputing row 1001/1946 with 0 missing, elapsed time: 2.554\n",
      "Imputing row 1101/1946 with 3 missing, elapsed time: 2.556\n",
      "Imputing row 1201/1946 with 1 missing, elapsed time: 2.559\n",
      "Imputing row 1301/1946 with 0 missing, elapsed time: 2.561\n",
      "Imputing row 1401/1946 with 0 missing, elapsed time: 2.565\n",
      "Imputing row 1501/1946 with 2 missing, elapsed time: 2.569\n",
      "Imputing row 1601/1946 with 4 missing, elapsed time: 2.573\n",
      "Imputing row 1701/1946 with 0 missing, elapsed time: 2.575\n",
      "Imputing row 1801/1946 with 1 missing, elapsed time: 2.578\n",
      "Imputing row 1901/1946 with 0 missing, elapsed time: 2.581\n",
      "Imputing row 1/1946 with 0 missing, elapsed time: 0.842\n",
      "Imputing row 101/1946 with 1 missing, elapsed time: 0.844\n",
      "Imputing row 201/1946 with 0 missing, elapsed time: 0.846\n",
      "Imputing row 301/1946 with 0 missing, elapsed time: 0.847\n",
      "Imputing row 401/1946 with 1 missing, elapsed time: 0.849\n",
      "Imputing row 501/1946 with 0 missing, elapsed time: 0.850\n",
      "Imputing row 601/1946 with 0 missing, elapsed time: 0.852\n",
      "Imputing row 701/1946 with 0 missing, elapsed time: 0.854\n",
      "Imputing row 801/1946 with 0 missing, elapsed time: 0.855\n",
      "Imputing row 901/1946 with 0 missing, elapsed time: 0.857\n",
      "Imputing row 1001/1946 with 0 missing, elapsed time: 0.858\n",
      "Imputing row 1101/1946 with 0 missing, elapsed time: 0.859\n",
      "Imputing row 1201/1946 with 2 missing, elapsed time: 0.860\n",
      "Imputing row 1301/1946 with 0 missing, elapsed time: 0.862\n",
      "Imputing row 1401/1946 with 0 missing, elapsed time: 0.864\n",
      "Imputing row 1501/1946 with 0 missing, elapsed time: 0.865\n",
      "Imputing row 1601/1946 with 1 missing, elapsed time: 0.867\n",
      "Imputing row 1701/1946 with 0 missing, elapsed time: 0.868\n",
      "Imputing row 1801/1946 with 1 missing, elapsed time: 0.869\n",
      "Imputing row 1901/1946 with 0 missing, elapsed time: 0.870\n",
      "Imputing row 1/714 with 0 missing, elapsed time: 0.306\n",
      "Imputing row 101/714 with 4 missing, elapsed time: 0.309\n",
      "Imputing row 201/714 with 0 missing, elapsed time: 0.311\n",
      "Imputing row 301/714 with 27 missing, elapsed time: 0.312\n",
      "Imputing row 401/714 with 4 missing, elapsed time: 0.315\n",
      "Imputing row 501/714 with 2 missing, elapsed time: 0.319\n",
      "Imputing row 601/714 with 0 missing, elapsed time: 0.321\n",
      "Imputing row 701/714 with 0 missing, elapsed time: 0.323\n",
      "Imputing row 1/714 with 0 missing, elapsed time: 0.115\n",
      "Imputing row 101/714 with 0 missing, elapsed time: 0.116\n",
      "Imputing row 201/714 with 0 missing, elapsed time: 0.117\n",
      "Imputing row 301/714 with 1 missing, elapsed time: 0.119\n",
      "Imputing row 401/714 with 0 missing, elapsed time: 0.120\n",
      "Imputing row 501/714 with 1 missing, elapsed time: 0.121\n",
      "Imputing row 601/714 with 0 missing, elapsed time: 0.122\n",
      "Imputing row 701/714 with 0 missing, elapsed time: 0.124\n"
     ]
    }
   ],
   "source": [
    "# Impute tr data\n",
    "data_tr = pd.DataFrame(data=KNN(k=3).complete(data_tr.values), index=data_tr.index, columns=data_tr.columns)\n",
    "labels_tr = pd.DataFrame(data=KNN(k=3).complete(labels_tr.values), index=labels_tr.index, columns=labels_tr.columns)\n",
    "\n",
    "# Impute ts data\n",
    "data_ts = pd.DataFrame(data=KNN(k=3).complete(data_ts.values), index=data_ts.index, columns=data_ts.columns)\n",
    "labels_ts = pd.DataFrame(data=KNN(k=3).complete(labels_ts.values), index=labels_ts.index, columns=labels_ts.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pp = MinMaxScaler(feature_range=(0, 1))\n",
    "X_tr = pp.fit_transform(data_tr.values)\n",
    "X_ts = pp.transform(data_ts.values)\n",
    "\n",
    "# Impute labels\n",
    "pp = MinMaxScaler(feature_range=(0, 1))\n",
    "Y_tr = pp.fit_transform(labels_tr.values)\n",
    "Y_ts = pp.transform(labels_ts.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dumpt the results into a pkl file\n",
    "with open('__vvrdata.pkl', 'wb') as f:\n",
    "    pkl.dump({'X_tr': X_tr, 'Y_tr': Y_tr, 'X_ts': X_ts, 'Y_ts': Y_ts},f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define pipeline and model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = {                         \n",
    "          'MLP': {'model': MLPRegressor(),\n",
    "                  'params': {'hidden_layer_sizes': [[2**i] for i in range(10, 16)],\n",
    "                             'alpha': np.logspace(-5, 1, 10),\n",
    "                             'early_stopping': [True]}},\n",
    "           \n",
    "          'ENET': {'model': MultiTaskElasticNet(),\n",
    "                   'params': {'l1_ratio': np.linspace(1e-3, 1, 20),\n",
    "                              'alpha': np.logspace(-3, 2, 20)}},\n",
    "    \n",
    "          'NNM': {'model': NNMRegressor(),\n",
    "                  'params': {'alpha': np.logspace(-5, 1, 20)}},\n",
    "    }\n",
    "\n",
    "def modelCV(model):\n",
    "    return GridSearchCV(models[model]['model'], param_grid=models[model]['params'], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Models competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running NNM ...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean abs err: 0.101\n"
     ]
    }
   ],
   "source": [
    "# for model in models:\n",
    "model = 'NNM'\n",
    "\n",
    "print('Running {} ...'.format(model))\n",
    "pipe = modelCV(model)\n",
    "\n",
    "pipe.fit(X_tr, Y_tr)\n",
    "\n",
    "# Save it\n",
    "with open(model+'VVR_estimator.pkl', 'wb') as dd:\n",
    "    pkl.dump(pipe, dd)\n",
    "\n",
    "# Measure scores\n",
    "Y_pred = pipe.predict(X_ts)\n",
    "_err = metrics.mean_absolute_error(Y_ts, Y_pred)\n",
    "print('Mean abs err: {:2.3f}'.format(_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MLP] Mean abs err: 0.107\n",
      "[NNM] Mean abs err: 0.101\n",
      "[ENET] Mean abs err: 0.099\n"
     ]
    }
   ],
   "source": [
    "# Score containers\n",
    "errors = {}\n",
    "\n",
    "for model in models:\n",
    "    with open(model+'VVR_estimator.pkl', 'rb') as f:\n",
    "        mdl = pkl.load(f)\n",
    "    \n",
    "    # Measure scores\n",
    "    Y_pred = mdl.predict(X_ts)\n",
    "    errors[model] = metrics.mean_absolute_error(Y_ts, Y_pred)\n",
    "    print('[{}] Mean abs err: {:2.3f}'.format(model, errors[model]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
