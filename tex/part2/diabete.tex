% !TEX root = ../main.tex

\chapter{Data-driven strategies for robust forecast of continuous glucose monitoring time-series} \label{chap:diabete}
% EMBC

\begin{displayquote}
\textit{Over the past decade, continuous glucose monitoring (CGM) has proven to be a very resourceful tool for diabetes management. To date, CGM devices are employed for both retrospective and online applications. Their use allows to better describe the patients' pathology as well as to achieve a better control  of  patients' level of glycemia. The analysis of CGM sensor data makes possible to observe a wide range of metrics, such as the glycemic variability during the day or the amount of time spent below or above certain glycemic thresholds. However, due to the high variability of the glycemic signals among sensors and individuals, CGM data analysis is a non-trivial task. Standard signal filtering solutions fall short when an appropriate model personalization is not applied. State-of-the-art data-driven strategies for online CGM forecasting rely upon the use of recursive filters. Each time a new sample is collected, such models need to adjust their parameters in order to predict the next glycemic level. In this paper we aim at demonstrating that the problem of online CGM forecasting can be successfully tackled by personalized machine learning models, that do not need to recursively update their parameters.}
\end{displayquote}

\section{Diabetes care}
Diabetes is a chronic metabolic disorder affecting nearly $400$ million of individuals worldwide. The number of diabetic patients is increasing and it is expected to reach almost $600$ million in the next future~\cite{guariguata2014global}.
%In the world people with type 2 diabetes are $400$ million and this  number will rise to $600$ million in $2035$~\cite{guariguata2014global}. 
%http://www.who.int/diabetes/global-report/en/
%https://www.cdc.gov/diabetes/statistics/incidence/fig2.htm
According to the {\em World Health Organization}~\cite{world2016global}, the global prevalence of diabetes among adults has nearly doubled in the last few decades, rising from $4.7\%$ in 1980 to $8.5\%$ in 2014.
%and its incidence per year is $6.9$ per $1,000$ individuals~\cite{}.
%In the western population the  prevalence of diabetes is $6$ to $10\%$ .
%The incidence of diabetes type $2$ is $5-7/1000/$year \todo{non sono sicuro di aver capito}.
If not treated correctly, diabetes may cause several permanent complications, such as visual impairment and kidney failure~\cite{world2016global}. 
%, that heavily affect the quality of life of the patients

Hypoglycemia is a severe risk in diabetes therapy. 
The mean incidence of hypoglycemia in patients with type $1$ diabetes (T1D) 
is 1-2 events per week, while severe hypoglicemia occurs 0.1-1.5 episodes per year~\cite{van2016continuous}. 
Moreover, hypoglycemia interferes with the quality of life and increases the risks of cardiovascular events in type $2$ diabetes (T2D) patients~\cite{van2016continuous}. On the other hand, hyperglycemia associates with an increased risk of diabetes complication as well.


The most common glucose monitoring solutions are self blood glucose meters and continuous glucose monitoring systems (CGM). 
CGM devices are minimally-invasive, and can be used in daily life for retrospective or online applications~\cite{vigersky2017role}. 
CGM systems measure interstitial glucose concentration at fixed time intervals, enabling an accurate observation of glycemic variability during the day as well as the ratio of time spent in hypo/hyperglycemia. 
When CGM is employed online, an improvement of the therapy can be achieved by embedding in the system a tool that foresees glucose levels using suitable time-series models trained on past  CGM  data~\cite{sparacino2007glucose}. In this case, alarms can be generated when the glucose concentration exceeds the normal range~\cite{vigersky2017role}.
%thresholds

%Such tools implement algorithms to foresee hypo/hyperglycemic  events that can be obtained  by  generating alerts on  the basis of future glucose level forecast by  using  past  CGM  data  and  suitable time-series  models \cite{sparacino2007glucose}.
%\cite{monnier2016near}
%\todo{
%State-of-the-art in diabetes monitoring is glycemia self assessment via glucose meters~\cite{monnier2016near} NON SONO SICURO DELLA REF},   it is possible to self-manage diabetes with continuous glucose monitoring (CGM) devices. CGM systems measure interstitial glucose concentration at fixed time intervals (e.g., each $5$ minutes). 
%
%CGM enables the observation of glycemic variability during the day as well as the ratio of time spent in hypo/hyperglycemia. 
%
%
%These devices are minimally-invasive, and can be used in daily life, but, to date, they are mainly used for retrospective and online applications~\cite{vigersky2017role}. 
%
%In both cases, CGM enables the observation of glycemic variability during the day as well as the ratio of time spent in hypo/hyperglycemia. 
%
%When CGM is employed online, an improvement of the therapy can be achieved by embedding in the system a tool able to generate alerts when the glucose concentration exceeds the normal range thresholds~\cite{vigersky2017role}, based on past  CGM  data  and  suitable time-series  models \cite{sparacino2007glucose}.

%In many clinical situations


To model patient-specific blood glucose (BG) levels, several approaches integrating various information were proposed~\cite{bunescu2013blood, zecchin2011new}. However, the problem of glycemic level prediction is still challenging, due to the high CGM signal variability among patients and acquisition devices.
In this paper, assuming that CGM signals have structural information on time-changes of BG concentration \cite{sparacino2007glucose, bunescu2013blood}, we explore the performance of a collection of purely data-driven techniques for time-series analysis.

Data-driven approaches, as opposed to models driven by an {\em a-priori} description of the underlying phenomena, are capable of modeling input/output relationships without requiring prior knowledge on the field of use. This family of approaches can be successfully employed to obtain reliable predictions without modeling complex, and possibly not completely understood, environments. Data-driven models are widely applied in  real-world scenarios often employing a moving-window paradigm, i.e., the system keeps track of the last $w$ acquisitions using them to forecast future values.

In this chapter, we focus on two main groups of data-driven methods for online time-series forecasting: {\em recursive filters} and {\em machine learning} models.
The first group includes linear stationary models, such as Autoregressive Moving Average (ARMA), non stationary models, such as Autoregressive Integrated Moving Average (ARIMA) and adaptive filtering techniques, such as the Kalman Filter (KF). These methods are well-established and extensively used, but they require recursive parameters adjustment for every new collected sample \cite{box2015time}. 
The second group comprises regularized kernel methods, such as Kernel Ridge Regression (KRR) and deep learning methods, such as Long Short-Term Memory networks (LSTM). In the recent past, machine learning methods showed very promising results in the context of time-series forecasting \cite{bunescu2013blood, schmidhuber2005evolino}. To achieve a predictive model, they need to {\em learn} the model parameters from a training set. This learning process is done only once and it conveys a model that does not require further parameter adjustments to predict future values. This remarkable advantage makes machine learning models more suitable to be delivered on embedded portable systems.

Our work aims at understanding whether purely data-driven machine learning methods can be successfully employed to forecast glucose values of diabetic patients.  


\section{Methods}

%\todo{aggiustare il tiro qui}
%In this study we aim at comparing four different data-driven models
% for online CGM sensor data {\em forecasting}: {\em Auto Regressive Integrated Moving Average} (ARIMA) , {\em Kalman Filter} (KF), {\em Kernel Ridge Regression} (KRR) and {\em Long Short-Term Memory} (LSTM) {\em Network}.

Given a set of data $\mathcal{S}$, we refer to data-driven models with hyperparameters $\bm{\theta}$ as $\mathcal{M}_\mathcal{S}(\bm{\theta})$.
%Given $y_i(t)$, the time-series associated with the $i$-th subject, for $t=t_k,\dots,t_{k+w}$ we aim at predicting $y_i(t_{k+w}+\Delta T)$, where $\Delta T$ is some prediction horizon.
Given the time-series $y(t)$ we aim at predicting $y(t+\Delta T)$, where $\Delta T$ is some {\em prediction~horizon}.

A well-known issue of CGM sensor data analysis is that signal properties, such as the SNR, may vary among devices and individuals~\cite{facchinetti2010online}. In order to achieve an accurate forecast of the glucose level of a given individual from past samples, any prediction model $\mathcal{M}_\mathcal{S}(\bm{\theta})$ must be {\em personalized}. Such model personalization procedure is two-fold: {\em hyperparameters ($\bm{\theta}$) optimization}, also known as model selection, and {\em parameter estimation}, or model fitting. 
In this work we fixed a common strategy for hyperparameters optimization whilst the actual model fitting procedure was defined differently according to the model. 
%(i.e., $25$ hours at 5 minutes sampling period)(in our dataset, this set lasted up to $6$ days)

\begin{figure}[h!]
	\caption{An example of two glycemic profiles obtained from T1D and T2D patients. The glucose target range is set between 70 mg/dL and 140 mg/dL (dashed lines). The yellow area at the left hand side of the plot is the initial {\em burn-in} interval used for model {\em personalization}.}\label{fig:cgm}
	\centering
	\includegraphics[width=0.8\textwidth]{part2/cgm_values.png}
\end{figure}

\subsection{Experimental Setting}
The hyperparameters optimization strategy can be described as follows. Given $y(t)$ for time points $t=1,\dots,T$, we split the time-series in two chunks: an initial {\em burn-in} of $T' = 300$ CGM observations and the {\em experiment set} made of the subsequent $T - T'$ samples (see Figure~\ref{fig:cgm}). For each model $\mathcal{M}_\mathcal{S}(\bm{\theta})$, and for each subject, we use the burn-in samples to optimize the hyperparameters $\bm{\theta}$ by grid-search. In other words, the best hyperparameters $\bm{\theta}^*$  are chosen in order to minimize an index $J(\bm{\theta})$ estimated on cross-validation (CV) splits and defined differently according to the model. %, hence $\bm{\theta}^* = \argmin{\bm{\theta}} J(\bm{\theta})$.
In the context of time-series, the CV training split consists only of observations occurred before the corresponding validation samples; such CV flavor is sometimes referred to as {\em rolling forecasting origin}~\cite{tashman2000out}. The grid-search CV scheme promotes the identification of models capable of returning robust predictions. %, where the negative influence of the variable SNR is reduced.
Once the best hyperparameters $\bm{\theta}^*$ are identified, the model $\mathcal{M}_\mathcal{S}(\bm{\theta}^*)$ is fitted on the whole burn-in and it is used to perform online forecasting on the data points of the experiment set (i.e., for $t = T'+1, \dots, T)$. For each personalized model $\mathcal{M}_\mathcal{S}(\bm{\theta}^*)$ we calculate the accuracy for prediction horizons $\Delta T$ of $30, 60, \text{and}~90$ minutes. The performance is estimated by  {\em Root Mean Square Error}
\[
%\text{RMSE} = \sqrt{\frac{\sum_{t=T'}^{T-T'-\Delta T}(y(t) - y(t+\Delta T))^2}{n}}
\text{RMSE} = \sqrt{\frac{\sum_t(y(t+\Delta T) - \hat{y}(t+\Delta T))^2}{n}}
\]
and {\em Mean Absolute Error}
\[
%\text{MAE} = \frac{\sum_{t=T'}^{T-T'-\Delta T}|y(t) - y(t+\Delta T)|}{n}
\text{MAE} = \frac{\sum_t|y(t+\Delta T) - \hat{y}(t+\Delta T)|}{n}
\]
assuming $n$ samples in the experiment set.
%for $t = T'+1,\dots,T-\Delta T$.
%\todo{non abbiamo mai detto quando vale la window size (36) come in sparacino2007}
Fixing some notation, we use $w=36$ as the size of the window in moving-window paradigm and we  indicate with $\{\bm{x}_i, y_i\}_{i=1}^N$  the input/output pairs for machine learning model. 
%Machine learning models need to be trained on input/output pairs $\{\bm{x}_i, y_i\}_{i=1}^N$. In this work we employed a moving-window paradigm of size $w=36$. 
Each input $\bm{x}_i$ is a $w$-dimensional vector made of the CGM samples acquired at times $t=t_{i-w},\dots,t_i$, while the corresponding output $y_i$ is the CGM acquisition at time $t_{i+1}$.



%We can split the forecasting strategies adopted in this paper in two groups: recursive filters (ARIMA, KF) and learning machines (KRR, LSTM). The main difference between this two group of approaches is in the online forecasting process. While recursive filters need to correct their parameters for each new coming input samples, the learning machines are fitted only on the burn-in samples. As this second class of methods does not need to perform recursive fitting computations, they are more suitable to be embedded in portable devices. \todo{aggiustare qui e trovargli un posto sensato}


\subsection{Autoregressive Integrated Moving Average}
ARIMA methods can be used to perform linear forecasting of non stationary time-series assuming that its $d$-th difference is a stationary ARMA process  \cite{box2015time}. The output of an ARMA process, with white input noise $u(t)\sim\mathcal{N}(0,\sigma^2)$, can be expressed as in Equation~\eqref{eq:arma}.
\begin{equation} \label{eq:arma}
y(t) = -\sum_{k=1}^pa_ky(t-k)+\sum_{k=0}^qb_ku(t-k)
\end{equation}
The output of an ARMA$(p, q)$ model can be seen as the sum of $p$ autoregressive and $q$ moving average terms. This strategy requires the modeled processes to be stationary. On the other hand, when a time-series can be considered stationary after $d$ differentiations, ARIMA$(p, d, q)$ models can be used. 
This is the case for non stationary time-series that exhibit local stationary behavior. In general,  $(p, d, q)$ are unknown and we will consider them as hyperparameters of the ARIMA model.
Here, the CV index for ARIMA models was defined as $J(\bm{\theta})= \text{AIC}(\mathcal{M}_{\mathcal{S}}(\bm{\theta})) + \bar{\varepsilon}_{\text{cv}}$, where AIC stands for Akaike Information Criterion~ \cite{box2015time} and $\bar{\varepsilon}_{\text{cv}}$ is the  average CV squared error~\cite{everitt2006cambridge}.
We refer to~\cite{box2015time} for a detailed description of ARIMA model fitting.
The application of this class of models to predict CGM sensor data was also explored in \cite{sparacino2007glucose,bunescu2013blood}.

\begin{figure}[tb!]
	\caption{Plots reporting the distributions of LBGI (left) and HBGI (right) for T1D and T2D. Green areas at the bottom of each plot represents low risk of hypo/hyperglycemia events.
		%  LBGI values below 2.5 indicate a low risk of hypoglycemia, while values above 5 correspond to an 
		%  Normal range for LBGI is $2.5-5$ and for HBGI is $10-15$.
	}\label{fig:plots}
	\centering
	\includegraphics[width=0.8\textwidth]{part2/boxes.png}
\end{figure}

\subsection{Kalman Filter}
%\begin{equation}\label{eq:kfx}
%x(t+1) = Fx(t) + w(t)
%\end{equation}
%with measurements $y \in \mathbb{R}^k$,
%\begin{equation}\label{eq:kfy}
%y(t) = Hx(t)+v(t)
%\end{equation}


The KF addresses the problem of estimating the state $\bm{x} \in \mathbb{R}^d$ of a discrete-time process governed by the linear stochastic difference equation
$\bm{x}(t+1) = F\bm{x}(t) + \bm{w}(t)$
with measurements $\bm{y} \in \mathbb{R}^k$,
$\bm{y}(t) = H\bm{x}(t)+\bm{v}(t)$,
where $\bm{w}(t)$ and $\bm{v}(t)$ are independent random variables representing state and measurement noise, respectively \cite{welch1995introduction}. It is usually assumed that $\bm{w}(t)\sim \mathcal{N}(\bm{0}, Q)$ and $\bm{v}(t)\sim \mathcal{N}(\bm{0}, R)$, with both $Q$ and $R$ unknown. In the context of CGM sensor data prediction, we can safely assume that the state space is two-dimensional, hence $x_1(t)=u(t)$, $x_2(t)=u(t-1)$ where the unknown signal $u(t)$ is described {\em a-priori} as an integrated random-walk $u(t)=2u(t-1)-u(t-2)+w(t)$ as in \cite{facchinetti2010online}. Consequently, the state space transition matrix $F$ can be written as $F=[\begin{smallmatrix}  2 & -1  \\  1 & 0\end{smallmatrix}]$,
%\begin{equation}\label{eq:F}
%F=
%\begin{bmatrix}
%  2 & -1  \\
%  1 & 0
% \end{bmatrix}
%\end{equation}
while the measurement vector is $H = [\begin{smallmatrix} 1 & 0 \end{smallmatrix}]$. The process noise covariance is
$Q=
[\begin{smallmatrix}
\lambda^2 & 0  \\
0 & 0
\end{smallmatrix}
]$
%\begin{equation}\label{eq:F}
%Q=
%\begin{bmatrix}
%  \lambda^2 & 0  \\
%  0 & 0
% \end{bmatrix}
%\end{equation}
and the measurement noise covariance is $R = \sigma^2$, as in \cite{facchinetti2010online}. Both $\lambda^2$ and $\sigma^2$ are unknown and we will consider them as hyperparameters of the KF forecasting strategy, while for the CV index we resort to $J(\bm{\theta})= \bar{\varepsilon}_{\text{cv}}$.
The application of KF to predict CGM sensor data was also explored in \cite{facchinetti2010online, knobbe2005extended}.


%\subsection{Kernel Ridge Regression}
%KRR is a generalization of the classical $\ell_2$-penalized linear regression. In KRR, the {\em kernel trick} is used to give the model the ability to learn nonlinear relationship in the data~\cite{shawe2004kernel}. The KRR solution can be computed solving the minimization problem in Equation~\eqref{eq:krr}, where the data fit is measured by means of the {\em square loss} and the $\ell_2$-norm is a {\em regularization penalty} that promotes smooth solutions shrinking the regression coefficients.
%\begin{equation}\label{eq:krr}
%\min_{\bm{\beta}\in\mathbb{R}^w}\frac{1}{N}\sum_{i=1}^N(y_i-\bm{\beta}^T\phi(\bm{x}_i))^2+\lambda\norm{\bm{\beta}}^2
%\end{equation}
%
%%\todo{vane dice: scorciare qui e spiega meglio cosa intendi per x, y ed n}
%%In this context, $\phi(\bm{x})$ is a {\em feature map}, i.e., a function that projects the samples from the input space $\mathcal{X}$ to a new {\em feature} space $\mathcal{F}$ where an inner product $\langle\phi(\bm{x}_i),\phi(\bm{x}_j)\rangle_{\mathcal{F}}$ is defined. The latter corresponds to the evaluation of a {\em kernel function} $\mathcal{K}(\bm{x}_i,\bm{x}_j)=(K)_{ij}$, where $K$ is the so called {\em kernel matrix}.
%In this context, $\phi(\bm{x})$ is a {\em feature map} defining an inner product $\langle\phi(\bm{x}_i),\phi(\bm{x}_j)\rangle= \mathcal{K}(\bm{x}_i,\bm{x}_j)$, where $\mathcal{K}(\cdot,\cdot)$ is a {\em kernel function}. 
%Thanks to the representer theorem \cite{shawe2004kernel}, the solution of the minimization problem of Equation~\eqref{eq:krr} can be formulated as $\hat{\bm{\beta}}^T=\sum_{i=1}^n\bm{x}_i^Tc_i$ where the coefficients $\bm{c}$ depend on the training data~\cite{shawe2004kernel}.
%% and satisfy the following linear system of equations $(K+\lambda n I)\bm{c}=y$, where $I$ is the identity matrix.
%In this paper we rely on two popular kernel functions: the polynomial kernel $\mathcal{K}(\bm{x}_i,\bm{x}_j)=(\bm{x}_i^T\bm{x}_j +1)^d$ and the Gaussian kernel $\mathcal{K}(\bm{x}_i,\bm{x}_j)=\exp{(-\gamma\norm{\bm{x}_i-\bm{x}_j}_2^2)}$. 
%The KRR hyperparameters are: the regularization penalty $\lambda$, the order $d$ of the polynomial kernel, the $\gamma$ parameter of the Gaussian kernel and, to some extent, the kernel function $\mathcal{K}(\cdot,\cdot)$ itself.
%The CV index used for KRR is the negative $R^2$ score, also known as {\em coefficient of determination}~\cite{everitt2006cambridge}.
%The application of kernel regularized methods to predict CGM sensor data was also explored in \cite{bunescu2013blood}.

\section{Data}
%{\color{red}We collected samples from 178 type 1 and type 2 diabetes patients were monitored for about 1 to 7 days in  free living conditions.
%Patients wore the iPro\textsuperscript{\footnotesize \textregistered}2 Professional CGM (Medtronic), which reported glucose values every 5 minutes}.\\
%

%Patients wore the iPro\textsuperscript{\footnotesize \textregistered}2 Professional CGM sensor (Medtronic), which reported glucose values every 5 minutes.


We acquired CGM samples from a group of 148 T1D and T2D patients wearing the iPro\textsuperscript{\footnotesize \textregistered}2 Professional CGM sensor (Medtronic), which reported glucose values every 5 minutes. Patients were monitored for up to 7 days in  free living conditions, keeping track of their treatments. %, as in Figure~\ref{fig:cgm}. 
From this initial cohort, we excluded the 18 individuals which acquisitions lasted for less than 3.5 days as well as the 24 time-series that presented artifacts due to incorrect use of the CGM acquisition device. Hence, our final dataset comprises 106 subjects of which 72 T1D and 34 T2D. 
On average, glycemic variability is relatively high, with $170.7\pm 70.0$ mg/dL for T1D and $158.4\pm 43.6$ mg/dL for T2D. Figure~\ref{fig:cgm} shows two examples, one for each diabetes type.
%For analysis purposes, patients having CGM acquisitions shorter than 3.5 days or presenting heavy artifacts were discarded.
%\subsection{Data Exploration}
For each patient, the risk of hypo/hyperglycemia was determined by computing the Low Blood Glucose Index (LBGI) and the High Blood Glucose Index (HBGI), defined as in~\cite{fabris2016risk}. These two indices are based on the frequency and extent of hypo- and hyper-glycemic episodes. Figure \ref{fig:plots} shows the LBGI and HBGI distributions for T1D and T2D. 

%LBGI and HBGI are summary statistics, extracted from a series of BG data, that increases when the frequency and/or the extent of low BG or high BG readings increases. \todo{cosa facciamo qui?}
% Hence, for each index, we identified three risk categories as in Table~\ref{tab:lbgihbgi}.
%To estimate LBGI, the BG readings are first transformed to obtain BG symmetric, zero-centered ranges~\cite{kovatchev1997symmetrization} as in Equation~\eqref{eq:preproc}. \todo{ma questi numeri?}
%
%\begin{equation}\label{eq:preproc}
%\text{Transformed BG} = 1.509 \cdot \{\left[\log (\text{BG})\right]^{1.084} - 5.381\}
%\end{equation}
%
%\noindent Hence, if $\text{Transformed BG} < 0$ then \mbox{$\text{Risk(BG)} = 10 \cdot \text{Transformed BG}^{2}$} else $\text{Risk(BG)} = 0$. Finally, LBGI is computed as the mean of these risk values across all BG readings.
%: low ($\text{LBGI} < 2.5$), moderate($2.5<\text{LBGI}<5$) and high ($\text{LBGI} > 5$) and
%HBGI is calculated in the same way, if TransformedBG is $>$ 0,
%\begin{equation}
%\text{Risk(BG)} = 10 \times \text{(TransformedBG)}^{2}
%\end{equation}
%otherwise Risk (BG) = 0.\\
%Finally, the HBGI is computed as the mean of these risk values across all BG readings.\\
%low ($\text{HBGI} < 10$), moderate ($10<\text{HBGI}<15$) and high ($\text{HBGI} > 15$).

%\begin{table}[h!]
%\centering
%\caption{My caption} \label{tab:lbgihbgi}
%\begin{tabular}{l|l|l|l}
%     & Low     & Normal       & High  \\ \hline
%LBGI & $< 2.5$ & $\in [2.5, 5]$ & $> 5$ \\ \hline
%HBGI & $<10$   & $\in [10, 15]$ & $>15$ \\ \hline
%\end{tabular}
%\end{table}

%Table \ref{table_dataexpl} shows an overview on a set of meaningful statistics of the continuous glucose monitoring (CGM) dataset divided in all considered periods. Mean, min and max blood BG \todo{non \`e BG} for all patients, in the whole day and in the five different periods. We also reported LBGI and HBGI in terms of percentage of subjects, during the whole day and in the five periods.



%\begin{table*}[h!]
%\caption{A description of the data collected in term of LBGI and HBGI estimated for different periods of the day.}
%\label{table_dataexpl}
%\begin{center}
%\begin{tabular}{lllllll}
%\hline
%&     Breakfast &         Lunch &        Dinner &       Evening &       Sleeping &           Tot \\
%\hline
%BG Mean      &  157.5 (48.0) &  166.6 (53.2) &  167.9 (55.3) &  168.4 (53.5) &   154.4 (52.9) &  161.7 (44.3) \\
%BG Min       &   74.6 (31.6) &   81.0 (41.9) &   79.7 (34.4) &   83.9 (36.5) &    69.8 (31.3) &   56.0 (21.1) \\
%BG Max       &  274.5 (83.5) &  280.7 (83.4) &  279.8 (89.5) &  272.6 (88.8) &  269.6 (100.2) &  321.9 (82.2) \\
%\hline
%LBGI Low      &         78.5\% &         89.0\% &         84.9\% &         83.7\% &          73.3\% &         79.7\% \\
%LBGI Moderate &         10.5\% &          7.6\% &          9.3\% &          7.6\% &          13.4\% &         12.8\% \\
%LBGI High     &         11.0\% &          3.5\% &          5.8\% &          8.7\% &          13.4\% &          7.6\% \\
%\hline
%HBGI Low      &         68.6\% &         66.3\% &         59.9\% &         61.6\% &          65.7\% &         62.8\% \\
% HBGI Moderate &         15.1\% &         17.4\% &         17.4\% &         14.0\% &          15.7\% &         23.3\% \\
%  HBGI High     &         16.3\% &         16.3\% &         22.7\% &         24.4\% &          18.6\% &         14.0\% \\
%\hline
%  \end{tabular}
%\end{center}
%\end{table*}


\section{Results}
Taking into account the available information on treatments, we divided the dataset into four groups, namely T1D with microinfusion pump (32 subjects), T1D with insulin injection (40 subjects), T2D with rapid acting insulin (10 subjects) and T2D with other therapies (24 subjects). For each group, we applied all forecasting models whose performance,
%obtaining predictions similar to the example in Figure~\ref{fig:fit}.
%Results are reported in Table~\ref{tab:errors}.
%\todo{The dataset was divided into two groups The subjects were divided in four groups, according to their diabetes type and the followed therapy.}
%A summary of the forecasting performance of the four methods, 
expressed in term of MAE and RMSE, is presented in Table~\ref{tab:errors}. 
The forecasting errors clearly increase with the prediction horizon as the errors accumulate at each forecasted time step. KRR achieves the most accurate prediction overall and for almost every group of patients, although ARIMA results are comparable. Moreover, its loss of accuracy from the first to the last prediction horizon is lower than for ARIMA.
Figure~\ref{fig:fit} displays an example of time-series forecast obtained with KRR showing the three different prediction errors at 30, 60 and 90 minutes.

%    Only the samples in the left side of the plot are assumed to be available (green line).

\begin{figure}[tb]
	\caption{Online time-series forecasting obtained by KRR model. {\em One-step-ahead prediction} (left): the green solid line shows the available samples; the dashed line represents one-step-ahead predictions that are obtained by applying the model on a moving window of 36 time-points. {\em Open-loop forecast} (right): with passing time, the moving-window incorporates an increasing number of predicted points, accumulating errors; the dashed line represents forecast with a prediction horizon of 90'.
		Absolute prediction errors are evaluated with respect to future measures (red solid line).
		%  The dashed line in the left side of the plot In the right side of the plot we simulate an open-loop forecast where 
		%  Assuming that only the samples in the left side of the plot are available, the figure shows one-step-ahead predictions.
		%  On the right side, forecasts for a prediction horizon of 90 minutes are shown. 
		%  The absolute errors reported are expressed in $mg/dL$.
	}\label{fig:fit}
	\centering
	\includegraphics[width=0.8\textwidth]{part2/cgm_fit.png}
\end{figure}


\section{Conclusions and Future Works}

In this work we compared the performance of two types of data-driven strategies for online CGM forecasting: recursive filters and machine learning models. We also proposed a general procedure for model personalization based on cross-validation with hyperparameters optimization via grid-search.
Finally, we showed that reliable CGM predictions can be obtained with machine learning models that do not need to recursively adjust their parameters along time.
%In the next future we plan to investigate further using more advanced methods comprising deep learning approaches with  and sparse regularization methods.
In the future, to improve the performance on long prediction horizon, we plan to investigate on sparse data representations with deeper architectures or regularized dictionary learning approaches.
\begin{table}[b!]
	\caption{Prediction errors $(\text{std})$ of the forecasting models for increasing prediction horizons overall and on the four groups. {\sl MP} = Microinfusion Pump, {\sl II} = Insulin Injection, {\sl RAI} = Rapid Acting Insulin, {\sl Other} = Other therapies. Bold numbers indicate best result.}
	\label{tab:errors}
		\begin{tabular}{lllllll}
			& \multicolumn{6}{c}{ARIMA}                                              \\ \cline{2-7}
			\multicolumn{1}{l|}{}                           & \multicolumn{3}{c|}{MAE mg/dL}          & \multicolumn{3}{c|}{RMSE mg/dL}         \\ \cline{2-7}
			\multicolumn{1}{l|}{}                           & 30 & 60 & \multicolumn{1}{l|}{90} & 30 & 60 & \multicolumn{1}{l|}{90}  \\ \hline
			\multicolumn{1}{l|}{Overall}                    & 11.5 (5.2) & 25.6 (12.5) & \multicolumn{1}{l|}{37.2 (20.3)} & 16.7 (7.9) & 37.1 (19.3) & \multicolumn{1}{l|}{53.8 (31.8)} \\
			\multicolumn{1}{l|}{T1D - {\sl MP}}   & 13.3 (3.6) & 30.3 (7.4) & \multicolumn{1}{l|}{45.3 (12.5)} & 19.5 (5.4) & 43.9 (10.9) & \multicolumn{1}{l|}{64.9 (18.1)}  \\
			\multicolumn{1}{l|}{T1D - {\sl II}}    & 13.4 (6.3) & 31.0 (16.2) & \multicolumn{1}{l|}{46.1 (27.1)} & 19.7 (10.6) & 45.1 (27.0) & \multicolumn{1}{l|}{66.6 (45.7)}  \\
			\multicolumn{1}{l|}{T2D - {\sl RAI}} & {\bf11.0 (3.3)} & {\bf 23.8 (6.5)} & \multicolumn{1}{l|}{33.2 (8.5)} & 16.2 (4.8) & 34.1 (8.3) & \multicolumn{1}{l|}{46.9 (10.9)}  \\
			\multicolumn{1}{l|}{T2D - {\sl Other}}      & 10.0 (5.1) & 21.4 (10.4) & \multicolumn{1}{l|}{29.2 (12.5)} & 13.6 (6.0) & 29.6 (12.7) & \multicolumn{1}{l|}{41.3 (16.5)}  \\ \hline
			
			\\
			& \multicolumn{6}{c}{KF}                                                \\ \cline{2-7}
			\multicolumn{1}{l|}{}                           & \multicolumn{3}{c|}{MAE mg/dL}          & \multicolumn{3}{c|}{RMSE mg/dL}         \\ \cline{2-7}
			\multicolumn{1}{l|}{}                           & 30 & 60 & \multicolumn{1}{l|}{90} & 30 & 60 & \multicolumn{1}{l|}{90} \\ \hline
			\multicolumn{1}{l|}{Overall}              & 46.8 (23.2) & 50.8(25.4) & \multicolumn{1}{l|}{159.6 (43.5)} & 58.3 (27.7) & 63.1 (30.3) & \multicolumn{1}{l|}{169.9 (47.1)} \\
			\multicolumn{1}{l|}{T1D - {\sl MP}}         & 56.6 (15.9) & 59.5 (15.2) & \multicolumn{1}{l|}{163.5 (26.7)} & 70.5 (19.0) & 74.3 (18.4) & \multicolumn{1}{l|}{177.5 (29.9)} \\
			\multicolumn{1}{l|}{T1D - {\sl II}}    & 59.9 (24.5) & 67.9 (26.7) & \multicolumn{1}{l|}{179.2 (38.9)} & 74.4 (28.7) & 83.5 (31.3) & \multicolumn{1}{l|}{193.8 (40.4)} \\
			\multicolumn{1}{l|}{T2D - {\sl RAI}} & 48.6 (20.4) & 50.9 (20.8) & \multicolumn{1}{l|}{195.2 (63.1)} & 60.4 (22.9) & 63.6 (24.1) & \multicolumn{1}{l|}{204.4 (63.8)} \\
			\multicolumn{1}{l|}{T2D - {\sl Other}}      & 33.0 (12.1) & 35.6 (15.8) & \multicolumn{1}{l|}{145.0 (30.4)} & 41.3 (14.2) & 44.3 (17.7) & \multicolumn{1}{l|}{150.1 (31.8)} \\ \hline
			
			
			\\
			
			& \multicolumn{6}{c}{KRR}       \\ \cline{2-7}
			\multicolumn{1}{l|}{}                           & \multicolumn{3}{c|}{MAE mg/dL}          & \multicolumn{3}{c|}{RMSE mg/dL} \\ \cline{2-7}
			\multicolumn{1}{l|}{}                           & 30 & 60 & \multicolumn{1}{l|}{90} & 30 & 60 & \multicolumn{1}{l|}{90}  \\ \hline
			\multicolumn{1}{l|}{Overall}                    & {\bf 11.1 (4.3)} & {\bf 25.1 (10.4)} & \multicolumn{1}{l|}{{\bf 35.2 (15.4)}} & {\bf 15.5 (6.7)} & {\bf 33.6 (13.8)} & \multicolumn{1}{l|}{{\bf 45.8 (19.7)}} \\
			\multicolumn{1}{l|}{T1D - {\sl MP}}   & {\bf12.9 (3.4)} & {\bf 30.2 (8.4)} & \multicolumn{1}{l|}{{\bf 43.1 (12.6)}} & {\bf17.9 (5.1)} & {\bf 40.2 (10.9)} & \multicolumn{1}{l|}{{\bf 56.0 (16.1)}}  \\
			\multicolumn{1}{l|}{T1D - {\sl II}}    & {\bf13.1 (4.9)} & {\bf 30.3 (10.6)}  & \multicolumn{1}{l|}{{\bf 43.1 (14.5)}} & {\bf18.6 (8.4)} & {\bf 40.5 (14.5)} & \multicolumn{1}{l|}{{\bf 55.8 (18.6)}}  \\
			\multicolumn{1}{l|}{T2D - {\sl RAI}} & 11.2 (2.9) & 24.1 (5.5) & \multicolumn{1}{l|}{{\bf 33.2 (7.1)}} & {\bf16.0 (4.7)} & {\bf 32.7 (7.5)} & \multicolumn{1}{l|}{{\bf 43.7 (9.5)}}  \\
			\multicolumn{1}{l|}{T2D - {\sl Other}}      & {\bf8.7 (2.3)} & {\bf 19.8 (6.9)} & \multicolumn{1}{l|}{{\bf 27.7 (11.8)}} & {\bf11.9 (3.4)} & {\bf 26.5 (8.9)} & \multicolumn{1}{l|}{{\bf 36.2 (14.4)}}  \\ \hline
			
			\\
			
			& \multicolumn{6}{c}{LSTM}                                              \\ \cline{2-7}
			\multicolumn{1}{l|}{}                           & \multicolumn{3}{c|}{MAE mg/dL}          & \multicolumn{3}{c|}{RMSE mg/dL}         \\ \cline{2-7}
			\multicolumn{1}{l|}{}                           & 30 & 60 & \multicolumn{1}{l|}{90} & 30 & 60 & \multicolumn{1}{l|}{90} \\ \hline
			\multicolumn{1}{l|}{Overall}                & 19.9 (10.6) & 44.0 (25.9) & \multicolumn{1}{l|}{61.3 (36.5)} & 26.2 (13.4) & 55.0 (30.0) & \multicolumn{1}{l|}{74.7 (41.3)} \\
			\multicolumn{1}{l|}{T1D -{\sl  MP}}   & 22.9 (9.1) & 51.6 (21.5) & \multicolumn{1}{l|}{71.6 (27.0)} & 30.6 (11.6) & 65.7 (27.0) & \multicolumn{1}{l|}{89.8 (33.5)} \\
			\multicolumn{1}{l|}{T1D - {\sl II}}    & 24.5 (12.9) & 56.5 (33.1) & \multicolumn{1}{l|}{81.4 (45.8)} & 31.2 (14.9) & 68.8 (35.9) & \multicolumn{1}{l|}{97.0 (48.4)} \\
			\multicolumn{1}{l|}{T2D - {\sl RAI}} & 18.3 (6.7) & 39.4 (14.8) & \multicolumn{1}{l|}{54.1 (21.0)} & 25.7 (10.1) & 51.6 (21.7) & \multicolumn{1}{l|}{67.7 (29.1)} \\
			\multicolumn{1}{l|}{T2D - {\sl Other}}      & 16.8 (8.9) & 35.0 (17.7) & \multicolumn{1}{l|}{47.0 (26.5)} & 22.2 (12.1) & 43.4 (19.0) & \multicolumn{1}{l|}{56.5 (27.3)} \\ 
			
			\hline
			
		\end{tabular}
\end{table}
