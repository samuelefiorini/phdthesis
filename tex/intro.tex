% !TEX root = ./main.tex

% \addcontentsline{toc}{chapter}{\nameref{chapter:introduction}}
% cercare soluzione tramite metodi statistici di problemi complessi biomedicali con particolare attenzione ai time-evolving data
% qui no soluzioni ma problemi: domande + esempi
%Value and influence of data in everyday life are becoming paramount. Every action we make, every decision we take or every message we receive generates da
%This is where artificial intelligence
%Understanding the underlying mechanisms of biological systems can be a challenging task. Different domains are involved and their interactions can be unknown.
%Most of the current life science research aims at extracting meaningful information from heterogeneous sources of biological data, that is continuously increasing in size, thanks to the remarkable technological progresses of the last decades.

%\todo{paginetta here}
\chapter{Introduction} \label{chapter:introduction}


The influence of data pervades our everyday lives. We are literally surrounded by data acquisition and processing devices that continuously interact with us.
The watches at our wrists, the phones in our pockets, the house where we live and even the cars we drive, everything is equipped with devices that can automatically collect and process information in order to make data-driven suggestions affecting our daily routines.
This is so pervasive as it often leads to effective, efficient and actionable solutions to real-world problems.

It sounds remarkable already, but there is more. Data collection and processing are not only influencing our daily actions, but also every recent discovery in almost every scientific domains, such as computer science, physics, chemistry, biomedicine and so on.
%that is always supported by data and thorough statistical analysis.
%This applies 

In these fields, data are so valuable, that it is rather common to achieve different discoveries from the same data collection. This lead to the development of a new paradigm that can be expressed by the motto: \textit{collect first, ask questions later}.

However, this comes at a price. 
Managing and maintaining infrastructure for data-intensive applications is expensive in terms of economical and human resources employed. In the last few years, the amount of generated data rapidly became overwhelming and it superseded human analysis and insights potential.


Biomedical data are prototypical in this sense. In this field, almost every medical diagnosis is currently supported by quantitative observations.
When a clinician is asked to analyze the medical records of a patient, he/she needs to deal with a large number of highly heterogeneous measures that can be hard to understand as they can be missing or incomplete and their interaction can be circumstantial or unknown.

In these circumstances, classical model, that are only driven by prior knowledge of the problem, fall short as they may not explain some relevant part of the phenomenon under investigation.

Nowadays, biomedical data are often analyzed with data-driven models. These models are designed to automatically infer structure and relationships hidden in the data and to use them to predict some target measure. For instance, models of this class can be used to assign a patient to a given class (\eg case/control, or the presence of some phenotype) given a number of blood or genetic measures, or they can predict some future disease-related event from historical medical records.

In this thesis, we will see an overview of the data-driven solutions most commonly adopted to solve biomedical challenges. Theoretical notions with practical examples and real case studies will be presented. Great effort will be devoted toward the presentation of statistics and engineering concepts in a unified fashion. The description of each method will balance the trade-off between providing an intuitive grasp of its behavior and its mathematical foundation, presented in a more rigorous way.

This thesis is divided in two parts. Part I presents a thorough description of the multi-disciplinary prerequisites that are relevant for the comprehension of Part II, which describes the original contributions of the work.

Part I is organized as follows: Chapter~\ref{chap:background} introduces the concept of \textit{data science} and its declination toward life science and biomedical studies. In this chapter, the major challenges of the field are presented along with several examples of the most common clinical/biological questions and their translation to data analysis tasks (Section~\ref{sec:clinical_to_data}).

Chapter~\ref{chap:state-of-the-art} summarizes basic notation and definitions adopted throughout the thesis (Section~\ref{sec:notation}) and presents an overview of the statistical and technological tools that are mostly relevant for this work. In particular, this chapter defines the concept of \textit{machine learning} from a general perspective and provides rigorous description of a selection of supervised and unsupervised learning strategies (Sections~\ref{subsec:supervised_learning} and~\ref{subsec:unsupervised_learning}).
This chapter also defines the concept of variable/feature selection (Section~\ref{subsec:feature_selection}) and introduces the most relevant model selection and evaluation strategies (Section~\ref{subsec:model_selection}).
At the end of this chapter, hints on the computational requirements and implementation strategies are presented (Section~\ref{sec:implementation}).

%twofold: the development of an exploratory data analysis tool and
Part II describes the contribution of this work which consisted in the process of devising data-driven strategies to tackle a number of biological data science challenges coming from real-world clinical environments. For each task, this part shows how the previously introduced tools can be exploited in order to develop statistically sound models that are capable of providing insightful answers to different clinical questions.

Part II is organized as follows:
Chapter~\ref{chap:adenine} introduces \ade, an open-source Python framework for large-scale data exploration. %that I developed during my PhD.
The material covered in this chapter is also available as conference proceedings paper~\cite{fiorini2017adenine}.
Chapter~\ref{chap:frassoni} describes the preliminary results of an ongoing work held in collaboration with \textit{Istituto Giannina Gaslini Children's Hospital} (Genoa, IT) on age prediction from molecular biomarkers (paper in preparation).

Chapter~\ref{chap:aism} describes a work held in collaboration with the \textit{Italian Multiple Sclerosis Foundation} (Genoa, IT). This work aims at devising a model to predict the evolution of multiple sclerosis patients exploiting the use of patient-friendly and inexpensive measures such as patient centered outcomes.
Most of the material covered in this chapter is also available as conference proceeding paper~\cite{fiorini2015machine}, and peer-reviewed journal articles~\cite{brichetto2015improving, fiorini2016temporal, brichetto2016predicting, pmlr-v68-fiorini17a, tacchino2017multiple}.

Chapter~\ref{chap:diabete} describes a work held in collaboration with \textit{Ospedale Policlinico San Martino}. In this work a machine learning time-series model is used to forecast future glucose sensor data values. This work is based on data collected by type I and type II diabetic patients.
The material covered in this chapter was recently presented at an international IEEE conference, thus it is available as proceeding paper~\cite{fiorini2017data}.

Conclusions are finally drawn in Chapter~\ref{chap:conclusions}.

Every figure and every experimental result obtained in this thesis can be easily reproduced by using the Python scripts and {\sc jupyter}  notebooks\footnote{ Source: \url{http://jupyter.org} (last visit 2018-01).} available on a dedicated GitHub repository: \url{https://github.com/samuelefiorini/phdthesis}, which also keeps track of the~\LaTeX~source code of the thesis.

\chapter{Basic notation and definitions} \label{sec:notation}
%% the number of variables
%\todo{bold vectors, capital matrices}
In this thesis, the following notation is adopted.

For unsupervised problems, datasets $\mathcal{D}$ are described as collection of samples $X \in \mathbb{R}^{n \times d}$. Whereas, for supervised problems, datasets are described as input-output pairs, $X \in \mathbb{R}^{n \times d}$ and $Y \in \mathbb{R}^{n \times k}$, respectively.
The $i$-th row of $X$ is a $d$-dimensional data point $\bm{x}_{i}$ belonging to the input space $\mathcal{X}\subseteq\mathds{R}^d$. The corresponding outputs $\bm{y}_{i}$ belong to the output space $\mathcal{Y}$.

The nature of the output space defines the problem as \textit{binary classification} if  $\mathcal{Y} = \{a,b\}$ (with $a\neq b$), \textit{multiclass classification} if
$\mathcal{Y} = \{\alpha,\beta,\dots,\omega\}$
(with $\alpha \neq \beta \neq \dots \neq \omega$),
% \textit{multi-label classification} if $\mathcal{Y} \in \mathbb{N}^k$,
\textit{regression} if $\mathcal{Y}\subseteq\mathds{R}$ and
\textit{vector-valued} or \textit{multi-task regression} if $\mathcal{Y}\subseteq\mathds{R}^k$.
For binary classification problems common choices for the label encoding are $a=1, b=-1$ or $a=0, b=1$.
For multiclass classification problems classes are usually encoded as natural numbers, \ie $\alpha, \beta, \dots, \omega \in \mathbb{N}$.

Predictive models are functions $f: \mathcal{X} \rightarrow \mathcal{Y}$.
The number of relevant variables is $d^*$.
In feature selection tasks, the number of selected features is $\tilde d$.

A kernel function acting on the elements of the input space is defined as $\mathcal{K}(\bm{x}_{i},\bm{x}_{j})=\langle \phi(\bm{x}_{i}), \phi(\bm{x}_{j})\rangle$, where $\phi(\bm{x})$ is a {\em feature map} from $\mathds{R}^d \rightarrow \mathds{R}^{d'}$.
In general, feature learning algorithms project the data into a $p$-dimensional space.
%The number of atoms in Dictionary Learning is $p$.

Whenever possible,
real-valued variables are indicated with lowercase letters (\eg $a$),
unidimensional vectors with lowercase bold letters (\eg $\bm{a}$) and
matrices, or tensors, with capital letters (\eg $A$).
When the value of some variable/parameter is the result of a data-driven estimation, such variable will be highlighted with a hat (\eg $\hat a$).
When used in the context of a data matrix, a subscript index will be used to identify a sample (row) whereas a superscript index will refer to a given feature (column).
So, for instance, given a data matrix $X \in \mathbb{R}^{n \times d}$ the $j$-th feature of the $i$-th sample is $x_i^j$, with $0 \leq i \leq n$ and $0\leq j\leq d$.
