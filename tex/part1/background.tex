\chapter{Background} \label{chap:background}
% from regularization in machine learning BIB (chloï¿½)

\section{What is data science and why we should care}
% data science vasta: gestione (data engineering) -> data exploration -> machine learning -> data viz
% DIKW pyramid
\todo{\begin{itemize}
\item{Data engineering}
\item{Data exploration}
\item{Machine learning and data understanding}
\item{Data visualization}
\end{itemize}}

\section{Challenges in biomedical data science}
% pochi sparsi rumorosi bucati

\section{From clinical questions to learning task}
In applied life science, the biological question at hand usually drives the data collection and therefore the statistical challenge to be solved. In order to achieve meaningful results, thorough data analysis protocol must be followed (see Section~\ref{subsec:model_selection}).
%\todo{XX Here we should say that if you do not follow rules in Section 5 all results may be meaningless}
In this section, my goal is to illustrate some of the most recurrent biological questions and how they can be translated into machine learning tasks.


\subsection{How to predict phenotypes from observed data?} % supervised learning
Starting from a collection of input measures that are likely to be related with some known target phenotype, the final goal here is to learn a model that represents the relationship between input and output. Several researches fall in this class, for instance in molecular (\eg lab tests, gene expression, proteomics, sequencing)~\cite{angermueller2016deep, okser2014regularized, abraham2013performance} or radiomics/imaging studies (\eg MRI, PET/SPECT, microscopy)~\cite{min2016deep, helmstaedter2013connectomic}. Biological questions of this class are usually tackled by \textit{supervised learning} models. In particular, when the observed clinical outcome is expressed as a one-dimensional continuous value, as in survival analysis, a \textit{single-output regression} problem is posed. Moreover, if the outcome is vector-valued, as in the case of multiple genetic trait prediction \cite{he2016novel}, the problem can be cast in a \textit{multiple-output regression }framework \cite{argyriou2008convex, baldassarre2012multi}. Biological studies involving categorical outcomes translate into \textit{classification} problems. In particular, if the clinical outcome assumes only two values, as in the \textit{case}-\textit{control} scenario, the classification problem is said to be \textit{binary}, whilst, if multiple classes are observed, the classification task becomes \textit{multi-class}.

\subsection{Which variables are the most significant?} % variable/feature selection
In the above case, a complementary question revolves around the interpretability of the predictive model. In particular, if dealing with high-throughput data, the main goal is to identify a relevant subset of meaningful variables for the observed phenomenon. This problem can be cast into a variable/feature selection problem \cite{guyon2002gene}. %To identify a model that uses only a reduced number of variables is of fundamental use in biology as it enhances its interpretability.
%\todo{group lasso for logistic regression- DNA sequences: \cite{}}

A machine learning model is said to be \textit{sparse} when it only contains a small number of non-zero parameters, with respect to the number of features that can be measured on the objects this model represents~\cite{hastie2015statistical, meier2008group}. This is closely related to feature selection: if these parameters are weights on the features of the model, then only the features with non-zero weights actually enter the model and can be considered \textit{selected}.

\subsection{How to stratify the data?} % clustering
Collecting measures from several samples, the final goal here is to divide them in homogeneous groups, according to some \textit{similarity} criterion. In machine learning, this is usually referred to as \textit{clustering}~\cite{hastie2009elements}.

\subsection{How to represent the samples?} % unsupervised feature learning and dimensionality reduction
In order to formulate a model of some natural phenomenon, it is necessary to design and follow a suitable data collection protocol. A natural question that may arise here is whether the raw collected measures are intrinsically representative of the target phenomenon or if some transformation must be applied in order to achieve a data representation that can be successfully exploited by a learning machine. For instance, it may be plausible to assume that the data lie in a low-dimensional embedding or that they can be better represented by a richer polynomial or Gaussian expansion.
A common solution, in this case, is to take advantage of \textit{feature engineering} techniques to obtain hand crafted features. However, this process can be very time-consuming and it may require the help of domain experts. The process of automatically identify suitable representations from the data itself is usually referred to as \textit{(un)supervised feature learning}~\cite{angermueller2016deep, mamoshina2016applications}.

\subsection{Are there recurring patterns in the data? } % \todo{maybe change title} sparse coding & dictionary learning
Analyzing data coming from complex domains, one may be interested in understanding whether complex observations can be represented by some combination of simpler events. In machine learning this typically translates into \textit{adaptive sparse coding} or \textit{dictionary learning} problems~\cite{masecchia2015genome, alexandrov2013signatures}.
%\todo{add bio example: pathological patterns in tumors from TCGA~\cite{alexandrov2013signatures},
%Neuroblastoma oncogenesis~\cite{masecchia2015genome}}


\subsection{How to deal with missing values?} % imputing
Applied life science studies must often deal with the issue of missing data. For instance, peaks can be missed in mass-spectrometry~\cite{jung2014adaption} or gene expression levels can be impossible to measure due to insufficient array resolution or image corruption~\cite{stekhoven2011missforest, troyanskaya2001missing}. Common strategies, such as discarding the samples with missing entries, or replacing the holes with the mean, median or most represented value, fall short when the missing value rate is high or the number of collected samples is relatively small. In machine learning this task usually translates into a \textit{matrix completion} problem~\cite{candes2009exact}.
