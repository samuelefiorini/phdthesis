% !TEX root = ../main.tex

\chapter{Machine learning state of the art} \label{chap:state-of-the-art}

\begin{displayquote}
	\textit{This chapter defines the concept of machine learning in its two major declinations: supervised and unsupervised. It continues providing a comprehensive overview  of algorithms, models and techniques relevant for the biomedical data science applications described in Part II. At the end of this chapter, an overview on the computational requirements and the most recent machine learning technologies is given.}
\end{displayquote}

The term \textit{Machine Learning} (ML) first appeared in the late 50's in the field of computer science and it is now becoming a buzzword used in several contexts spanning from particle physics and astronomy to medicine and social sciences~\cite{service2017ai}.
With a simple search on Google Trends\footnote{Source \url{https://trends.google.com}.} it is possible to roughly quantify the pervasiveness of this term on the Internet in the last few years. From Figure~\ref{fig:google_trend_ML} we can see that the interest toward both the terms \textit{machine learning} and \textit{data science} is growing, with the first consistently superior to the second.

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.8\textwidth]{part1/google_trends_MLDS.png}
  \caption{The Internet popularity over the past five years of two terms: \textit{data science} and \textit{machine learning}. The vertical axis represents the number of Google searches of an input term normalized with respect to its maximum.} \label{fig:google_trend_ML}
\end{figure}

A partial explanation to this phenomenon can be found in a recent article published on Science~\cite{appenzeller2017revolution}, where the authors observed how the explosion of modern data collection abilities is leading the human kind toward another \textit{scientific revolution}.
Biomedical applications are prototypical in this sense. For instance, the volume of the raw data acquired from a genome sequencer for a single \ac{DNA} has a volume of approximately 140 GB~\cite{marx2013biology}. Another example can be the 3D reconstruction of cardiac MRI acquisition which needs around 20 GB for a single human heart, or the 3D \ac{CT} scan which has a volume in the order of GB for each patient, and so on. It has been estimated that an average hospital currently stores more than 665 TB of data that needs to be analyzed and understood.
%the typical resolution of MRI images is currently $512 \times 512$
Such massive amounts of data have long overwhelmed human analysis and insights potential. This makes ML, and artificial intelligence in general, a key element for clinicians and scientists that try to make sense of large-scale observations.

But, what is \textit{machine learning}? And how does it differ from classical statistics?

A unique answer to this question is not easy to provide. In fact, ML can be defined in different ways and from several standpoints. Let's see three remarkable examples.

\begin{enumerate}
  \item Kevin P. Murphy in its \emph{Machine Learning - A Probabilistic Perspective}~\cite{murphy2012machine} defines ML as follows.

  \begin{displayquote}
  "$[\dots]$ \emph{a set of methods that can automatically detect patterns in data, and then use the uncovered patterns to predict future data, or to perform other kinds of decision making under uncertainty} $[\dots]$"
  \end{displayquote}

  \item Trevor Hastie, a well-known applied statistician, in a famous seminar\footnote{ Part of Data Science @ Stanford Seminar series (source: \url{https://goo.gl/UFgqxU}).}, held in October 2015 at the Stanford University, gave the following three definitions.

  \begin{displayquote}
    \begin{itemize}
      \item[] \emph{{\bf Machine Learning} constructs algorithms that can learn from data.}
      \item[] \emph{{\bf Statistical Learning}  is a branch of applied statistics that emerged in response to machine learning, emphasizing statistical models and assessment of uncertainty.}
      \item[] \emph{{\bf Data Science}  is the extraction of knowledge from data, using ideas from mathematics, statistics, machine learning, computer science, engineering...}
    \end{itemize}
  \end{displayquote}

  \item Carl E. Rasmussen in the preface of its renowned \emph{Gaussian Processes for Machine Learning}~\cite{rasmussen2006gaussian} introduces the difference between classical statistics and ML as follows.

  \begin{displayquote}
    "$[\dots]$ \emph{in statistics a prime focus is often in understanding the data and relationships in terms of models giving approximate summaries such as linear relations or independencies. In contrast, the goals in machine learning are primarily to make predictions as accurately as possible and to understand the behaviour of learning algorithms} $[\dots]$"
  \end{displayquote}

\end{enumerate}

It looks like each author, according to his background, expertise and experience, provides a slightly different definition of ML. Trying to summarize these three standpoints, we can say that \emph{ML is an interdisciplinary field that borrows the concept of data-driven model from statistics in order to devise algorithms that can exploit hidden patterns in current data and make accurate predictions on future data}.

As of today ML is the workhorse of data science.


  \section{Supervised learning} \label{subsec:supervised_learning}

	  Humans are remarkably good at \emph{learning by examples}. When a kid is taught what a pencil looks like, he will be capable of understanding the concept of pencil from a limited number of guided observations. Similarly, when future radiologists are trained to distinguish between healthy tissues from tumors in MRI scans, they will be provided with several annotated biomedical images from which they will be able to generalize.
	  The applied learning paradigm is characterized by the presence of two key objects: \textit{data} and \textit{labels}. In the last example, the MRI scans are the data, and their annotations (\eg tumor \vs healthy tissue) are the labels.

	  Supervised learning is the branch of ML in which predictive models are trained on labeled data. In the ML jargon, and in this thesis, one usually refers to \textit{data} as collections of \textit{samples} described by an arbitrarily large number of \textit{predictors} (\textit{features}) that are used as \textit{input} in a training process having labels as \textit{output}.

	  Input samples throughout this thesis are represented as $d$-dimensional vectors $\bm{x}$ belonging to an input space $\mathcal{X}$, where typically $\mathcal{X}\subseteq\mathbb{R}^d$ and labels are represented with the variable $y$ belonging to an output space $\mathcal{Y}$.
	  The nature of $\mathcal{Y}$ defines the learning task as \textit{binary classification} if  $\mathcal{Y} = \{-1,+1\}$, \textit{multiclass classification} if $\mathcal{Y} = \{1,2,\dots,k\}$,
	  \textit{regression} if $\mathcal{Y}\subseteq\mathds{R}$ or
	  \textit{vector-valued regression} if $\mathcal{Y}\subseteq\mathds{R}^k$.
	%  In the second part of this thesis each of these learning problems will be faced.
	  The remainder of this section summarizes the methods that are most relevant with the data-driven strategies adopted to tackle the biomedical data science challenges described in the second part of in this thesis.

	  Given a set of input-output pairs $\mathcal{D} = \{\bm{x}_i, y_i\}_{i=1}^n = (X, \bm{y})$, supervised learning methods aim at finding a function of the inputs $f(\bm{x})$ that approximates the output $y$. This translates into the minimization problem defined in Equation~\eqref{eq:loss}.

	  \begin{equation}\label{eq:loss}
	    \argmin_{f \in \mathcal{F}} \frac{1}{n}\sum_{i=1}^n L(f(\bm{x}_i),y_i) %+ \lambda R(f)
	  \end{equation}

	  The loss function $L(f(\bm{x}),y)$ can be seen as a measure of \textit{adherence} to the available training data. Several loss functions for regression and classification problems were proposed; Table~\ref{tab:losses} defines the most commonly adopted in biomedical studies while their visual representation is presented in Figure~\ref{fig:loss}.
	  Choosing the appropriate loss function for the problem at hand is crucial and there is no trivial solution for this problem.
	  Different choices for $L(f(\bm{x}),y)$ identifies different learning machines, that are known under different names. The most popular methods will be presented in the next few sections.
	  % \todo{Different choices for the loss function imply different ML model, see the remainder of this section + this is possibly ill-posed}

	  \begin{table}[!h]
	  	\centering
	  	\caption{Definition of the loss functions for regression (top) and classification (bottom) problems represented in Figure~\ref{fig:loss}.}\label{tab:losses}
	  	\begin{tabular}{@{}ll@{}ll@{}}
	  		\toprule
	  		Loss function & $L(f(\bm{x}),y)$  & Learning problem           \\ \midrule
	  		Square                   & $(y - f(\bm{x}))^2$ & regression \\
	  		Absolute                 & $|y - f(\bm{x})|$ & regression   \\
	  		$\varepsilon$-insensitive   & $|y - f(\bm{x})|_\varepsilon$  & regression                 \\
	  		\midrule
	  		Zero-one               & $\mathds{1}\{y=f(\bm{x})\}$ & classification \\
	  		Square                 & $(1 - yf(\bm{x}))^2$ & classification  \\
	  		Logistic                 & $\log(1 + e^{-yf(\bm{x})})$ & classification  \\
	  		Hinge                 & $|1 - yf(\bm{x})|_+$ & classification  \\
	  		Exponential & $e^{- yf(\bm{x})}$ & classification \\
	  		\bottomrule
	  	\end{tabular}
	  \end{table}

	  \begin{figure}[!h]
	  	\centering
	  	\subfloat[]{%
	  		\includegraphics[width=0.8\textwidth]{part1/regression_losses.png}
	  		\label{fig:regression}%
	  	}%
       \hfill%
	  	\subfloat[]{%
	  		\includegraphics[width=0.8\textwidth]{part1/classification_losses.png} \label{fig:classification}
	  	}%
	  	\caption{An overview on the most common loss functions for regression (a) and classification (b) problems plotted against the corresponding prediction error.}\label{fig:loss}
	  \end{figure}

	   % rubare da BIB
	  % In its most classical definition, the aim of modeling is to infer some unknown structure underlying the data.
	  Identifying a reliable data-driven model can be a very tricky task. Many unwanted and concurrent factors may be misleading and the solution may have poor predictive power for several reasons.
		Including:
		\begin{enumerate}
			\item the acquisition devices may introduce random fluctuations in the measures;
			\item the amount of collected samples $n$ may be small with respect to the number of observed variables $d$;
			\item a non-negligible number of the measured variables may not be representative of the target phenomenon.
	  \end{enumerate}
		From a modeling standpoint, every combination of the factors above can be seen as \textit{noise} affecting the data.
		Precautions in the model formulation process must be taken in order to achieve solutions that are insensitive to small changes in the input data and that are, in general, \textit{robust} to the noise effect.

		Considering a ML model ($\hat f$) fitted on a collection of data ($\mathcal{D}$), the most desirable property of $\hat f$ is that it should be able to achieve good prediction performance not only on $\mathcal{D}$, but also on all the future, therefore unseen, data points $\mathcal{D}'$.
		In other words, assuming that the samples in $\mathcal{D}$ are affected by some kind of random\footnote{ Here with \textit{random} means "\textit{uncorrelated with the input-output relationship}".} component, $\hat f$ should be a predictive function that does not \textit{follow the noise}, but rather models the true input-output relationship.
		In ML, a model that fits well $\mathcal{D}$ but performs poorly on $\mathcal{D}'$ is said to be \textit{overfitting}.

	  \begin{figure}[!h]
	  	\centering
	  	\subfloat[]{%
	  		\includegraphics[width=0.5\textwidth]{part1/regression_underfit.png}
	  		\label{fig:regression_u}%
	  	}%
	  	%\hfill%
	  	\subfloat[]{%
	  		\includegraphics[width=0.5\textwidth]{part1/regression_overfit.png}
	  		\label{fig:regression_o}%
	  	}%
	      \hfill%
	  	\subfloat[]{%
	  		\includegraphics[width=0.5\textwidth]{part1/regression_fit.png}
	  		\label{fig:regression_f}%
	  	}%
	  	\caption{An example of underfit (a), overfit (b) and optimal fit (c) for a nonlinear regression problem. The data are a downsampled version ($f_s = 546~Hz$) of the first observation of gravitational waves from a binary black hole merger detected on September 14\textsuperscript{th}, 2015, 09:50:45 {\sc utc} at LIGO, Hanford (WA).}\label{fig:regression_fit}
	  \end{figure}

	  \begin{figure}[!h]
			\centering
			\subfloat[]{%
				\includegraphics[width=0.5\textwidth]{part1/classification_underfit.png}
				\label{fig:regression_uu}%
			}%
			%\hfill%
			\subfloat[]{%
				\includegraphics[width=0.5\textwidth]{part1/classification_overfit.png}
				\label{fig:regression_oo}%
			}%
			\hfill%
			\subfloat[]{%
				\includegraphics[width=0.5\textwidth]{part1/classification_fit.png}
				\label{fig:regression_ff}%
			}%
			\caption{An example of underfit (a), overfit (b) and optimal fit (c) for a nonlinear binary classification problem. Each data point is a pulsar candidate randomly sampled from the High Time Resolution Universe Survey (South) dataset. The data are standardized and and projected on a two-dimensional plane by the \ac{t-SNE}~\cite{van2008visualizing}. }\label{fig:classification_fit}
	  \end{figure}


%	    Real-world data collections can be heavily affected by random noise,
%	    As the biomedical world is the the main area of interest of this thesis (see Section~\ref{sec:challenges_biomedical}),
%	    The most widely adopted common strategy to build predictive models out of noisy data is called \textit{regularization}.
      In ML literature, \textit{regularization} is the most important countermeasure to overfitting and it is widely adopted, under several forms, to build predictive models out of noisy data.

	    The original contribution of this PhD thesis mainly relies on the application of data science and ML concepts to noisy domains. Therefore, regularization strategies are of primary interest in this discussion.
	    For each learning algorithm described, particular emphasis will be put on the relevant regularization strategies.

	    In its broader definition \textit{regularization} can be seen as the process of introducing additional information in order to solve a possibly ill-posed problem.
			As shown in Equation~\eqref{eq:losspen}, this is typically translated in the use of a regularization penalty $\mathcal{R}(f)$, controlled by a regularization parameter $\lambda$ \cite{tikhonov1963solution, evgeniou2000regularization}.

	     \begin{equation}\label{eq:losspen}
	    	\argmin_{f \in \mathcal{F}} \frac{1}{n}\sum_{i=1}^n L(f(\bm{x}_i),y_i) + \lambda \mathcal{R}(f)
	    \end{equation}

			% The expected result is a function that fits the training data while having good generalization properties, \ie accurate predictions on previously  \textit{unseen} test data \cite{hastie2009elements}.
			Choosing different $\mathcal{R}(f)$ implies inducing different effects on the solution and it also leads to the definition of different learning machines (see Section~\ref{subsec:regularization_methods}).
			With the regularization parameter $\lambda$ it is possible to control the trade-off between adherence to the training data and strength of the effect induced by $\mathcal{R}(f)$.
			As an example, we can think of using a penalty that induces smoothness, such as the $\ell_2$-norm, or sparsity, such as the $\ell_1$-norm, in the solution.
			A pictorial representation of a learning machine working in overfitting, underfitting and optimal fitting regime in regression and classification cases can be seen in
			Figure~\ref{fig:regression_fit}~\footnote{ Source \url{https://losc.ligo.org/events/GW150914/}.}
			and
		  Figure~\ref{fig:classification_fit}~\footnote{ Source \url{https://archive.ics.uci.edu/ml/datasets/HTRU2}.}, respectively.


	%    The regularization penalty $R(f)$ imposes stability on the expected function exploiting the available prior knowledge on the problem~\cite{tikhonov1963solution}. More details on $R(f)$ will be given in the next section.

	    Supervised learning machines may rely on very different mathematical backgrounds such as generalized linear models, nonlinear deep neural networks, kernels, trees, ensemble of trees, \etc. Nevertheless, disregarding their nature, they all share the common structure defined in Equation~\eqref{eq:losspen}.
	    The solution of this problem can be achieved either by Empirical (or Structured) Risk Minimization (\ac{ERM}) either by Maximum Likelihood/A Posteriori (\ac{MLE}/\ac{MAP}) Estimation. See Appendix~\ref{appendix:A} for more details on this two strategies, and their connection.

	    \subsection{Regularization methods} \label{subsec:regularization_methods}
	    Regularization methods is a broad class of models that include linear and nonlinear techniques for both regression and classification. The main characteristic of the methods falling in this class, is that they are particularly straightforward to express as in Equation~\eqref{eq:losspen}. In fact, as described in~\cite{evgeniou2000regularization}, they are easily identifiable by the use of one loss function $L(f(\bm{x}), y)$ and one, or more, regularization penalty $\mathcal{R}(f)$.

	    In the following sections an overview of the most popular regularization methods is presented.

			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\subsubsection{Ordinary least squares} \label{sec:ols}
			We start this discussion focusing on linear models $\hat y = f(\bm{x})=\bm{x}^T\bm{w}$ and taking into account the most popular loss function for regression problems: the square loss $L(\hat y, y) = (\bm{x}^T\bm{w}-y)^2$. The data fitting problem expressed in Equation~\eqref{eq:ols} is known as \textit{Ordinary Least Squares} (\ac{OLS}), or simply as \textit{linear regression}, and it does not include any regularization term.

			\begin{equation} \label{eq:ols}
				\bm{\hat w}_{\text{OLS}} = \argmin_{\bm{w} \in \mathbb{R}^d} \frac{1}{n} \sum_{i=1}^n (\bm{x}_i^T\bm{w} - y_i)^2 = \argmin_{\bm{w} \in \mathbb{R}^d} \frac{1}{n} \norm{X\bm{w} - \bm{y}}_2^2
			\end{equation}

			The minimization problem in Equation~\eqref{eq:ols} is convex and differentiable and its solution can be achieved in closed-form as
			$$
				\bm{\hat w}_{\text{OLS}} = (X^TX)^{-1}X^T\bm{y}
			$$
			or by iterative minimization routines such as (stochastic) gradient descent-like algorithms~\cite{boyd2004convex, sra2012optimization}. A pictorial representation of the solution of OLS can be seen in Figure~\ref{fig:w_ols}.

			In case of multiple regression tasks, the OLS approach can be extended to vector-valued regression as well. In this case the least squares problem can be written as
			\begin{equation} \label{eq:vvr_ols}
				\begin{aligned}
				\hat W_{\text{OLS}}={} & \argmin_{W \in \mathbb{R}^{d \times k}} \frac{1}{n} \sum_{i=1}^n \sum_{t=1}^k (\bm{x}_i^T\bm{w}^t - y_i^t)^2  \\
				& =	\argmin_{W \in \mathbb{R}^{d \times k}} \frac{1}{n} \norm{XW - Y}_F^2
				\end{aligned}
			\end{equation}
			where $\norm{A}_F= \sqrt{\sum_{i=1}^n\sum_{t=1}^k |a_i^t|^2}$ is the \textit{Frobenius} norm (also known as  \textit{Hilbert-Schmidt} norm) and it can be considered as an extension of the $\ell_2$-norm to the matrix case.
			Lacking of appropriate regularization penalties, solving the problem in Equation~\eqref{eq:vvr_ols} corresponds to solving $k$ isolated regression problems, one for each task.
			The vector-valued OLS approach, even if theoretically legit, is rather uncommon in practical applications and a regularized version of Equation~\eqref{eq:vvr_ols} is typically preferred (see following sections).

			Even though mainly used for regression, the square loss can also be used to solve binary classification problems (see Table~\ref{tab:losses}). In this case, it can be rewritten as
			\begin{equation} \label{eq:margin_square_loss}
				(\bm{x}^T\bm{w}-y)^2 = (1-y \cdot \bm{x}^T\bm{w})^2
			\end{equation}
			exploiting the fact that the two classes are encoded with binary labels: $\bm{y} \in \{+1,-1\}^n$. For multiclass classification problems, strategies such as \textit{One-vs-One} (\ac{OVO}) or \textit{One-vs-All} (\ac{OVA}) can be adopted to reduce the problem to multiple binary classifications~\cite{hastie2009elements}.

			OLS is probably the most na\"ive prediction strategy, nevertheless it is widely adopted in several studies. Let's see what happens when we use the OLS model on a real regression problem.

			For this example, and the following ones, we take into account the dataset $\mathcal{D}_{\text{aging}} = \{(\bm{x}_i, y_i)\}_{i=1}^{n=111}$ where each input sample $\bm{x}_i \in \mathbb{R}^{12}$ presents a set of measures describing the metabolic state of a healthy subject and $y_i \in \mathbb{N}_+$ is its age expressed in years.
			For the sake of this discussion a  thorough description of $\mathcal{D}_{\text{aging}}$ at this point is irrelevant\footnote{ This regression problem is widely described and analyzed in Chapter~\ref{chap:frassoni}.}, we can simply think as the $d=12$ variables as predictors of the outcome $y$ and we look for some linear input-output relationship.
			In order to do that, we randomly split $\mathcal{D}_{\text{aging}}$ in two chunks obtaining a training and a test set of $n_{\text{tr}}=74$ and $n_{\text{ts}}=37$ samples respectively. Then, we fit the OLS model on the training set obtaining the weights vector $\bm{\hat w}_{\text{OLS}}$ represented in Figure~\ref{fig:coefs_ols}.
			\begin{figure}[!h]
				\centering
				\includegraphics[width=0.8\textwidth]{part1/coefs_ols.png}
				\caption{A pictorial representation of the vector $\bm{\hat w}_{\text{OLS}}$ obtained fitting an OLS model on $74$ randomly selected training samples of $\mathcal{D}_{\text{aging}}$. Variables associated with positive (\ie directly proportional to the output) and a negative (\ie inversely proportional) weight are represented in blue and red, respectively.} \label{fig:coefs_ols}
		  \end{figure}
			As we can see, in order to achieve a predictive model, OLS can only spread the weights across all the input variables. Evaluating $\bm{\hat w}_{\text{OLS}}$ on the test set, this model has a Mean Absolute Error (\ac{MAE}) of $10.598$ years and explains the $74.29\%$ of the variance.

			This result looks promising, but we will see in the next sections whether they can be improved with the use of some regularization penalty.

%			We have already pointed out that learning predictive functions only by minimizing a loss function may lead to suboptimal solutions; therefore in the following sections we will explore pro and cons of using different penalties.

			\subsubsection{Ridge regression} \label{sec:ridge_regression}
			In its original proposition, \textit{ridge regression}~\cite{hoerl1970ridge} is defined as a least squares problem penalized by the squared $\ell_2$-norm of the regression coefficients, see Equation~\eqref{eq:l2}.

			\begin{equation}\label{eq:l2}
				\mathcal{R}_{\mbox{$\ell_2$}}(\bm{w}) = \sum_{j=1}^d (w_j)^2 = \norm{\bm{w}}_2^2
			\end{equation}

			Therefore, the ridge regression minimization problem can be written as in Equation~\eqref{eq:ridge_regression}.

			\begin{equation} \label{eq:ridge_regression}
				\bm{\hat w}_{\ell_2} = \argmin_{\bm{w} \in \mathbb{R}^d} \frac{1}{n} \sum_{i=1}^n (\bm{x}_i^T\bm{w} - y_i)^2 + \lambda  \sum_{j=1}^d (w_j)^2 = \argmin_{\bm{w} \in \mathbb{R}^d} \frac{1}{n} \norm{X\bm{w} - \bm{y}}_2^2 + \lambda \norm{\bm{w}}_2^2
			\end{equation}

			This penalty leads to smooth solutions as it shrinks the coefficients toward zero, but it does not achieve a parsimonious representation, as it always keep all the variables in the model.
			The ridge regression problem of Equation~\eqref{eq:ridge_regression} is convex  and differentiable and a pictorial representation of its solution in a 2D case is depicted in Figure~\ref{fig:w_ridge}. The ridge coefficients $\bm{\hat w}_{\ell_2}$ can be estimated in closed-form as
			$$
				\bm{\hat w}_{\ell_2} = (X^TX + \lambda n I)^{-1}X^T\bm{y}
			$$
			where $I$ is the $d \times d$ identity matrix. An estimate for the ridge coefficients can also be obtained with gradient descent-like optimization routines~\cite{boyd2004convex, sra2012optimization}.

			The regularization parameter $\lambda$ plays the fundamental role of balancing the trade-off between data adherence and smoothness of the solution.
			Penalizing the $\ell_2$-norm of the regression coefficients, their value is shrunk toward zero. This results in an increased robustness of the solution to the noise affecting the training data.

			In case of multiple outputs, the vector-valued ridge regression problem can be written as in Equation~\eqref{eq:vvr_ridge}.
			\begin{equation} \label{eq:vvr_ridge}
				\begin{aligned}
					\hat W_{\ell_2}={} & \argmin_{W \in \mathbb{R}^{d \times k}} \frac{1}{n} \sum_{i=1}^n \sum_{t=1}^k (\bm{x}_i^T\bm{w}^t - y_i^t)^2  + \lambda \sum_{j=1}^d \sum_{t=1}^k |w_j^t|^2\\
					& =	\argmin_{W \in \mathbb{R}^{d \times k}} \frac{1}{n} \norm{XW - Y}_F^2 + \lambda \norm{W}_F^2
				\end{aligned}
			\end{equation}
			As already seen for vector-valued OLS, the Frobenius norm penalty does not induce any task coupling, hence solving the problem in Equation~\eqref{eq:vvr_ridge} still corresponds to individually solve $k$ regression tasks.

			This method can be applied to binary classification problems by using the margin loss function of Equation~\eqref{eq:margin_square_loss}. Nevertheless, for $\ell_2$-norm penalized classification problems the use of the \textit{logistic loss} is usually preferred (see Section~\ref{sec:logistic_regression}).

			In deep learning literature, penalizing the regression coefficients with the $\ell_2$-norm is known as \textit{weight decay}~\cite{krogh1992simple}. Ridge regression can also be considered a form of  \textit{Tikhonov regularization}~\cite{tikhonov1963solution} and of \textit{regularization network}~\cite{evgeniou2000regularization}.
%			ridge regression is successfully applied in countless and heterogeneous biological studies mainly involving regression problems.
%			For instance, in~\cite{kratsch2014} the authors propose a ridge regression-based method to estimate ancestral characters from phylogenetic trees, while in~\cite{bovelstad2007} ridge regression is used to perform survival prediction from gene expression data.
			%\todo{\begin{enumerate}
			%	% \item loss: square loss
			%	% \item penalties: l2
			%	% \item use in regression, classification
			%%	\item use cases
			%%	\item extension to multi-task and multi-class
			%\end{enumerate}}
			%Ridge regression can be extended to the case of multiple-output prediction for both vector-valued regression and multi-category classification problems.
			%In the first case,

			Let's see what happens when this method is applied to a real regression problem. For ease of comparison, we take into account the dataset $\mathcal{D}_{\text{aging}}$, introduced in Section~\ref{sec:ols}.
			Compared to OLS, ridge regression has the parameter $\lambda$ that must be fixed before fitting the model. In this example, we estimated the best value $\hat \lambda_{\text{cv}}$ according to a standard grid-search cross-validation strategy. %~\cite{hastie2009elements}.
			This consists in fixing a range of ($30$) possible values for $\lambda$ (in a logarithmic scale from $10^{-3}$ to $10^2$) and
			pick the best value as the one achieving the lowest validation error, estimated via ($5$-fold) cross-validation (see Section~\ref{sec:model_evaluation}).
			Therefore, once the best value for the regularization parameter is fixed ($\hat \lambda_{\text{cv}} = 20.43$), in this case. the experimental setup used for OLS is preserved.
			
			\begin{figure}[]
				\centering
				\includegraphics[width=0.8\textwidth]{part1/coefs_ridge.png}
				\caption{A pictorial representation of the vector $\bm{\hat w}_{\ell_2}$ obtained fitting a ridge regression model on $74$ randomly selected training samples of $\mathcal{D}_{\text{aging}}$. Variables associated with positive (\ie directly proportional to the output) and a negative (\ie inversely proportional) weight are represented in blue and red, respectively.} \label{fig:coefs_ridge}
			\end{figure}
			The ridge coefficients $\bm{\hat w}_{\ell_2}$ are represented in 	Figure~\ref{fig:coefs_ridge}.
			Comparing Figure~\ref{fig:coefs_ridge} and Figure~\ref{fig:coefs_ols} we can see that the amplitude of the ridge regression coefficients is, in absolute value, decreased by the use of the $\ell_2$ penalty. Indeed, several entries of $\bm{\hat w}_{\ell_2}$ are very small, but none of them is exactly zero. This is the expected behavior of the $\ell_2$-norm penalty. Evaluating $\bm{\hat w}_{\ell_2}$ on the test set, this model has  $\text{MAE}=8.615$ years and explains the $81.19\%$ of the variance, outperforming OLS.

			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\subsubsection{Lasso} \label{sec:the_lasso}
			%Tibshirani, in its seminal paper~\cite{tibshirani1996regression}, introduced the use of the $\ell_1$-norm  Equation~\eqref{eq:lasso}, as sparisty-enforcing regularizer for least squares problems.
			%In the original Lasso paper, the sparisty-enforcing properties of the $\ell_1$-norm, Equation~\eqref{eq:lasso}, as regularizer for least squares problems is typically known as the Lasso.
			%Such penalty is among the most popular sparsity-enforcing regularization method.

			The Lasso~\cite{tibshirani1996regression} can be defined as a least square problem penalized by the $\ell_1$-norm of the regression coefficients, see Equation~\eqref{eq:l1}.
			% In the Lasso, as in ridge regression (see Section~\ref{sec:ridge}), the data fidelity is measured by means of the square loss, while the $\ell_1$-norm, defined in Equation~\eqref{eq:lasso}, is used as regularization penalty.
			\begin{equation}\label{eq:l1}
				\mathcal{R}_{\mbox{$\ell_1$}}(\bm{w}) = \sum_{j=1}^d |w_j| = |\bm{w}|_1
			\end{equation}
			Therefore, the Lasso minimization problem can be written as in Equation~\eqref{eq:lasso}.

			\begin{equation} \label{eq:lasso}
				\bm{\hat w}_{\ell_1} = \argmin_{\bm{w} \in \mathbb{R}^d} \frac{1}{n} \sum_{i=1}^n (\bm{x}_i^T\bm{w} - y_i)^2 + \lambda  \sum_{j=1}^d |w_j| = \argmin_{\bm{w} \in \mathbb{R}^d} \frac{1}{n} \norm{X\bm{w} - \bm{y}}_2^2 + \lambda |\bm{w}|_1
			\end{equation}

			The Lasso can be used to perform linear model fitting and, thanks to its desirable properties, it is a popular choice for embedded variable selection~\cite{guyon2003introduction}, as described in Section~\ref{subsec:feature_selection}. At first, the $\ell_1$-norm enforces sparsity in the solution, hence producing compact and easily interpretable results. Secondly, the Lasso optimization problem is convex and, although non-differentiable, it is computationally feasible even in very high dimensional scenarios. Popular minimization algorithms for the Lasso problem are, for instance, the \textit{Fast Iterative Shrinkage-Thresholding Algorithm}  (\ac{FISTA})~\cite{beck2009fast} and the \textit{coordinate descent algorithm}~\cite{wu2008coordinate}. A pictorial representation of the Lasso solution can be seen in Figure~\ref{fig:w_lasso}.

			A popular application of the Lasso is to perform shrinkage and variable selection in survival analysis for Cox proportional hazard regression \cite{tang2017spike, gui2005penalized, tibshirani1997lasso} and additive risk models~\cite{ma2007additive}.
			Such $\ell_1$-penalized methods are extensively applied in literature to predict survival time from molecular data collected from patients affected by different kinds of tumor.
			%\todo{Add use cases: lu2011lasso, rakitsch2013lasso $\dots$}

			The Lasso can also be extended to vector-valued regression problems by using the mixed $L_{2,1}$-norm, defined in Equation~\eqref{eq:L21}, as regularization penalty~\cite{gramfort2012mixed}.
			\begin{equation}\label{eq:L21}
				\mathcal{R}_{\ell_1}(W)= \sum_{j=1}^d\sqrt{\Bigg(\sum_{t=1}^k |w_j^t|^2\Bigg)} = \norm{W}_{2,1}
			\end{equation}
			Therefore, the vector-valued regularization problem can be written as in Equation~\eqref{eq:multitask_lasso}
			\begin{equation} \label{eq:multitask_lasso}
				\begin{aligned}
					\hat W_{\ell_1}={} & \argmin_{W \in \mathbb{R}^{d \times k}} \frac{1}{n} \sum_{i=1}^n \sum_{t=1}^k (\bm{x}_i^T\bm{w}^t - y_i^t)^2 + \lambda  \sum_{j=1}^d \sqrt{\Bigg(\sum_{t=1}^k |w_j^t|^2\Bigg)} \\
					& =	\argmin_{W \in \mathbb{R}^{d \times k}} \frac{1}{n} \norm{XW - Y}_F^2 + \lambda ||W||_{2,1}
				\end{aligned}
			\end{equation}
			and it is known as Multi-task Lasso~\cite{lee2010adaptive}. Such norm enforces a row-structured sparsity in the regression weights, hence preserving the interpretability of the solution.

			Originally proposed to solve regression problems, the Lasso can also be adopted in binary classification tasks; although, in this case, \textit{sparse logistic regression} is often preferred~\cite{tong2009genome} (see Section~\eqref{sec:logistic_regression}).

			Let's see what happens when the Lasso model is applied to a real regression problem. Once again, for ease of comparison with OLS and ridge, we take into account the dataset $\mathcal{D}_{\text{aging}}$, introduced in Section~\ref{sec:ols}. The experimental setup in this case is identical to the one previously applied for ridge regression. The best value for the regularization parameter, chosen via grid-search cross-valudation, is $\hat \lambda_{\text{cv}}=1.27$. The Lasso coefficients $\bm{ \hat w}_{\ell_1}$ are represented in Figure~\ref{fig:coefs_lasso}.

			\begin{figure}[]
				\centering
				\includegraphics[width=0.8\textwidth]{part1/coefs_lasso.png}
				\caption{A pictorial representation of the vector $\bm{\hat w}_{\ell_1}$ obtained fitting a Lasso model on $74$ randomly selected training samples of $\mathcal{D}_{\text{aging}}$. Variables associated with positive (\ie directly proportional to the output) and a negative (\ie inversely proportional) weight are represented in blue and red, respectively.} \label{fig:coefs_lasso}
			\end{figure}
		
			Comparing $\bm{\hat w}_{\ell_1}$ with $\bm{\hat w}_{\ell_2}$ and $\bm{\hat w}_{\text{OLS}}$ (Figure~\ref{fig:coefs_lasso}, Figure~\ref{fig:coefs_ridge} and Figure~\ref{fig:coefs_ols}, respectively) we can observe that, for the first time, to only $6$ variables, out of $12$, a non-negative value is assigned. This is an example of the sparsity-enforcing effect of the $\ell_1$-norm regularization penalty.
			The $6$ variables with nonzero weight can be considered as \textit{selected} for the prediction problem at hand.
			Evaluating $\bm{\hat w}_{\ell_1}$ on the test set, the Lasso has  $\text{MAE}=8.387$ years and explains the $81.51\%$ of the variance, slightly outperforming ridge.

			Using sparsity-enforcing penalties, such as the $\ell_1$-norm, an insightful experiment can be to observe the regularization path. This can be done by iteratively fitting the model with decreasing values of the regularization parameter $\lambda$, usually expressed in logarithmic scale. An example of the Lasso path for the aging problem is reported in Figure~\ref{fig:lasso_path}.

			\begin{figure}[]
				\centering
				\includegraphics[width=0.8\textwidth]{part1/lasso_path.png}
				\caption{Profiles of the Lasso coefficients for the aging problem as $\lambda$ decreases. The vertical dashed line represents the optimal value $\hat \lambda_{\text{cv}}$ estimated by grid-search ($5$-fold) cross-validation.} \label{fig:lasso_path}
			\end{figure}
		
			Weights corresponding to features that are more likely to be relevant for the prediction problem should early move away from the horizontal axis. Conversely, as the regularization parameter increases, the model should enforce less sparsity, hence tolerating more and more irrelevant features having nonzero weight. The vertical dashed line in Figure~\ref{fig:lasso_path} corresponds to $\hat \lambda_{\text{cv}}$ and it hits the profiles of the weights consistently with what shown in Figure~\ref{fig:coefs_lasso}.

			When used for variable selection, the Lasso has two major drawbacks. First, in presence of groups of correlated variables, this method tends to select only one variable per group, ignoring the others. Secondly, the method cannot select more variables than the sample size~\cite{waldmann2013evaluation, de2009regularized}.
			The effect of such drawbacks is dramatic when using the Lasso in $n \ll d$ scenarios.
			In order to ameliorate this issues, several Lasso-inspired models were proposed~\cite{meinshausen2010stability, hoggart2008simultaneous, zou2006adaptive}.
			% (see Section~\ref{sec:other_sparsity}).
			In the next section we will describe one of the most popular and straightforward Lasso extensions: the Elastic-Net~\cite{zou2005regularization}.

			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\subsubsection{Elastic-Net} \label{sec:elastic_net}
			The Elastic-Net method~\cite{zou2005regularization, de2009elastic} can be formulated as a least squares problem penalized by a convex combination of Lasso ($\ell_1$-norm) and ridge regression ($\ell_2$-norm) penalties, as in Equation~\eqref{eq:l1l2}.

			\begin{equation}\label{eq:l1l2}
				R_{\mbox{$\ell_1\ell_2$}}(\bm{w}) = \sum_{j=1}^d (\alpha\,|w_j| + (1-\alpha)\,w_j^2) = \alpha\,|\bm{w}|_1 + (1-\alpha)\,\norm{\bm{w}}_2^2
			\end{equation}

			Therefore, the Elastic-Net minimization problem can be written as in Equation~\eqref{eq:enet}
			\begin{equation} \label{eq:enet}
				\begin{aligned}
				\bm{\hat w}_{\ell_1\ell_2} = {} & \argmin_{\bm{w} \in \mathbb{R}^d} \frac{1}{n} \sum_{i=1}^n (\bm{x}_i^T\bm{w} - y_i)^2 + \lambda\,\bigg[ \sum_{j=1}^d (\alpha\,|w_j| + (1-\alpha)\,w_j^2) \bigg] \\
				& = \argmin_{\bm{w} \in \mathbb{R}^d} \frac{1}{n} \norm{X\bm{w} - \bm{y}}_2^2 + \lambda\,\big[ \alpha\,|\bm{w}|_1 + (1-\alpha)\,\norm{\bm{w}}_2^2 \big]
			\end{aligned}
		  \end{equation}
			with $0 \leq \alpha \leq 1$, or equivalently as
			\begin{equation} \label{eq:enet_l1l2}
			\bm{\hat w}_{\ell_1\ell_2} = \argmin_{\bm{w} \in \mathbb{R}^d} \frac{1}{n} \norm{X\bm{w} - \bm{y}}_2^2 + \tau\,|\bm{w}|_1 + \mu\,\norm{\bm{w}}_2^2
			\end{equation}
			where $\tau = \lambda\alpha$ and $\mu = \lambda(1-\alpha)$. The first formulation of the problem, Equation~\eqref{eq:enet}, is more convenient when we want to control the overall amount of regularization with $\lambda$, given a certain amount of sparsity $\alpha$. In fact, it is easy to see that fitting the Elastic-Net model for $\alpha=0$ or $\alpha=1$ is equivalent to solve ridge or Lasso regression, respectively. On the other hand, writing the Elastic-Net problem as in Equation~\eqref{eq:enet_l1l2} is more convenient when we want to separately control the $\ell_1$- and $\ell_2$-norm penalties, as in~\cite{de2009regularized}.

			The Elastic-Net\footnote{ Actually, in the original paper the authors refer to Equations~\eqref{eq:enet} and~\eqref{eq:enet_l1l2} as \textit{na\"ive} Elastic-Net~\cite{zou2005regularization} because empirical evidence show that the weights $\bm{\hat w}_{\ell_1\ell_2}$ may suffer from over-shrinking when appropriate rescaling strategies, also described in~\cite{de2009regularized},  are not applied.} model is widely adopted to perform linear model fitting and variable selection. Indeed, the combined presence of the two norms promotes sparse solutions were groups of correlated variables can be simultaneously selected, hence overcoming the variable selection drawbacks of the Lasso and making the Elastic-Net suitable for variable selection in $n \ll d$ scenarios.
			As already seen for the Lasso, the minimization problem in Equation~\eqref{eq:enet} is convex and non-differentiable, due to the $\ell_1$-norm, and it can be efficiently solved either by proximal forward-backward splitting strategies (\eg FISTA~\cite{beck2009fast}), or by coordinate descent~\cite{wu2008coordinate}. A pictorial representation of the Elastic-Net solution can be seen in Figure~\ref{fig:w_enet}.

			The Elastic-Net method is successfully applied in several biomedical fields, including gene expression~\cite{jacob2015robust, de2009regularized}, genome-wide association studies~\cite{waldmann2013evaluation} and other molecular data~\cite{aben2016tandem, jacob2015robust}.
			% In~\cite{csala2017sparse} the authors propose an iterative algorithm that exploits the variable selection capabilities of this method to estimate explanatory variables weights  in sparse redundancy analysis; such method is applied to explain the variability in gene expressions by epigenomic data (\ie methylation markers) collected from blood leukocytes of Marfan Syndrome patients.


			The Elastic-Net can also be extended to vector-valued regression problems by using a convex combination of the $L_{2,1}$- and the Frobenius norms. This multiple output regression problem is known as Multi-task Elastic-Net~\cite{chen2012adaptive} (\ac{MTEN}) and it can be written as in Equation~\eqref{eq:multitask_enet}.
			\begin{equation} \label{eq:multitask_enet}
				\begin{aligned}
					\hat W_{\ell_1\ell_2}={} & \argmin_{W \in \mathbb{R}^{d \times k}} \frac{1}{n} \sum_{i=1}^n \sum_{t=1}^k (\bm{x}_i^T\bm{w}^t - y_i^t)^2 + \lambda \bigg[ \alpha \sum_{j=1}^d \sqrt{\Bigg(\sum_{t=1}^k |w_j^t|^2\Bigg)} + (1-\alpha)  \sum_{j=1}^d \sum_{t=1}^k |w_j^t|^2 \bigg] \\
					& =	\argmin_{W \in \mathbb{R}^{d \times k}} \frac{1}{n} \norm{XW - Y}_F^2 + \lambda [\alpha\,||W||_{2,1} + (1-\alpha)\,\norm{W}_F^2]
				\end{aligned}
   		    \end{equation}

			Even if originally proposed to solve regression problems, as already seen for Lasso and ridge regression, the Elastic-Net can be adopted in binary classification tasks. Nevertheless, for classification problems, the use of the logistic loss is usually preferred.

			Let's see what happens when the Elastic-Net model is applied to an actual regression problem. As usually, we tackle the aging task (introduced in Section~\ref{sec:ols}).
			The experimental setup in this case is the same already adopted for the Lasso in Section~\ref{sec:the_lasso}. The only difference is that the grid-search cross-validation routine looks for the optimal values for the two regularization parameters $(\hat \lambda_{\text{cv}}, \hat \alpha_{\text{cv}}) = (0.57, 0.48)$ in a 2D grid consisting of $30 \times 30$ values ($\alpha$ candidates range from $0$ to $1$ in a linear scale, whilst $\lambda$ candidates range from $10^{-3}$ to $10^{2}$ in a logarithmic scale). The Elastic-Net coefficients $\bm{\hat w}_{\ell_1\ell_2}$ are represented in Figure~\ref{fig:coefs_enet}.

			\begin{figure}[!h]
				\centering
				\includegraphics[width=0.8\textwidth]{part1/coefs_enet.png}
				\caption{A pictorial representation of the vector $\bm{\hat w}_{\ell_1\ell_2}$ obtained fitting a Elastic-Net model on $74$ randomly selected training samples of $\mathcal{D}_{\text{aging}}$. Variables associated with positive (\ie directly proportional to the output) and a negative (\ie inversely proportional) weight are represented in blue and red, respectively.} \label{fig:coefs_enet}
		  \end{figure}

			Comparing $\bm{\hat w}_{\ell_1\ell_2}$ in Figure~\ref{fig:coefs_enet} with $\bm{\hat w}_{\ell_1}$ in Figure~\ref{fig:coefs_lasso} we can notice that in the Elastic-Net solution $10$ variables have nonzero weight, that is more with respect to the $6$ of the Lasso. This is the expected behavior of the added $\ell_2$-norm, which helps the model to select groups of collinear variables. Evaluating the Elastic-Net solution on the test set, $\bm{\hat w}_{\ell_1\ell_2}$ achieves $\text{MAE} = 8.321$ years explaining the $82.13\%$, slightly outperforming Lasso, hence ranking first in this little challenge of square loss-based linear regression methods.

			As already seen for the Lasso, we can inspect the Elastic-Net weights path, obtained fixing $\alpha = \hat \alpha_{\text{cv}}$ for decreasing values of $\lambda$, see Figure~\ref{fig:enet_path}.
			\begin{figure}[!h]
				\centering
				\includegraphics[width=0.8\textwidth]{part1/enet_path.png}
				\caption{Profiles of the Elastic-Net coefficients for the aging problem as $\lambda$ decreases. The vertical dashed line represents the optimal value $\hat \lambda_{\text{cv}}$ estimated by grid-search ($5$-fold) cross-validation.} \label{fig:enet_path}
			\end{figure}
			The vertical dashed line in Figure~\ref{fig:enet_path} corresponds to $\hat \lambda_{\text{cv}}$ and it hits the profiles of the weights consistently with the Elastic-Net solution represented in Figure~\ref{fig:coefs_lasso}. The Elastic-Net, compared to the Lasso, produces smoother regularization paths in which the allegedly correlated variables enter the solution earlier.

			Another comparison between the behavior of OLS, ridge, Lasso and Elastic-Net regression is presented in Figure~\ref{fig:l1l2_norms_scatterplot}. The four achieved solutions of the same aging regression problem are represented in a scatter plot where horizontal and vertical axis represents their $\ell_2$- and $\ell_1$-norm, respectively.
			
			\begin{figure}[]
				\centering
				\includegraphics[width=0.8\textwidth]{part1/l1l2_norms_scatterplot.png}
				\caption{A comparison of the value of the $\ell_1$ and $\ell_2$ norms of the weights obtained by OLS, ridge, Lasso and Elastic-Net.} \label{fig:l1l2_norms_scatterplot}
		  \end{figure}
	  
			As expected,
			\begin{enumerate}
				\item the unpenalized solution $\bm{\hat w}_{\text{OLS}}$ shows the highest values for the two norms and it is placed in the top-right side of the plot,
				\item the ridge solution $\bm{\hat w}_{\ell_2}$ has lowest $\ell_2$-norm,
				\item the Lasso solution $\bm{\hat w}_{\ell_1}$ has lowest $\ell_1$-norm and
				\item the Elastic-Net solution shows the lowest values for the two norms and it is placed in the bottom-left side of the plot.
		  \end{enumerate}
			This is consistent with the type of regularization imposed to each method. Interestingly, in this case, the method that performs better on the test set, Elastic-Net, has lowest norms.

			The Elastic-Net penalty is not the only method in which sparsity is enforced on a group level. For example, (overlapping) group Lasso and graph Lasso penalties can be applied when the variables are naturally partitioned in (overlapping) groups, or when their interaction can be modeled by a graph. A detailed description of these methods is beyond the scope of this thesis and we refer to~\cite{jacob2009group, witten2009covariance} and references therein for their comprehensive description.

			\begin{figure}[]
				\centering
				\subfloat[]{%
					\includegraphics[width=0.5\textwidth]{part1/w_ols.png}
					\label{fig:w_ols}%
				}%
				%\hfill%
				\subfloat[]{%
					\includegraphics[width=0.5\textwidth]{part1/w_ridge.png}
					\label{fig:w_ridge}%
				}%
				\hfill%
				\subfloat[]{%
					\includegraphics[width=0.5\textwidth]{part1/w_lasso.png}
					\label{fig:w_lasso}%
				}%
				%\hfill%
				\subfloat[]{%
					\includegraphics[width=0.5\textwidth]{part1/w_enet.png}
					\label{fig:w_enet}%
				}%
				\caption{Pictorial representation of the contour lines of the square loss in a 2D regression problem with various penalties: (a) ordinary least squares (no penalty), (b) ridge regression ($\ell_2$-norm penalty), (c) the Lasso ($\ell_1$-norm penalty) and finally (d) the Elastic-Net ($\ell_1$- and $\ell_2$-norm penalties).} \label{fig:square_loss_penalties}
			\end{figure}

			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%			\subsubsection{Other sparsity-inducing penalties} \label{sec:other_sparsity}
    		\subsubsection{Nuclear norm minimization} \label{sec:nucelear_norm_minimization}
			% 
			Dealing with vector-valued linear regression problems, $L_{2,1}$- and the Frobenius norms are not the only options. Another popular sparsity-enforcing penalty is the nuclear norm (also known as trace norm) that, given the matrix $W \in \mathbb{R}^{d \times k}$, is defined as
			
			$$
				\norm{W}_* = \text{trace}\big(\sqrt{W^TW}\big) = \sum_{j=1}^{\min[d, k]} \sigma_j(W)
			$$
			
			where $\sigma_j(W)$ is the $j^{\text{th}}$ singular value of $W$.
			The nuclear norm minimization problem, widely adopted for instance in matrix completion tasks, can be stated as in Equation~\ref{eq:nnm}.
			
			\begin{equation} \label{eq:nnm}
				\begin{aligned}
					\hat W_{*}={} & \argmin_{W \in \mathbb{R}^{d \times k}} \frac{1}{n} \sum_{i=1}^n \sum_{t=1}^k (\bm{x}_i^T\bm{w}^t - y_i^t)^2 + \lambda  \sum_{j=1}^{\min[d, k]} \sigma_j(W) \\
					& =	\argmin_{W \in \mathbb{R}^{d \times k}} \frac{1}{n} \norm{XW - Y}_F^2 + \lambda ||W||_{*}
				\end{aligned}
			\end{equation}
			
			With the nuclear norm penalty it is possible to achieve a low-rank solution~\cite{candes2009exact}.

			% \begin{table}[htb]
			% 	\centering
			% 	\caption{Overview of the matrix norms used for vector-valued regression.}\label{tab:norms}
			% 	\begin{tabular}{l|l|l}
			% 		\toprule
			% 		Matrix norm  & Notation         & Definition                      \\  \midrule
			% 		Frobenius        & $\norm{A}_F$     & $\sqrt{\text{trace}(A^TA)}$     \\ [0.05cm]
			% 		Nuclear          & $\norm{A}_*$     & $\text{trace}\sqrt{(A^TA)}$     \\ [0.05cm]
			% 		Mixed $\text{L}_{2,1}$        & $\norm{A}_{2,1}$ & $\sum_{j}\norm{\bm{a}_j}_2$ \\ \bottomrule
			% 	\end{tabular}
			% \end{table}

			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\subsubsection{Logistic Regression} \label{sec:logistic_regression}
			Logistic regression is one of the most popular linear methods for classification problems\footnote{ As counter-intuitive as it sounds.}. In its unpenalized form, it can be simply posed as the problem of minimizing the logistic loss (see Table~\ref{tab:losses}) on a given training dataset. Therefore, logistic regression can be written as in Equation~\eqref{eq:logistic_regression},
			\begin{equation} \label{eq:logistic_regression}
				\bm{\hat w}_{\text{LR}} = \argmin_{\bm{w} \in \mathbb{R}^d} \frac{1}{n}\sum_{i=1}^n\log(1 + e^{-y_i\,\bm{x}_i^T\bm{w}})
			\end{equation}
			where the labels $y_i \in \{+1,-1\}, \forall i=1,\dots,n$. The minimization problem above is convex and differentiable, although as the gradient is nonlinear in $\bm{w}$, it does not have a closed-form solution. So, unpenalized logistic regression problems are typically solved by (stochastic) gradient descent-like techniques~\cite{boyd2004convex, sra2012optimization}.

			Unpenalized logistic regression, although theoretically sound, is somewhat uncommon to meet in recent applied studies. As for the square loss, regularization penalties are typically used. For instance, we can have $\ell_2$-regularized logistic regression,
			\begin{equation} \label{eq:logistic_l2}
				\bm{\hat w}_{\ell_2} = \argmin_{\bm{w} \in \mathbb{R}^d} \frac{1}{n}\sum_{i=1}^n\log(1 + e^{-y_i\,\bm{x}_i^T\bm{w}}) + \lambda \norm{\bm{w}}_2^2
		  \end{equation}
			or various forms of sparse logistic regression such as
			\begin{equation} \label{eq:logistic_l1}
				\bm{\hat w}_{\ell_1} = \argmin_{\bm{w} \in \mathbb{R}^d} \frac{1}{n}\sum_{i=1}^n\log(1 + e^{-y_i\,\bm{x}_i^T\bm{w}}) + \lambda |\bm{w}|_1
		  \end{equation}
			which uses the Lasso penalty, or
			\begin{equation} \label{eq:logistic_l1l2}
				\bm{\hat w}_{\ell_1\ell_2} = \argmin_{\bm{w} \in \mathbb{R}^d} \frac{1}{n}\sum_{i=1}^n\log(1 + e^{-y_i\,\bm{x}_i^T\bm{w}}) + \lambda [\alpha |\bm{w}|_1 + (1-\alpha) \norm{\bm{w}}_2^2]
		  \end{equation}
			which uses the Elastic-Net penalty.
      The sparse logistic regression minimization problem can be efficiently solved either by proximal forward-backward splitting strategies, such as FISTA~\cite{beck2009fast}, or by coordinate descent~\cite{wu2008coordinate}.

			Moreover, multi-class classification with logistic regression can be achieved by OVO, AVA approaches, as well as with the multiclass generalization of logistic regression: \textit{softmax} regression~\cite{hastie2009elements} (\aka multinomial logistic regression), which can be expressed as
			\begin{equation} \label{eq:softmax}
				\hat W_{\text{LR}} = \argmin_{W \in \mathbb{R}^{d \times k}} -\frac{1}{n} \bigg[ \sum_{i=1}^n \sum_{t=1}^k \mathds{1}\{y_i = t\} \log \bigg(\frac{e^{\bm{x}_i^T\bm{w}^t}}{\sum_{l=1}^k e^{\bm{x}_i^T \bm{w}^l}} \bigg) \bigg]
			\end{equation}
			where the indicator function $\mathds{1}\{\cdot\}$ includes in the functional only the correctly classified samples.

			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\subsubsection{Support Vector Machine} \label{sec:svm}
			Support Vector Machine (\ac{SVM}) is a class of powerful ML algorithms that can be written as penalized models and that can used for both regression (\ac{SVR}) and classification (\ac{SVC}) problems~\cite{evgeniou2000regularization}. In the first case, the adopted loss function is Vapnik's $\varepsilon$-insensitive loss:
			\begin{equation} \label{eq:epsilon_insensitive}
				|y - \bm{x}^T\bm{w}|_\varepsilon =
   			\begin{cases}
					0 & \text{if } |y-\bm{x}^T\bm{w}| < \varepsilon\\
					|y-\bm{x}^T\bm{w}| - \varepsilon & \text{otherwise}
				\end{cases}
			\end{equation}
			and in the second case the Hinge loss:
			\begin{equation} \label{eq:hinge_loss}
				|1 - y\,\bm{x}^T\bm{w}|_+ = \max[0, 1 - y\,\bm{x}^T\bm{w}]
			\end{equation}
			as reported in Table~\ref{tab:losses}, and shown in Figures~\ref{fig:regression}~\ref{fig:classification}. The standard formulation of SVM is penalized by the $\ell_2$-norm~\cite{vapnik2013nature}.
			Therefore, sticking to linear models, the SVR minimization problem can be written as
			\begin{equation} \label{eq:svr}
				\bm{\hat w}_{\text{SVR}} = \argmin_{\bm{w} \in \mathbb{R}^d} \frac{1}{n} \sum_{i=1}^n |y_i - f(\bm{x}_i)|_\varepsilon + \lambda \norm{\bm{w}}_2^2
			\end{equation}
			while the SVC minimization problem is
			\begin{equation} \label{eq:svc}
				\bm{\hat w}_{\text{SVC}} = \argmin_{\bm{w} \in \mathbb{R}^d} \frac{1}{n} \sum_{i=1}^n |1 - y_if(\bm{x}_i)|_+ + \lambda \norm{\bm{w}}_2^2
			\end{equation}
			where, as usually, $\lambda$ controls the trade-off between data adherence and smoothing of the solution.
			Equations~\eqref{eq:svr} and~\eqref{eq:svc} are known as the \textit{primal} SVM problem. However, in order to generalize the SVM to nonlinear cases (see Section~\ref{sec:kernel_trick}), it is often convenient to transform this minimization problem in its \textit{dual} form. Several algorithms to find the solution of the SVM minimization problem in both primal and dual forms were developed, such as Newton's method or coordinate descent algorithm. See~\cite{smola2004tutorial, shawe2011review} for an exhaustive review.

			The standard formulation of SVM does not cope well with high-dimensional data, as no sparsity-enforcing penalty is adopted. Recently, $\ell_1$-norm penalized SVM were proposed as well in~\cite{zhu20041, peng2016error}.

			As for the square loss-based methods, let's apply ($\ell_2$-penalized) SVR to the aging problem (introduced in Section~\ref{sec:ols}).
			The experimental setup in this case is the same already adopted for Lasso and ridge regression (see Sections~\ref{sec:the_lasso} and~\ref{sec:ridge_regression}).
			The weights $\bm{\hat w}_{\text{SVR}}$ are represented in Figure~\ref{fig:coefs_svr}. As expected, none of the is exactly zero and they look similar to ridge coefficients in Figure~\ref{fig:coefs_ridge}, as the two model share the same regularization penalty.
			\begin{figure}[!h]
				\centering
				\includegraphics[width=0.8\textwidth]{part1/coefs_svr.png}
				\caption{A pictorial representation of the vector $\bm{\hat w}_{\text{SVR}}$ obtained fitting a SVR model on $74$ randomly selected training samples of $\mathcal{D}_{\text{aging}}$. Variables associated with positive (\ie directly proportional to the output) and a negative (\ie inversely proportional) weight are represented in blue and red, respectively.} \label{fig:coefs_svr}
			\end{figure}
			Evaluating $\bm{\hat w}_{\text{SVR}}$ on the test set, the SVR model has $\text{MAE}=9.280$ and explains the $77.74\%$ of the variance.

			% \begin{enumerate}
			% 	\item loss: hinge
			% 	\item dual problem?
			% 	\item penalties: l2 + l1
			% 	\item use in classification and regression
			% 	\item extension to multiple-class? (not sure)
			% 	\item use cases
			% \end{enumerate}

			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\subsection{The kernel trick} \label{sec:kernel_trick}
			For ease of reading, all the regularization methods presented so far were focused on learning linear input-output relationships $y = \bm{x}^T\bm{w}$. Actually, they can all be extended to the nonlinear case exploiting the so-called \textit{kernel trick}.

			The basic idea of the kernel trick is to use a map that projects the features on a higher (possibly infinite) dimensional space in which the prediction problem is, to some extent, easier to solve. An example of kernel trick for nonlinear classification problem is presented in Figure~\ref{fig:kernel_trick}.

			\begin{figure}[!h]
				\centering
				\subfloat[]{%
					\includegraphics[width=0.5\textwidth]{part1/kernel_trick0.png}
					\label{fig:kernel_trick0}%
				}%
				%\hfill%
				\subfloat[]{%
					\includegraphics[width=0.5\textwidth]{part1/kernel_trick1.png}
					\label{fig:kernel_trick1}%
				}%
				\hfill%
				\subfloat[]{%
					\includegraphics[width=0.5\textwidth]{part1/kernel_trick2.png}
					\label{fig:kernel_trick2}%
				}%
				\caption{Pictorial representation of its kernel trick. Panel (a) shows a 2D classification problem where the input-output relationship is nonlinear. Panel (b) shows ITS \textit{kernel explosion}, \ie the projection of the 2D problem in a higher (3D) dimensional space in which the two classes are linearly separable, as shown in Panel (c).}\label{fig:kernel_trick}
			\end{figure}

			We define the nonlinear feature mapping function as $\phi: \mathcal{X} \rightarrow \mathcal{F}$; for instance $\mathcal{X} \in \mathbb{R}^d$ and $\mathcal{F} \in \mathbb{R}^p$ with $p \geq d$, or even $p \gg d$.
			Given $\phi$, the representer theorem~\cite{smola1998learning}, states that the learning problem of Equation~\eqref{eq:losspen} admits a solution of the form
			$$
			\hat f(\bm{x}) = \sum_{i=1}^n k(\bm{x}_i, \bm{x}_j) \alpha_i
			$$
			where $\bm{\alpha} \in \mathbb{R}^n$ and the function $k$ behaves like an inner product, \ie it is: symmetric, positive definite, ad it can be defined as in Equation~\eqref{eq:kernel_function}.
			\begin{equation} \label{eq:kernel_function}
				k(\bm{x}_i, \bm{x}_j) =  \phi{(\bm{x}_i)}^T \cdot \phi{(\bm{x}_j)} % = \langle \phi{(\bm{x}_i)}, \phi{(\bm{x}_j)} \rangle_{\mathcal{H}}
			\end{equation}
			In the ML literature, $k$ is typically called the \textit{kernel} function and it comes in handy to solve the dual version of the learning problems.

			% Following~\cite{evgeniou2000regularization}, we assume that $\mathcal{F}$ is a Reproducing Kernel Hilbert Space (RKHS) $\mathcal{H}$.
			% Given $\phi$, the nonlinear input-output relationship can be formulated as
			% $$
			% f(\bm{x}) = \phi(\bm{x})^T\bm{w}
			% $$
			% which, nevertheless, is linear in its parameters $\bm{w}$.
      %
			% In this context it is easy to see that every feature map defines the positive definite function $k$ of Equation~\eqref{eq:kernel_function}.
			% \begin{equation} \label{eq:kernel_function}
			% 	k(\bm{x}_i, \bm{x}_j) =  \phi{(\bm{x}_i)}^T \cdot \phi{(\bm{x}_j)} % = \langle \phi{(\bm{x}_i)}, \phi{(\bm{x}_j)} \rangle_{\mathcal{H}}
			% \end{equation}
			% Therefore, for fixed loss function and regularization penalty, a kernel machine can be written as
			% \begin{equation} \label{eq:kernel_learning_problem}
			% 	\hat f = \argmin_{f \in \mathcal{H}} \frac{1}{n} \sum_{i=1}^n L(f(\bm{x}_i), y_i) + \lambda \norm{f}^2_\mathcal{H}
			% \end{equation}
			% where $\norm{\cdot}_\mathcal{H}^2$ is the norm associated by the RKHS.
			% The problem in Equation~\eqref{eq:kernel_learning_problem}, for the
			% can be casted in the so-called \textit{dual} representation in which the outputs are predicted exploiting a \textit{kernel function} defined as in Equation~\eqref{eq:kernel_function}.

			Let's see a practical example: kernel ridge regression. We recall that standard ridge regression problem can be rewritten as
			$$
			\bm{\hat w}_{\ell_2} = \argmin_{\bm{w} \in \mathbb{R}^d} J(\bm{w})
			$$
			where the objective function $J(\bm{w})$ is
			$$
			J(\bm{w}) = \frac{1}{n}\norm{X\bm{w}-\bm{w}}_2^2+\lambda\norm{\bm{w}}_2^2 = \frac{1}{n}(X\bm{w}-\bm{y})^T(X\bm{w}-\bm{y})+\lambda\bm{w}^T\bm{w}
			$$
			Therefore, the solution of the ridge regression problem can be evaluated by imposing $\frac{\partial J(\bm{w})}{\partial \bm{w}}=0$ which leads to the following linear system of equations.
			$$
			X^TX\bm{w}-X^T\bm{y}+\lambda n \bm{w}=0 \Rightarrow (X^TX+\lambda n I)\bm{w} = X^T\bm{y}
			$$
			This can either be solved with respect to $\bm{w}$ (see Section~\ref{sec:ridge_regression}) obtaining the solution of the \textit{primal} form
			\begin{equation} \label{eq:ridge_backup}
				\bm{w} = (X^TX+\lambda n I)^{-1}X^T\bm{y}
		  \end{equation}
			or it can be rewritten as
			$$
			\bm{w} = \frac{1}{\lambda n} X^T(\bm{y}-X\bm{w}) = X^T\bm{\alpha}
			$$
			where $\bm{\alpha} = \frac{1}{\lambda n}(\bm{y} - X\bm{w})$ is the \textit{dual} variable. Therefore, the dual formulation of the problem becomes
			$$
				\begin{aligned}
					\bm{\alpha} = {} & \frac{1}{\lambda n}(\bm{y} - X\bm{w}) \\
						{} & \Rightarrow \lambda n \bm{\alpha} = (\bm{y} - X\bm{w})\\
						{} & \Rightarrow \lambda n \bm{\alpha} = (\bm{y} - XX^T\bm{\alpha})\\
						{} & \Rightarrow (XX^T + \lambda n I) \bm{\alpha} = \bm{y}\\
				\end{aligned}
			$$
			\begin{equation} \label{eq:ridge_dual}
				\Rightarrow \bm{\alpha} = (K + \lambda n I)^{-1}\bm{y} \\
			\end{equation}
			where $K$ is the $n \times n$, symmetric and positive semi-definite kernel matrix, with entries $K_i^j = k(\bm{x}_i, \bm{x}_j)$. In this case $k(\bm{x}_i, \bm{x}_j) = \bm{x}_i^T\bm{x}_j$, and this corresponds to the linear kernel. Nevertheless, several other options are available, see Table~\ref{tab:kernels} or~\cite{bishop2006pattern} for more details. Choosing a different kernel for ridge regression simply boils down to THE choice a different formulation for the kernel function $k$. Plugging the obtained kernel matrix $K$ in Equation~\eqref{eq:ridge_dual} it is possible to achieve the desired solution.

			The advantage of the kernel trick is not only statistical, but also computational. Indeed, we may notice that solving the primal problem (\ie in $\bm{w}$) requires $O(n^3)$ operations, while solving the dual (\ie in $\bm{\alpha}$) requires $O(d^3)$ operations which is better for large scale and relatively low-dimensional problem.

    	 \begin{table}[htb]
				\centering
				\caption{Popular kernel functions. \ac{RBF} stands for Radial Basis Function.}
				\label{tab:kernels}
				\begin{tabular}{@{}ll@{}}
					\toprule
					name       & formulation                                            \\ \midrule
					linear     & $k(\bm{x}, \bm{x}') =\bm{x}^T\bm{x}' $                               \\
					polynomial & $k(\bm{x},\bm{x}') = (\bm{x}^T\bm{x}' + c)^d$    \\
					RBF        & $k(\bm{x},\bm{x}') = e^{-\gamma\norm{\bm{x} - \bm{x}'}^2}$ \\
					sigmoid    & $k(\bm{x},\bm{x}') = \tanh(\gamma \bm{x}^T\bm{x}' + c)$ \\ \bottomrule
				\end{tabular}
			\end{table}

			A similar derivation can also be obtained applying the kernel trick to logistic regression and SVM~\cite{bishop2006pattern, shawe2004kernel}.
			
    	\begin{table}[htb]
			\centering
			\caption{Overview of the matrix norms used for multiple-output regression.}\label{tab:norms}
			\begin{tabular}{l|l|l}
				\toprule
				Matrix norm  & Notation         & Definition                      \\  \midrule
				Frobenius        & $\norm{A}_F$     & $\sqrt{\text{trace}(A^TA)}$     \\ [0.05cm]
				Nuclear          & $\norm{A}_*$     & $\text{trace}\sqrt{(A^TA)}$     \\ [0.05cm]
				Mixed $\text{L}_{2,1}$        & $\norm{A}_{2,1}$ & $\sum_{j}\norm{\bm{a}_j}_2$ \\ \bottomrule
			\end{tabular}
		\end{table}	

		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		\subsection{Decision trees} \label{sec:decision_trees} 
		Decision trees are simple, yet powerful, methods for regression and classification.
		They are based on recursive binary splits of the feature space, in which they fit a very simple model, such as a constant.
		Fitting a decision tree on a given dataset results in partitioning the feature space in \textit{cuboid} regions with edges aligned to the axes.
		In each region, there is a separate model to predict the target variable. In regression problems this can be the average of the samples falling in the region, whereas for classification problems, it can be the assignment to the most represented class.
		In particular, when decision trees are applied to classification problems, the feature space partitioning aims at keeping samples with the same labels grouped together.
		In this thesis, we refer to one of the most popular method for decision tree-based classification and regression, known as \ac{CART}~\cite{breiman1984classification}.
		
		An example of application of this technique for classification and regression is shown in Figure~\ref{fig:decision_trees}.
		
		\begin{figure}[!ht]
			\centering
			\subfloat[]{%
				\includegraphics[width=0.8\textwidth]{part1/decsion_tree_classification.png}
				\label{fig:dt_clf}%
			}%
			\hfill%
			\subfloat[]{%
				\includegraphics[width=0.8\textwidth]{part1/decsion_tree_regression.png}
				\label{fig:dt_rgr}%
			}%
			\caption{Two examples of decision tree applications for: (a) 2D multiclass classification, (b) 1D regression.}\label{fig:decision_trees}
		\end{figure}
		
		Fitting a decision tree implies learning from the data the structure of the tree itself, including the selection of the input variable for each splitting recursion, as well as the optimal splitting threshold. Moreover, the value of the prediction for each region must also be defined.
		
		Let's take into account the usual setting: $\mathcal{D} = \{(\bm{x}_i, y_i)\}_{i=1}^n = (X, \bm{y})$. Even when the number of nodes is fixed, defining at the same time: which variables to split, their splitting threshold and the predictive value is computationally unfeasible as it has a combinatorially large number of possible solutions. Therefore, starting from the root note, \ie the whole input space, and fixing a maximum tree depth, we resort to a greedy approximation.
		
		Let the data at the node $m$ be denoted by $Q_m$, then for each candidate split $\theta=(j, s_m)$, consisting of the $j^{\text{th}}$ feature and the threshold $s_m$, we define two half-planes as follows.
		$$
		Q_{m_1}(\theta) = (\bm{x}_i, y_i) | \bm{x}_i^j \leq s_m~\text{and}~Q_{m_2}(\theta) = (\bm{x}_i, y_i) | \bm{x}_i^j > s_m
		$$
		Then, at each node an impurity measure is evaluated using a suitable impurity function $H(Q_m)$, which depends on the given task. Therefore, we can formulate an objective function like
		$$
		J(Q_m,\theta) = \frac{n_1}{n_m}H(Q_{m_1}(\theta)) + \frac{n_2}{n_m}H(Q_{m_2}(\theta))
		$$
		where $n_1$ and $n_2$ is the number of samples in region $Q_{m_1}$ and $Q_{m_2}$, respectively, while $n_m=n_1+n_2$.
		For each node we then select the best parameter solving the optimization problem in Equation~\eqref{eq:CART}.
		\begin{equation} \label{eq:CART}
			\hat \theta_m = \argmin_{\theta} J(Q_m,\theta)
		\end{equation}
		This procedure is recursively applied to $Q_{m_1}(\hat \theta_m)$ and $Q_{m_2}(\hat \theta_m)$ until the maximum tree depth, that we have fixed at the beginning, is reached.
		
		Dealing with regression problems, a typical impurity measure can be, for instance, the Mean Squared Error (\ac{MSE})
		\begin{equation} \label{eq:MSE}
			H_{\text{MSE}}(Q_m) = \frac{1}{n_m} \sum_{\bm{x}_i \in R_m} \big(y_i - \bar y_m \big)^2
		\end{equation}
		where $\bar y_m = \frac{1}{n_m} \sum_{\bm{x}_i \in R_m} y_i$, is the average output of $Q_m$, \ie the training samples in region $R_m$;
		or the Mean Absolute Error (\ac{MAE})\footnote{ We previously came across this measure when evaluating the performance of learning methods on the aging problem, see Sections~\ref{sec:ols},~\ref{sec:ridge_regression},~\ref{sec:the_lasso}, \etc.}, which is simply defined as below.
		\begin{equation} \label{eq:MAE}
				H_{\text{MAE}}(Q_m) = \frac{1}{n_m} \sum_{\bm{x}_i \in R_m} | y_i - \bar y_m |
		\end{equation}
		On the other hand, dealing with (multiclass) classification problems, where the classes are identified by the index $k=1,\dots,K$, typical impurity measures are, for instance, the Gini criterion
		\begin{equation} \label{eq:gini}
			H_{\text{Gini}}(Q_m) = \sum_{k=1}^K p_{mk}(1-p_{mk})
		\end{equation}
		where $p_{mk}=\frac{1}{n_m} \sum_{\bm{x_i} \in R_m} \mathds{1}(y_i = k)$ is the proportion of class $k$ observation on the region $R_m$; or the Cross-Entropy (\ac{CE})
		\begin{equation} \label{eq:ce}
			H_{\text{CE}}(Q_m) = - \sum_{k=1}^K p_{mk} \log(p_{mk})
		\end{equation}
		which is an impurity measure also heavily applied as loss function for other  learning machines (see Appendix~\ref{appendix:A}).
		
		The main free parameter of this model is the maximum depth of the tree, \ie its size. Deeper trees are capable of modeling complex input-output relationships with respect to shallow ones. Obviously, too deep trees can overfit the training data. Therefore, the optimal depth of the tree should be estimated from the data via, for instance, grid-search cross-validation.
				
		Decision trees are important in biomedical data science applications as they are easily interpretable and capable of modeling nonlinear input-output relationships. Indeed, the prediction is given following a number of binary decisions which, in some cases, may mimic the way doctors perform diagnosis on their patients. Relatively small decision trees can actually be represented graphically. For instance, applying a this method to the aging problem presented in Section~\ref{sec:ols}, we obtain the tree shown in Figure~\ref{fig:tree_graph}. As usually, the main free parameter of the model, \ie the maximum depth of the tree,  is chosen via grid-search cross-validation (and it is $3$, in this case).
		
		\begin{figure}[]
			\centering
			\includegraphics[width=0.8\textwidth]{part1/decsion_tree_graph.png}
			\caption{The graph structure learned from the aging problem (see Section~\ref{sec:ols}). The maximum depth of the tree is $3$ and it is chosen via grid-search cross-validation.} \label{fig:tree_graph}
		\end{figure}
	
		A very well-known weakness of decision trees is their instability. In fact, decision trees are not robust to noise affecting the input data. Even minor perturbations of a single feature may generate a different split which propagates down to all the splits below. In practice, this is alleviated by the use of decision trees as base learners of ensemble strategies (see Section~\ref{sec:ensemble_methods}).
		More details on this learning paradigm and its applications can be found reading Chapter 9 of~\cite{hastie2009elements} or Chapter 14 of~\cite{bishop2006pattern}.

		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	    \subsection{Ensemble methods} \label{sec:ensemble_methods}
	    The key idea of ensemble methods is to build a predictive model by aggregating a collection of multiple \textit{base learners} that are trained to solve the same problem~\citep{zhou2012ensemble}.

	    % FIXME
	    \textit{Bagging} is a common ensemble strategy that consists in fitting multiple models $f_b(\bm{x})$ for $b=1,\dots,B$ each one on a \textit{bootstrap} sampling\footnote{ Random sampling with replacement.} $\mathcal{D}_b$ of the training dataset  $\mathcal{D}=\{\bm{x}_i, y_i\}_{i=1}^n$.
%	    \citep{hastie2009elements}
	    For each sample $\bm{x}_i$, the bagging estimate $\hat{f}(\bm{x}_i)$ is obtained by combining the predictions of the base learners $\hat{f}_b(\bm{x}_i)$. For instance, in case of classification tasks, the bagged model may select the most predicted class casting a vote among the $B$ base learners. On the other hand, for regression problems, the bagging estimate can be a (weighted) mean, or the median, of the predictions of the $B$ base learners. Decision trees (see Section~\ref{sec:decision_trees}) are typical base learner for bagged estimators.

	    % FIXME
	    \textit{Boosting} is another popular ensemble strategy that, unlike bagging, performs predictions by sequentially fitting a collection of base learner that cast a weighted vote~\citep{hastie2009elements}. At each boosting step, the weight corresponding to samples that were misclassified at the previous iterations increases. Therefore, each successive classifier is somewhat forced to learn the relationships between input and output that were previously missed. From a theoretical standpoint, it would be possible to boost any learning machine, nevertheless boosting methods are mainly used with decision trees as base learners~\cite{hastie2009elements}.

		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	    \subsubsection{Random Forests} \label{sec:random_forests}
%	    Decision trees are easily interpretable models that recursively partitions the training data into subsets, based on the test of a single feature value at each split (or node). At each iteration, the feature that yields the best split in terms of a pre-selected metric (Gini impurity, information gain or variance reduction) is chosen to create a new node. Decision trees is that they tend to not perform well in practice, which led to the introduction of random forests in 2001~\cite{breiman2001random}.

	    Random Forests (\ac{RF}) are ensembles of decision trees, proposed by Leo Breiman in the early 2000s~\cite{breiman2001random} to ameliorate the instability of decision trees.
	    In RF each tree is grown on a bootstrap sample from the training data, typically to its maximum depth.
	    To increase robustness to noise and diversity among the trees, each node is split using the best split among a subset of features randomly chosen at that node. The number of features on which the trees are grown is one of the free parameters of the model.
	    The final prediction is made by aggregating the prediction of $M$ trees, either by a majority vote in the case of classification problems, or by averaging predictions in the case of regression problems.
	    Another important free parameter of the model is the number of trees in the ensemble.
	    RF are a bagging approach, which works on the assumption that the variance of individual decision trees can be reduced by averaging trees built on many uncorrelated subsamples.
	    Moreover, increasing the number of trees in the ensemble, RF does not overfit the data. Therefore, this parameter is typically chosen to be \textit{as large as possible}, consistently with the available hardware and computational time.
%	    By contrast, {\it boosted decision trees} are made by building an iterative collection of decision trees, trained by giving more importance to training examples that were incorrectly classified by the previous trees. \todo{Add use cases for boosted decision trees. $\dots$}

	    RF can provide several measures of \textit{feature importance}, computed by looking at the increase in prediction error % (mean decrease in accuracy; mean decrease in node impurity)
	    when data for a feature is permuted while all other features remain unchanged. 
	    Feature selection based on RF if most often performed using one of these measures. However, several techniques for applying regularization to random forests have been proposed. These techniques broadly fall under two categories:
	    \begin{enumerate}
			\item cost-complexity pruning, which consists in limiting tree depth, resulting in less complex models~\cite{ishwaran2008random,kulkarni2012pruning};
			\item Gini index penalization~\cite{deng2013gene, liu2014learning}.
	    \end{enumerate}
    	In addition,~\cite{joly2012ell1} proposed to use an $\ell_1$-norm  to reduce the space-complexity of RF.

	    RF naturally handles numerical, ordinal and categorical variables, multiple scales and non-linearities. They also require little parameter tuning. This makes them popular for the analysis of diverse types of biological data, such as gene expression, Genome-wide Association Studies (\ac{GWAS}) data or mass spectrometry~\cite{qi2012random}.
		In practice, feature selection schemes that rely on RF may to be unstable~\cite{kursa2014robustness}, therefore feature selection stability measures must be adopted to avoid drawing inconsistent conclusions.
		
		Let's see what happens when RF are applied to the aging problem (see Section~\ref{sec:ols}). The experimental setup is identical to the one previously applied for ridge and Lasso regression (see Section~\ref{sec:ridge_regression} and~\ref{sec:the_lasso}), where the only parameter optimized via grid-search cross-validation is the maximum number of features each tree can grow on (and in this case it is chosen as $4$). The number of trees in the ensemble is fixed to $500$. The RF model on the test set achieves $\text{MAE} = 6.321$ and Explained Variance $\text{EV} = 88.17\%$.
		Figure~\ref{fig:rf_feature_importance} shows the feature importance measure estimated by this model.
		
		\begin{figure}[]
			\centering
			\includegraphics[width=0.8\textwidth]{part1/rf_feature_importance.png}
			\caption{A pictorial representation of the feature importance achieved by a RF model with $500$ trees growing on $4$, out of $12$, features.} \label{fig:rf_feature_importance}
		\end{figure}

		As we can see, the features that are most relevant for RF are comparable to the ones selected by Elastic-Net (see Figure~\ref{fig:w_enet}).
		For RF models it is interesting to investigate on the effect of an increasing value for the number of trees in the forest. As shown in Figure~\ref{fig:rf_increasing}, increasing the number of trees in the forest does not lead to overfit. In fact, test error (Figure~\ref{fig:rf_mae}) and explained variance (Figure~\ref{fig:rf_ev}), after approximately $250$ trees, reach a plateau region instead of growing.
		
	    \begin{figure}[]
			\centering
			\subfloat[]{%
				\includegraphics[width=0.8\textwidth]{part1/rf_increasing_MAE.png}
				\label{fig:rf_mae}%
			}%
			\hfill%
			\subfloat[]{%
				\includegraphics[width=0.8\textwidth]{part1/rf_increasing_EV.png}
				\label{fig:rf_ev}%
			}%
			\caption{The effect of the number of trees in a RF model in terms of MAE, panel (a), and EV, panel (b). As expected, larger forests do not lead to overfit, \ie the training error does not reach zero.}\label{fig:rf_increasing}
		\end{figure}

		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	    \subsubsection{Gradient Boosting} \label{sec:gradient_boosting}
	    
	    \textit{Gradient boosting}~\cite{friedman2001greedy} (\ac{GB}) is one of the most widely applied boosting methods in biological problems.
	    This technique iteratively combines the predictions obtained by several base learners, such as decision trees, into a single model.
	    The key idea behind GB is that, under some general hypothesis on the cost function, boosting can be seen as an iterative gradient method for numerical optimization.
	    In particular, in GB at each boosting iteration a new base learner is fitted on the residuals obtained at the previous boosting iteration.
	    GB has several desirable properties~\cite{mayr2014evolution}, such as its capability to learn nonlinear input-output relationship, its ability to embed a feature importance measure (as RF) and its stability in case of high-dimensional data~\cite{buehlmann2006boosting}. When used for feature selection, GB shows interesting performance when casted in the stability selection framework~\cite{meinshausen2010stability}, leading to an effective control of the false discovery rate~\cite{everitt2006cambridge}.

	    As for most of the learning machine, GB may suffer of overfitting. The main regularization parameter to control is the number of boosting iterations $M$, \ie the number of base learners, fitted on the training data. This is typically optimized by cross-validated grid-search, information criteria-based heuristics~\cite{tutz2006generalized, tutz2007boosting} or early stopping~\cite{prechelt1998early}.
	    Regularization in GB can also be controlled by shrinking the contribution of each base learner by a constant $0<\nu<1$ that controls the learning rate of the boosting procedure. In order to achieve comparable predictive power, smaller values of $\nu$ imply larger number of $M$, so there is a trade-off between them.
	    As usually, the base learners are decision trees and another important parameter to tune is their maximum depth~\cite{hastie2009elements}.

	    In a recent paper~\cite{lusa2015boosting}, the authors show that in high-dimensional balanced binary classification problems, if the base learner is likely to overfit the training data, the use of \textit{Stochastic GB}~\cite{friedman2002stochastic} is preferable. The latter is a modified version of the original method, where each base learner is fitted on a random extraction without resubmission of a fraction $\eta$ of the training data, where $\eta$ is another regularization parameter to choose.
	    
	    Let's see what happens when GB is used to tackle the aging problem (see Section~\ref{sec:ols}).
	    The experimental setup is identical to the one previously applied for Elastic-Net regression (see Section~\ref{sec:elastic_net}), where the two parameters optimized via grid-search cross-validation are the maximum depth of the trees (in a range from $3$ to $20$) and the number of boosting iterations (with a maximum value of $500$).
	    The selected tree depth is $8$ and the number of boosting iterations is $31$.
		The GB model on the test set achieves $\text{MAE} = 7.955$ and Explained Variance $\text{EV} = 84.88\%$.
	    Figure~\ref{fig:gb_feature_importance} shows the feature importance measure estimated by this model.
	    As we can see, the features that are most relevant for GB are comparable to the ones relevant for RF (see Figure~\ref{fig:rf_feature_importance}).

	    \begin{figure}[]
	    \centering
	    \includegraphics[width=0.8\textwidth]{part1/gb_feature_importance.png}
    	\caption{A pictorial representation of the feature importance achieved by a GB model with learning rate $0.1$ after $31$ boosting iterations. Each tree has maximum depth of $11$ and it grows on $4$, out of $12$, features.} \label{fig:gb_feature_importance}
	    \end{figure}
   		

	    As already seen for RF, we shall investigate on the effect of increasing boosting iterations for GB. As shown in Figure~\ref{fig:gb_increasing}, after approximately $50$ boosting iterations the model reaches perfect overfit of the training set, \ie zero training error (Figure~\ref{fig:gb_mae}) and $100\%$ trained explained variance (Figure~\ref{fig:gb_ev}).

        \begin{figure}[]
    	\centering
    	\subfloat[]{%
    		\includegraphics[width=0.8\textwidth]{part1/gb_increasing_MAE.png}
    		\label{fig:gb_mae}%
    	}%
    	\hfill%
    	\subfloat[]{%
    		\includegraphics[width=0.8\textwidth]{part1/gb_increasing_EV.png}
    		\label{fig:gb_ev}%
    	}%
    	\caption{The effect of the number of boosting iterations in a GB model (learning rate $0.1$) in terms of MAE, panel (a), and EV, panel (b). As expected, increasingly large boosting iterations lead to perfect overfit, \ie zero training error.}\label{fig:gb_increasing}
	    \end{figure}
	    

%	    Approaches based on gradient boosting classification are used to detect \textit{de novo} mutations showing an improved specificity and sensitivity with respect to state-of-the-art methods~\cite{liu2014gradient}.
%	    When combined with stability selection~\cite{meinshausen2010stability}, gradient boosting has demonstrated to be a very resourceful method for variable selection, leading to an effective control of the false discovery rate. This strategy was followed to associate overall survival with single-nucleotide polymorphisms of patients affected by cutaneous melanoma~\cite{he2016component} and to detect differentially expressed amino acid pathways in autism spectrum disorder patients~\cite{hofner2015controlling}.

%	    \todo{consider adding boosting method for cox models...}

		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	    \subsection{Deep learning} \label{sec:deep_learning}
	    
		Deep Learning (DL) is a branch of ML that, in the recent past, is becoming extremely appealing thanks to the high predictive power that it has empirically shown on real-world problems.
		This section is definitely not an attempt to summarize the heterogeneous plethora of the DL methods presented in literature so far. Indeed, even a dedicated PhD thesis may not be enough to accomplish this task.
		In this section we will only get a grasp on the mechanisms behind the supervised DL methods applied in Part II\footnote{ Unsupervised DL methods were presented in literature as well, but as they are not used in Part II, their discussion is not presented here. We recall to~\cite{chollet2018deep, lecun2015deep} for a more comprehensive overview.}.
	    
	    % I shall not forget that this is Regularization in bio studies, there are plenty of ML in bio studies out there
	    One of the main characteristics of DL methods is that, starting from raw data, they aim at learning a suitable data representation (see Section~\ref{subsec:unsupervised_learning}) and a prediction function, at the same time.
	    DL methods stand on the shoulders of the classical, and shallow, Neural Networks of the $80$s, and they can actually be seen as their extension.
	    In DL the the final prediction is achieved by composing several layers of nonlinear transformations.
	    The intuition behind DL method is that, starting from raw data, their multi-layer architecture can achieve representations at a more abstract level, leading to top performance in prediction tasks.
	    DL architectures can be devised to tackle binary or multiclass classification~\cite{angermueller2016deep, leung2014deep} as well as single or multiple output regression~\cite{Chen2016GeneEI, ma2015deep}.
	    %\todo{microarray gene xpression multi-task regression }

		\subsubsection{Multi-Layer Perceptron} \label{sec:mlp}
		In order to understand the principles guiding DL methods, we sketch here the ideas behind the most basic one: the Multi-Layer Perceptron (\ac{MLP}), also known as deep Feedforward Neural Network.
	    Typically, MLPs are structured as fully connected graphs organized in \textit{layers} that can be of three different types: \textit{input}, \textit{hidden} and \textit{output} (see Figure~\ref{fig:mlp}).
	    Each node of the graph is called \textit{unit}.
	    The number of units in the input layer matches the dimensionality of the raw data ($d$), while number and type of output units are related to the learning task.
	    For multiclass classification problems, with $K$ classes, the output layer has $K$ units, each one representing the probability of the input sample to be assigned to one specific class.
	    On the same line, for multiple-output regression problems, with $K$ tasks, each one of the $K$ units corresponds to the prediction for a given task.
	    Size and number of the hidden layers can be chosen according to the problem at hand and the available computational resources.
	    
   	    \input{part1/nn_tikz.tex}
	    
	    In MLPs the information flows through the graph from the input to the output.
	    Each layer $l$ transforms its input data $\bm{x}^{l-1}$ by composing an affine transformation and an activation function $\sigma(\bm{x})$ that acts element-wise on its input vector.
	    In other words, defining as $p_{l-i}$ the number of units in the layer $l-i$, the layer $l$ applies the transformation
	    $$
	    \bm{x}^{l} = \sigma(\bm{x}^{(l-1)}W_l+b_l)
	    $$
	    where $W_l \in \mathbb{R}^{p_{l-1} \times p_l}$ and $\bm{b} \in \mathbb{R}^{p_l}$ are the weights of the model that are learned from the data.
	    
	    The function $\sigma(\bm{x})$ is known as \textit{activation function} and it can be defined in different ways.
	    In classical neural networks, activation functions are modeled as sigmoids (\eg $\sigma(x)=\tanh(x)$, $\sigma(x)=(1+e^{-x})^{-1}$) whilst in modern DL architectures the most used activation function is the Rectified Linear Unit: $f(x)=\max(0,x)$.
	    % \begin{equation}\label{eq:relu}
	    % 	f(\bm{x})=\begin{cases} 0  &\text{for}~~  x <0 \\ x  &\text{for~}~x \geq 0 \end{cases}
	    % \end{equation}

	    % \todo{output layers + regularization + works}
	    Particular attention must be paid when fitting deep models as they can be prone to overfit the training set~\cite{angermueller2016deep}.
	    The network topology itself defines the \textit{degrees of freedom} of the model: deeper and wider networks can approximate well very complicated input-output relationship, but also the noise affecting the data.
	    Although, tuning the number of hidden layers and their size is not the recommended strategy to prevent from overfitting, as it may lead to suboptimal solutions.

	    Regularization in MLPs can be controlled by penalizing the weights of the network. A common regularization strategy consists in adding an $\ell_2$-norm penalty in the objective function, as in Equation~\eqref{eq:losspen}. In the DL community this procedure is known as weight decay~\cite{krogh1992simple}.
	    Although less common, the $\ell_1$-norm can also be adopted as regularization penalty, as in~\cite{leung2014deep}.
%	    This strategy is adopted in \cite{Chen2015TransspeciesLO} to train a deep architecture on rat cell responses to given stimuli with the final aim to predict human cell responses in the same conditions.
%	    Moreover, weight decay is also adopted in \cite{Yuan2016DeepGeneAA} to train \textit{DeepGene}, \ie an MLP which is designed to classify the tumor type from a set of somatic point mutations.
%	    Furthermore, weight decay is used in~\cite{Fakhry2016DeepMF} to train a DL architecture for brain electron microscopy image segmentation.
	    
	    Training MLPs, and deep networks in general, consists in solving a minimization problem via suitable optimization algorithms~\cite{ruder2016overview}. All these methods iteratively update the weights of the network in order to decrease the training error. Another fundamental regularization strategy is the \textit{Early stopping}~\cite{prechelt1998early}. This consists in interrupting the fitting process as soon as the error on an external validation set increases~\cite{angermueller2016deep}.

	    Another common regularization strategy in DL is \textit{Dropout}~\cite{srivastava2014dropout}. This techniques consists in temporarily deactivating a defined number of randomly chosen units of the network at training phase. This reduces the degrees of freedom of the model and it implicitly allows to achieve an ensemble of several smaller networks whose predictions are combined.
	    The use of dropout alone can improve the generalization properties, as in~\cite{Chen2016GeneEI}, but it is usually adopted in combination with weight decay or other forms of regularization, as in~\cite{leung2014deep}.
	    
	    Training MLPs implies dealing with non-convex optimization problems which are usually tackled via stochastic gradient descend, or similar methods~\cite{ruder2016overview, sra2012optimization}.
%	    , where the authors propose to use a deep network to predict splicing patterns.
%	    where the authors propose a \textit{D-GEX}, DL regression architecture trained to predict the expression of a number of target genes. Dropout

		MLP architectures are also used as final layers of another class of DL methods called: Convolutional Neural Network (\ac{CNN}).
		CNN are a different class of DL methods that exploit convolutional masks with weights estimated from the data to learn a suitable feature representation. This class of methods is largely applied to computer vision and image recognition problems.
		Describing CNN architectures is beyond the scope of this thesis, as they are not mentioned in the Part II. A thorough description of CNN can be found here~\cite{chollet2018deep, goodfellow2016deep}.

		Let's see what happens when a relatively simple MLP is applied to the aging problem (see Section~\ref{sec:ols}). The experimental setup is the same used for RF in Section~\ref{sec:random_forests}, where the only parameter optimized via grid-search cross-validation is the weight decay constant.
		The resulting MLP has $1$ hidden layer with $1024$ units, ReLu activation and constant weight decay $0.001$.
		Such MLP is trained with early stopping and, after $329$ iterations reaches $\text{MAE} = 9.14$ explaining the $78.84\%$ of the variance of the test set.
		The relatively worse performance of this method, compared, for instance, to RF, may be due to the fact that this dataset has a reduced number of samples. It is known that training DL models from scratch, in order to reach optimal solutions, it is necessary to have more training samples than conventional (shallow) learning machines.
		
		\subsubsection{Long-Short Term Memory Network} \label{sec:lstm}
		Long-Short Term Memory Network (LSTM) belongs to the class of Recurrent Neural Network (\ac{RNN})~\cite{goodfellow2016deep}.
		LSTM use special units, called {\sl memory cells}, which have the ability to learn long-term dependencies \cite{lecun2015deep}. 
		Instead of just applying an element-wise nonlinear function, such as sigmoid or hyperbolic tangent, to the affine transformation of inputs and recurrent units,
		LSTM cells have an internal self-loop together with a system of {\sl input}, {\sl output} and {\sl forget gates}~\cite{goodfellow2016deep}.
		Compared to standard RNNs, LSTMs can be trained via gradient descent as they do not suffer from the vanishing/exploding gradient problem~\cite{bengio1994learning}.
		In the recent past, this deep learning architectures demonstrated to achieve promising results in several sequences learning tasks, such as speech recognition~\cite{graves2005framewise}, image captioning~\cite{vinyals2015show}, or time-series forecast~\cite{schmidhuber2005evolino}. 
		
  \section{Feature selection} \label{subsec:feature_selection}
  
%  The process of modeling complex systems often implies collecting large amount of data in the field of life science, where large, multivariate and noisy measurements are typically acquired with the aim of describing multifactorial diseases.
%  
%  In the era of personalized medicine, biospecimen collection and biological data management is still a challenging and expensive task~\cite{toga2015sharing}. Only few large-scale research enterprises, such as ENCODE~\cite{encode2004encode},  ADNI~\cite{jack2008alzheimer},  MOPED~\cite{kolker2012moped} or TCGA~\footnote{ Source \url{https://cancergenome.nih.gov}.}
%  , have sufficient financial and human resources to manage, share and distribute access of heterogeneous types of biological data. To date, many biomedical studies still rely on a small number of collected samples~\cite{mcneish2016effect, button2013power, yu2013shrinkage}. This effect is even worse in case of rare diseases~\cite{Garg2016ABM} or in high-throughput molecular data (\eg genomics and proteomics) where the dimensionality of the problem can be in the order of hundreds of thousands.
	Dealing with real-world data science challenges it is very often the case that the number of acquired variables $d$ is high. It may even happen that $d$ heavily outnumbers the number of collected samples $n$. This setting is usually referred to as \textit{large d small n} scenario, or simply $n \ll d$. Practical examples of this case can be easily found in the context of applied life sciences, \eg gene expression, DNA sequencing, proteomics, spectroscopy, medical imaging and so on.
	
	In such cases, the main goal of the analysis is often to identify a meaningful subset of relevant variables that are the most representative of the observed phenomenon. In ML, this is known as variable/feature selection and several techniques addressing this task were presented so far~\cite{guyon2002gene}. Variable selection does not only increase the prediction power of the learning machine, but it also promotes model interpretability, that is crucial in biology~\cite{altmann2010permutation}.

	Variable selection techniques are traditionally organized in three groups: filter, wrapper and embedded methods~\cite{guyon2003introduction}.

	\begin{itemize}
		
		\item[] \textbf{Filters}
		In these techniques, \textit{irrelevant} features are filtered out from the feature set before any learning step.
%		Indeed, filter methods may suffer from selection bias, if the filtering is performed before splitting the data, and it does not guarantee that the identified feature subset is optimal with respect to the learning problem.
		A fair majority of filter approaches is based on some variable ranking criteria that are usually based on the definition of a scoring function $F(\bm{x}^{j}, \bm{y})$, where $\bm{x}^{j}$ is the $j-th$ variable and $\bm{y}$ is the output vector. Usually high values of the scoring function correspond to highly relevant features. Once the scoring function is evaluated for each feature, the entire feature set is sorted by means of their score. Varying the selection threshold one may generate different set of nested features. The selection of one of such subsets can be done after an actual learning step, \ie when a reliable estimate of the prediction error is available.
		Variable selection techniques that belong to this class may be based on some statistical scores, \eg t-test, ANOVA, Pearson correlation, or some entropy measures, \eg the mutual information~\cite{everitt2006cambridge}. Statistical score-based algorithms may fail when the assumptions on the probability distribution function does not hold for the available data. On the same line, information theoretic techniques should need the probability distribution of the data, which is usually unknown.
%		In practical cases, an estimate of the probability distribution is estimated by means of frequency counts.
%		Such empirical estimates are, of course, weak in case of small $n$ and their computation is harder with several classes and variable values ($X$).
%		This last observation underlines also how entropy-based criteria are more suitable for classification problem than for regression ones and, above all, how ineffective they could be on high-dimensional data.\\
		
		Single variable classifiers is another class of methods that, to some extent, may fall into this family. They still define a scoring function $F$, but such function is a measure of the predictive power of each features. For instance, in a classification scenario one may select the list of variables that, individually, have best accuracy, or false positive or false negative classification rate. Moreover, in case of regression problems best residual sum of squares or $\text{R}^2$ score may be used  (see Section~\ref{sec:performance_metrics}). A major drawback of this class of methods is that features that are not relevant by themselves will be discarded even if they would have been relevant used in combination with some other ones.
		
		\item[] \textbf{Wrappers}
		The key idea behind the algorithms of this second family is that the variable selection process should not only depend on input and outputs, but also on a specified learning algorithm. These algorithms select a feature subset according to the prediction performance achieved by some learning algorithms evaluated on subsamples of the original feature set. ML methods are here used as black boxes that, given some input, simply return a prediction performance score.
		Exhaustive search over the whole space of all possible features subset, that has cardinality $2^d$, is clearly unfeasible even in low dimension. In order to cope with this problem several \textit{greedy} techniques have been proposed. Among them, the most popular algorithms are the \textit{matching pursuit} (also known as forward stage-wise selection)~\cite{cai2011orthogonal} and the Recursive Feature Elimination scheme (RFE)~\cite{guyon2002gene}.
		While the first starts from an empty vector and greedily incorporate the most relevant features, RFE starts with the whole feature set and then iteratively exclude the least promising variables.
		
		\item[] \textbf{Embedded}
		The main characteristic of these methods is that they can learn and perform variable selection simultaneously.
%		In other words, the variable selection is here embedded in the learning machine.
		Embedded variable selection can be achieved by two families of learning machines: tree-based and regularization methods.
		Decision trees can be used for variable selection as only some of the input variables are used to partition the feature space and variables not involved in any split are can be discarded, see Section~\ref{sec:decision_trees}. Moreover, ensemble of decision trees natively offer a feature importance measure~\ref{sec:ensemble_methods}.
		On the other hand, regularization methods as extensively discussed in Section~\ref{subsec:regularization_methods}, can easily be used for variable selection by exploiting sparsity enforcing penalties. Only the variables having nonzero weight will be considered as selected by the model.
		Regardless of the learning machine, regularization can be introduced in several ways and it is of fundamental use in order to achieve the following desired properties~\citep{okser2014regularized}:
		%
		\begin{itemize}
			\item identify models with good generalization properties, even with a limited amount of collected samples;
			\item achieve solutions that are robust to noise;
			\item learn the data structure when unknown;
			\item exploit prior knowledge on the data structure;
%			\item promote interpretability reducing the number of variables in the model;
			\item reduce the feasible set in order to help solving inverse problems.
		\end{itemize}
		
	\end{itemize}

%	\todo{discuss a bit the variable selection stability}
	Assessing the stability of the selected variables is a major issue that is typically faced when performing variable selection on high-dimensional problems. This observation is even more evident in $n \ll d$ cases. Classical variable selection strategies, \eg the Lasso (see Section~\ref{sec:the_lasso}), are sensitive to random sample extraction.
	This reflects in having a, possibly, very different list of selected variables each time the algorithm is evaluated on a cross-validation training split of the data.
	Variable selection stability is widely studied in literature and different strategies to achieve stable variable selection were proposed in literature~\cite{kuncheva2007stability, de2009elastic, hofner2015controlling, nogueira2016measuring}.
	
	In particular, in~\cite{meinshausen2010stability} the authors propose the so-called \textit{stability selection} framework.
	For a given variable selection strategy (let's think about the Lasso, for simplicity) this can be seen as an extension of standard regularization paths (see Section~\ref{subsec:regularization_methods}). For increasing values of the free parameter of the chosen variable selection strategy (\eg the regularization parameter $\lambda$), a \textit{selection probability} is estimated for each variable. Such probability is estimated by evaluating the variable selection algorithm on several random extractions of a portion of the training set. This produces the so-called \textit{stability path}.
	A stable list of selected variables can therefore be obtained by including the variables that, for a given value of the free parameter, are selected with high probability.
	As shown in the original paper, this framework is not heavily influenced by the values of free parameter and probability threshold. However, they can be chosen by validation on an external validation set, which is fundamental in order to avoid selection bias~\cite{ambroise2002selection}. Similar strategies are also explored in~\cite{de2009regularized, barbieri16palladio}
	

  \section{Unsupervised learning} \label{subsec:unsupervised_learning}
  In Section~\ref{subsec:supervised_learning} we started the discussion of supervised learning methods with a metaphorical comparison with the human ability of learning from examples.
  Actually, in order to understand concepts, human beings need way less supervision than modern learning algorithms.
  In some circumstances it may be even the case that little/no supervision at all is needed to accomplish a task.

  For instance, let's say that a kid growing with a Brittany dog at home visit some friend having a Bernese Mountain Dog.
  The kid, so far, has only being taught what his dog looks like and these two dog breeds definitely look different one another.
  Nevertheless, the kid will certainly be able to understand that both of them are animals and, in particular, dogs.
  Who taught the kid the appearance of every possible dog breed?
  
  The learning paradigm in which we try to infer some interesting properties, or recurrent patterns from unlabeled samples is known as unsupervised learning and,
  quoting Kevin P. Murphy, "\textit{Unsupervised learning is arguably more typical of human and animal learning}."~\cite{murphy2012machine}.
  
  In this section, given a set of input data $\mathcal{D}=\{\bm{x}_i\}_{i=1}^n$, with samples represented as $d$-dimensional vectors $\bm{x}_i \in \mathbb{R}^d$ ($\forall i=1,\dots,n$), our interest will be directed toward two different goals:
  \begin{enumerate*}[label=(\roman*)]
  	\item grouping similar objects together (Section~\ref{sec:clustering}) and
  	\item achieving a meaningful low-dimensional representation of our data (Section~\ref{sec:dimred}).
  \end{enumerate*}
  In both cases we will try to infer the properties of the probability distribution $p(\bm{x})$ from which the collected data are assumed to be drawn.
  
    \subsection{Cluster analysis} \label{sec:clustering}
    \textit{Cluster analysis}, \aka \textit{data segmentation} or \textit{clustering}, is the class of unsupervised learning algorithm which aim at grouping together similar objects, by some definition of \textit{similarity}.
        
    This task is fairly common in biomedical studies, where cluster analysis can be used, for instance, to perform patients stratification or to find groups of correlated biological measures, such as gene expression, blood exams, and so on. This section presents an overview on the most popular clustering algorithms\footnote{ A good reference for the clustering algorithms proposed in literature is \textit{Section 2.3. - Clustering} of the \sklearn documentation \url{http://scikit-learn.org/stable/modules/clustering}.}.

    \subsubsection{The $k$-means algorithm} \label{sec:k-means}
    $k$-means is the most intuitive strategy for cluster analysis.
    
%    It is basically what $k$-nearest neighbor and OLS are for supervised classification and regression, respectively.
	Given $n$ samples $\bm{x}_i \in \mathbb{R}^d$, the goal of $k$-means is to partition the data space into $K$ non-overlapping groups containing similar samples.
    Let $\bm{\mu}_k \in \mathbb{R}^d$ be the \textit{prototypical} vector associated with the $k^{\text{th}}$ cluster, we can think of this as the center of mass of the cluster, \ie its centroid.
    Fixing the number of cluster $K$, the output of the $k$-means algorithm is a set of cluster centroids as well as the assignment of each sample to a single, \ie the closest, cluster.
    For ease of notation, we introduce a binary indicator function $r_k$ defined as
    $$
 	r_k(\bm{x}_i)=\begin{cases} 1  &\text{if}~\bm{x}_i~\text{is assigned to cluster}~k \\ 0  & \text{otherwise} \end{cases}
    $$
    which is known as 1-of-$K$ encoding. Then, we can define the objective function
    $$
    J(\bm{\mu}) = \sum_{i=1}^n\sum_{k=1}^K r_k(\bm{x}_i) d(\bm{x}_i, \bm{\mu}_k)
    $$
    where $d(a,b)$ is some distance measure. In case $d(a,b)$ is the squared Euclidean distance, which is the typical choice for $k$-means, the objective function can be rewritten as in Equation~\eqref{eq:kmeans}.
    \begin{equation} \label{eq:kmeans}
    J(\bm{\mu}) = \sum_{i=1}^n\sum_{k=1}^K r_k(\bm{x}_i) \norm{\bm{x}_i- \bm{\mu}_k}_2^2
    \end{equation}
    Our goal is to find the centroids $\bm{\mu}_k$ for $k=1,\dots,K$ that minimize $J(\bm{\mu})$.
    We can argue that, given the best centroids, the samples can be simply associated to the cluster with the nearest centroid. So, let's focus on the problem of finding $\bm{\mu}_k$.
    Fixing $r_k(\bm{x})$ for all $k$, the objective function in Equation~\eqref{eq:kmeans} is quadratic in $\bm{\mu}$, thus the minimization problem can be easily solved setting its derivative to zero, see Equation~\eqref{eq:kmeans_solved}.
    \begin{equation} \label{eq:kmeans_solved}
	2\sum_{i=1}^n r_k(\bm{x}_i)(\bm{x}_i - \bm{\mu}_k) = 0 \Rightarrow \bm{\mu}_k = \frac{\sum_{i=1}^n r_k(\bm{x}_i)\bm{x}_i}{\sum_{i=1}^nr_k(\bm{x}_i)}
    \end{equation}
    Intuitively, $\bm{\mu}_k$ corresponds to the mean of the points assigned to the cluster $k$.
    
    The $k$-means algorithm starts with random centroids. Then the points are assigned to the cluster having the nearest centroid. Successively, the centroids position is updated. At each step the value of $J(\bm{\mu})$ is reduced, hence the convergence of $k$-means is assured. Nevertheless, the algorithm may converge to a local minimum, achieving a suboptimal solution.
    
    Defining an optimal value of $K$ for any dataset is still an open problem. The $k$-means algorithm works with a number of cluster fixed a priori\footnote{ Some relevant work on automatic cluster discovery is presented in~\cite{ball1967clustering, pelleg2000x, muhr2009automatic}.}.
    $K$ can be given from some prior knowledge on the problem, or it can be estimated following some heuristics. Otherwise, an estimate of $K$ can be defined by optimizing data-driven coefficients like the silhouette score (see Section~\ref{sec:performance_metrics}), or some information criteria, such as \ac{AIC} or \ac{BIC}~\cite{bishop2006pattern}.
    These estimates can be done with or without cross-validation~\cite{fiorini2017adenine}.
    
    As shown in~\cite{arthur2007k}, smart choices of the initial position of the clusters can substantially improve the result. For instance, the initial $\bm{\mu}_k$, for $k=1,\dots,K$, can be chosen to be \textit{far away} from each other.
    This solution is called $k$-means$++$ and it follows the steps summarized below.
%    \begin{enumerate}
%    	\item choose the first centroid $\bm{\mu}_1$ at random from the samples in the dataset;
%    	\item compute the distance between $\bm{\mu}_1$ and all the other points in the dataset: $D = d(\bm{x}_i, \bm{\mu}_1)$ $\forall i=1,\dots,n$;
%    	\item choose a new data point as centroid $\bm{\mu}_2$ following a weighted probability distribution $\propto~D^2$;
%    	\item update the distance vector as $D = \min[d(\bm{x}_i, \bm{\mu}_1), d(\bm{x}_i, \bm{\mu}_2)]$ $\forall i=1,\dots,n$;
%    	\item repeat steps 3 and 4 until $k$ centroids are defined;
%    	\item use this centroids initialization to run standard $k$-means.
%    \end{enumerate}

	\begin{algorithm}[h!]
		\begin{algorithmic}[1]
			\State choose the first centroid $\bm{\mu}_1$ at random from the samples in the dataset;
			\State compute the distance between $\bm{\mu}_1$ and all the other points in the dataset: $D = d(\bm{x}_i, \bm{\mu}_1)$ $\forall i=1,\dots,n$;
			\State choose a new data point as centroid $\bm{\mu}_2$ following a weighted probability distribution $\propto~D^2$;
			\State update the distance vector as $D = \min[d(\bm{x}_i, \bm{\mu}_1), d(\bm{x}_i, \bm{\mu}_2)]$ $\forall i=1,\dots,n$;
			\State repeat steps 3 and 4 until $k$ centroids are defined;
			\State use this centroids initialization to run standard $k$-means.
		\end{algorithmic}
	\end{algorithm}


	Even though this initial seeding comes with an increased computational time at the beginning, then the $k$-means algorithm converges super fast, leading to a decreased overall computational time. Moreover, the achieved solution has considerably better final cluster reconstruction, compared to standard $k$-means, hence $k$-means$++$ initialization is usually preferable.
    
    \subsubsection{Spectral clustering} \label{sec:spectral_clustering}
	Spectral clustering is a popular clustering algorithm initially proposed in~\cite{shi2000normalized} and then further analyzed and explained in~\cite{ng2002spectral, von2007tutorial}.
	Assuming the usual setup, the problem of clustering can be translated in the problem of inferring a \textit{similarity graph} $\mathcal{G} = (V, E)$, where each data point $\bm{x}_i$ corresponds to a vertex and the weight $s_{ij}$ of the edge connecting $\bm{x}_i$ and $\bm{x}_j$, is proportional to the similarity between the two points.
	For instance, in the so-called $\varepsilon$-neighborhood graph, the vertexes corresponding to $\bm{x}_i$ and $\bm{x}_j$ are connected if $s(\bm{x}_i, \bm{x}_j) > \varepsilon$, for a given $\varepsilon$.
	Our hope is then to achieve a graph structure where the edges between points of the same group is high and it is low between points belonging to different groups.

	Therefore, let's consider a set of points $\mathcal{D}=\{\bm{x}_i\}_{i=1}^n$, the steps of the unnormalized spectral algorithm are summarized as follows\footnote{ Normalized version this algorithm is identical but the formulation of $L$ which is substituted by the normalized graph Laplacian~\cite{von2007tutorial}.}.
	\begin{algorithm}[h!]
	\begin{algorithmic}[1]
		\State fix a symmetric and non-negative similarity function $s(\bm{x}_i, \bm{x}_j)$ and compute the similarity matrix $S=
		\begin{bmatrix}
		1       & \dots & s_{1n} \\
		\vdots & \ddots & \vdots \\
		s_{1n}       & \dots & 1
		\end{bmatrix}
		$;
		\State construct a similarity graph $\mathcal{G}$ from $S$, let $W$ be its adjacency matrix and $D$ its degree matrix;
		\State compute the unnormalized graph Laplacian $L = D - W$;
		\State compute the first $k$ eigenvectors $\bm{v}_1, \dots, \bm{v}_k$ of $L$ and organize them column-wise in the matrix $V \in \mathbb{R}^{n \times k}$;
		\State for $i=1,\dots,n$ let $\bm{\psi}_i \in \mathbb{R}^k$ be the vector corresponding to the $i^{\text{th}}$ row of $V$;
		\State cluster the points $\{\bm{\psi}_i\}_{i=1}^n$ using $k$-means into $K$ clusters.
	\end{algorithmic}
	\end{algorithm}
	
	As for the $k$-means algorithm, finding $K$ is a difficult problem. A common solution for spectral clustering is the \textit{eigengap heuristics}, described in~\cite{von2007tutorial}.
    
    \subsubsection{Hierarchical clustering} \label{sec:hierarchical_clustering}
	As suggested by the name, hierarchical clustering techniques produces hierarchical (tree) representation of a given dataset, where each level of the hierarchy consist in groups of samples (clusters). Individual data points can be found at the leaves of the learned tree, while all the data points are grouped in a single cluster at the root level.
	
	There are two main families of hierarchical clustering: the \textit{agglomerative} (bottom-up) and the \textit{divisive} (top-down) approaches.
	Both of them are iterative techniques. The first, at each iteration, groups together the most similar points, hence creating more and more large clusters, while the divisive strategy works the other way around: by iteratively dividing groups of points. From now on we will refer to agglomerative clustering only, but similar considerations can be followed for the divisive approach as well.
	
	Hierarchical clustering techniques have two main degrees of freedom: the choice of point-wise and group-wise similarity, \aka \textit{affinity} and \textit{linkage}, respectively. The first is used to compute the initial value of the similarity matrix
	$$
	S_0 = \begin{bmatrix}
	0       & \dots & s_{1n} \\
	\vdots & \ddots & \vdots \\
	s_{1n}       & \dots & 0
	\end{bmatrix}
	$$
	while the second is used to update it once the two most similar data points are collapsed into a single cluster $S_0 \in \mathbb{R}_+^{n \times n} \rightarrow S_1 \in \mathbb{R}_+^{n-1\,\times\,n-1}$.
	While any symmetric and non-negative function is a valid affinity, the choices of for linkage function are more interesting and worth mentioning.
	Let $G$ and $H$ be two clusters, there are four main linkage options:
	\begin{enumerate}
		\item[] \textbf{Single} $~~~~~~~~l(G,H) = \max_{\substack{\bm{x}_i \in G \\ \bm{x}_j \in H}} s(\bm{x}_i, \bm{x}_j)$

		\item[] \textbf{Complete} $~~l(G,H) = \min_{\substack{\bm{x}_i \in G \\ \bm{x}_j \in H}} s(\bm{x}_i, \bm{x}_j)$
		
		\item[] \textbf{Average} $~~~~l(G,H) = \frac{1}{n_G n_H} \sum_{\bm{x}_i \in G} \sum_{\bm{x}_j \in H} s(\bm{x}_i, \bm{x}_j)$
		
		\item[] \textbf{Ward} $~~~~~~~~l(G,H) = \frac{n_G n_H}{n_G + n_H} \norm{\bm{\mu}_G - \bm{\mu}_K}^2$
	\end{enumerate}
	where $n_A$ and $\bm{\mu}_A$ are the number of points and the centroid of the cluster $A$, respectively.
	
	Interestingly, finding the number of clusters $K$ translates here into the problem of pruning the tree.
    
    \subsection{Dimensionality reduction and feature learning} \label{sec:dimred}
	Most of the ML methods aim at learning some hidden relationship in a set of data.
	Typically, the dataset at hand is a collection of $d$ measures from $n$ samples $\bm{x}_i \in \mathcal{X} \in \mathbb{R}^d$ and we want our ML algorithms to crunch such data in order to perform some learning task, \eg classification, regression, clustering and so on.
	
	It is often the case that the feature space $\mathcal{X}$ is not the most suitable to our purposes. Sometimes projecting the data in a different space $\mathcal{X}' \in \mathbb{R}^p$, maybe with $p<d$ or even $p \ll d$, can greatly ease the further learning steps. We have already explored a similar concept talking about the kernel trick in Section~\ref{sec:kernel_trick}.
	
	In particular, heavily decreasing the dimensionality of the data is helpful as it achieves (lossy) data compression, reducing the computational time of further analyses. Moreover, dimensionality reduction eases data visualization and understanding. These aspects are really important when dealing with large scale biomedical data.
	
	This section presents a compact overview of the dimensionality reduction and unsupervised feature learning algorithms that are most relevant with the biomedical data science projects presented in the second part of this thesis.
	
	\subsubsection{Principal component analysis} \label{sec:pca}
	
	Principal Component Analysis (\ac{PCA})~\cite{jolliffe2002principal} is probably the most popular dimensionality reduction technique. PCA is typically defined as linear projection of the data onto a lower dimensional space where the variance is maximized.
	
	In order to understand PCA in general, we first focus on how to find the first principal component, \ie the main direction in which the data shall be projected in order to preserve as much variance as possible. Let $\bm{u}_1$ be the $d$ dimensional vector which identifies the direction of the first principal component. As we are not interested in the magnitude of this vector, but rather in its direction, without loss of generality we can assume $\bm{u}_1^T\bm{u}_1 = 1$.
	
	The projection of each data point $\bm{x}_i$ on the direction identified by $\bm{u}_1$ can easily be found as $\bm{x}_i' = \bm{u}_1^T\bm{x}_i$. We shall notice that the variance of the projected data can be expressed as
	$$
	\sigma^2_{\bm{x}_i'} = \frac{1}{n} \sum_{i=1}^n \big[\bm{u}_1^T\bm{x}_i - \bm{u}_1^T\bar{x}\big]^2 = \bm{u}_1^T S \bm{u}_1
	$$
	where $\bar{x} = \frac{1}{n} \sum_{i=1}^n \bm{x}_i$ is the empirical mean of the samples and $S$ is the data covariance matrix, defined as $S = \frac{1}{n} \sum_{i=1}^n (\bm{x}_i - \bar{x})(\bm{x}_i - \bar{x})^T$. Therefore, finding $\bm{u}_1$ can be casted in the following optimization problem
	$$
	\max_{\bm{u} \in \mathbb{R}^d} \big[ \bm{u}_1^T S \bm{u}_1 + \lambda_1 (1 - \bm{u}_1^T  \bm{u}_1) \big]
	$$
	where $\lambda_1$ is a Lagrangian multiplier. By setting the derivative with respect to $\bm{u}_1$ equal to zero we can easily write
	$$
	S \bm{u}_1 = \lambda_1 \bm{u}_1 \Rightarrow \bm{u}_1^T S \bm{u}_1 = \lambda_1
	$$
	which means that the variance is maximum when $\bm{u}_1$ is the eigenvector of $S$ with maximum eigenvalue $\lambda_1$. The eigenvector $\bm{u}_1$ is hence known as the principal components.
	
	Additional components can be iteratively found by choosing new directions, orthogonal to the ones already identified, that maximize the projected variance. %\todo{use cases?}
	
	\subsubsection{Multi-dimensional scaling} \label{sec:mds}

	Multi-dimensional scaling (\ac{MDS})~\cite{borg2005modern} is a dimensionality reduction technique which aims at finding a low-dimensional projection that maximally preserves the pairwise distance between data points.
		
	Adopting the Euclidean distance metric, MDS is similar to PCA. However, the MDS algorithm can be extended to a wide range of similarity measures. %\todo{use cases?}
	
	\subsubsection{Isomap} \label{sec:isomap}
	
	Isomap~\cite{tenenbaum2000global} is a non-linear dimensionality reduction technique that can be seen as an extension of MDS.
	
	In this algorithm, the similarity between the data points are evaluated as geodesic distances along some manifold. At first the algorithm finds the $k$ nearest neighbors of each sample. Then it builds a graph where each sample is a node and the edges of neighboring nodes are weighted with the Euclidean distance of the corresponding samples. Finally, the geodesic distance is approximated by summing the weights of the edges corresponding to the shortest path between each pair of points. The final low-dimensional projection is obtained performing MDS on such distance matrix. %\todo{use cases?}
	
	
	\subsubsection{t-Distributed Stochastic Neighbor Embedding} \label{sec:tsne}
	
	t-Distributed Stochastic Neighbor Embedding	(\ac{t-SNE})~\cite{van2008visualizing} is a relatively recently proposed nonlinear dimensionality reduction technique specifically developed for high-dimensional data visualization. t-SNE aims at learning a low-dimensional data projection that reflects the original data similarities as much as possible.
	
	The algorithm has two main steps. At first t-SNE devises pairwise probability distributions over high-dimensional samples such that the similar ones are picked together with high probability. Then, the algorithm defines another probability distribution on a low-dimensional map identified by minimizing the KullbackLeibler divergence between the two distributions. The minimization problem is solved by gradient descent. %\todo{use cases?}
    

  \section{Model selection and evaluation} \label{subsec:model_selection}
  
  The previous chapters present an overview of ML techniques and algorithms that are most relevant with the data science challenges presented in the second part of this PhD thesis.
  The described algorithms are very different for both goals and mathematical structure.
  However, they all share a common trait: the presence of one, or more, free parameters.
  These free parameters must be specified before the actual learning step and, in ML literature, they are usually referred to as \textit{hyperparameters}\footnote{ This term is inherited from the filed of statistics, where an hyperparameter is a parameter of a prior distribution probability. This sometimes generates confusion and possible misunderstanding in technical literature.}.
  As opposed to the model parameters, which are learned from data, hyperparameters are tuning knobs that must be user-defined.
  
  In order to better understand this difference, let's see a practical example: a regularized least square approach (such as Ridge or Lasso).
  In this case, the weights of the input variables $w_j$ (for $j=1,\dots,d$) are learned from training data and they are the model \textit{parameters}; whereas the regularization parameter $\lambda$ is the only hyperparameter of the model.
  This simple example has only one hyperparameter, but more complicated models, such as gradient boosting or deep architectures (see Section~\ref{sec:deep_learning} and~\ref{sec:gradient_boosting}), present way more hyperparameters to tune (\eg depth and size of the tree, learning rate, number of layers, \etc).
  
  In order to successfully carry out real-world data science challenges, ML practitioners need two key ingredients:
  \begin{enumerate*}[label=(\roman*)]
  	\item reliable model assessment strategies and %prediction performance evaluation strategies and
  	\item robust protocols for model selection.
  \end{enumerate*}
  The first point often boils down to the use of a robust cross-validation technique to estimate the generalization error~\cite{molinaro2005prediction}, whilst the second is typically tackled with (exhaustive) search over a grid of possible values. 
  In ML jargon, each different choice of an hyperparameter set corresponds a different model. The process of finding the best performing set of hyperparameters is typically called model selection.

  \subsection{Model assessment strategies} \label{sec:model_evaluation}
  % cross validation flavours
  The performance of a ML model can be assessed by estimating its generalization error, that is a measure of the performance the model is expected to achieve on future data.
  As this is, in general, not possible, the predictive power of a given learning machine should be evaluated on previously unseen data, simulating future observation.
  Resampling protocols are the most popular class of techniques to simulate the acquisition of future data~\cite{molinaro2005prediction} and these methods can be successfully applied to estimate the generalization error.
  
  Moreover, particular attention should be paid when the data analysis pipeline embeds a variable selection step. In fact, as shown in~\cite{ambroise2002selection}, an error estimated on the same set of data used for variable selection can be heavily affected by \textit{selection bias}.
  This form of overfitting leads to an overoptimistic estimate of the model performance.
  Hence, in order to avoid too optimistic estimations of the generalization error, ML practitioners should rely on nested resampling protocols, as shown for instance in~\cite{de2009regularized}.
  
  This section briefly presents the cross-validation resampling routines that are most relevant for the second part of this PhD thesis.
  
  \begin{itemize}
  	\item[] \textbf{Monte Carlo cross-validation}
  	This strategy repeatedly splits the $n$ samples of the dataset in two mutually exclusive sets. For each split, $n\cdot(1/\nu)$ samples are labeled as \textit{validation} set and the remaining $n\cdot(1 - 1/\nu)$ as \textit{training} set. The data points of the two sets are randomly sampled without replacement from the entire dataset.
  	At each repetition, the learning machine
  	is fitted on the training set and its predictive power is assessed by evaluating a performance metric (see Section~\ref{sec:performance_metrics}) on the independent test set. If needed, a variable selection step can be part of the training phase. Monte Carlo cross-validation gives the opportunity to assess not only the value of the generalization error, that can be obtained, for instance, averaging across iterations, but also to infer its probability distribution. However, a major drawback of this method is its computational burden that increases with the number of repetitions, that could be arbitrarily large.
  	
  	\item[] \textbf{Bootstrap} This cross-validation strategy is similar to the Monte Carlo approach, but it presents a substantial difference. In this strategy, new versions of the dataset are generated by uniform random extraction with replacement of $n'$ samples out of $n$.
  	The extracted data are used as training set, while the left out samples are used for validation.
  	Typically the value of $n'$ is chosen close to $n$. For large $n$, bootstrapped datasets are expected to have $\approx 63.2\%$ of unique samples, while the remaining $36.8\%$ being duplicates.
  	
  	\item[] \textbf{$\bm{K}$-fold cross-validation}
    This strategy splits the data in $K$ non-overlapping subsets that are iteratively kept aside and used to estimate the prediction error of a model trained on the remainder of the data. The final estimation of the prediction error on future data can be done by averaging on the $K$ estimates obtained during the cross-validation procedure. The number of splits $K$ is here limited by the cardinality of the dataset at hand. The major advantage of a $K$-fold with respect to a Monte Carlo cross-validation strategy is its much lighter computational workload. However, as the number of splits $K$ cannot be arbitrarily large, this method is not suitable for estimating the probability distribution of the generalization error.
    When $K=n$ the validation set has only $1$ sample and this strategy degenerates in the so-called \textit{leave-one-out} cross-validation scheme.
  \end{itemize}

  
  \subsection{Model selection strategies} \label{sec:model_selection}
  As anticipated at the beginning of this section, hyperparameters tuning is still considered, by the majority of the ML practitioners, more an \textit{art} than a science.
  
  Nevertheless, different statistically sound techniques to perform hyperparameters optimization were presented over the years . At the time of writing, model selection is a hot-topic in ML research. It often happens that several ML solutions adopted to tackle data science competitions\footnote{ Such as the ones hosted on the Kaggle website: \url{www.kaggle.com}.} are based on the same model. What really makes the difference, and defines the most competitive prediction strategy, is to adopt an efficient and effective model selection strategy.
  
  The model selection procedure applied in the data science challenges described in this PhD thesis are carried out by one of the strategies described below.
  
  \begin{enumerate}
  	\item[] \textbf{Grid-search cross-validation} This is a very straightforward hyperparameter optimization strategy which, for given ML model and cross-validation scheme, takes as input a multi-dimensional grid of possible hyperparameter values and creates a different model for each hyperparameter tuple. Then, for each model, a given performance metric (see Section~\ref{sec:performance_metrics}) is estimated by the adopted cross-validation scheme.
  	The best hyperparameter tuple is chosen as the one achieving top cross-validation prediction score.
  	In other words, this strategy systematically searches for the best hyperparameters across multiple possibilities. 
  	This brute-force algorithm typically returns top performing models, but its is very computational demanding.
  	
 	\item[] \textbf{Randomized-search cross-validation} This can be considered as an alternative to standard grid-search. The main difference is that, instead of trying each possible hyperparameters tuple of a given grid, this strategy generates a fixed number of models each having hyperparameters randomly sampled from given distributions. All these models are then assessed using some cross-validated performance metric (see Section~\ref{sec:performance_metrics}) and the best hyperparameters tuple is chosen as the one corresponding to the top performing model.
 	Compared to standard grid-search, this algorithm may perform slightly worse, but it comes with a way less demanding computational burden.
  \end{enumerate}

  Interestingly, both these hyperparameters optimization strategies involve independently fitting multiple models multiple times. In practice, this embarrassingly parallel problem is in practice often accelerated by parallel computing strategies. This allows to reduce the computational time needed to test several models exploiting the computing capabilities of modern multi-core processors, or multi-node high-performance computing facilities, where available. More details about computational requirements and implementation of modern ML models will be further discussed in Section~\ref{sec:implementation}.


%    \subsection{Feature selection stability}
    % stability selection
    \subsection{Performance metrics} \label{sec:performance_metrics}
    % sup and unsup
    % acc, f1, mcc, ...
    Dealing with real-world datasets, it is very important to have a quantitative and robust way to assess the performance of the applied algorithm. Of course, according to the learning task, different performance metrics should be used. We have already met some of them in the previous section. Here, a comprehensive overview of the most representative performance metrics is provided. To ease readability, the performance metrics are organized according to the corresponding learning task.
    
    \begin{enumerate}
    	\item[] \textbf{Regression}
    	The ideal metric for a regression task is a quantitative measure of how distant the predictions $\bm{\hat  y}$ are with respect to the actual output values $\bm{y}$. The following list shows different ways to measure such distance.
    	\begin{enumerate}[label=(\roman*)]
    		\item \textit{Mean Absolute Error} (\ac{MAE}): this is probably the most intuitive way to assess the regression performance. MAE is very well suited for single task regression and time-series forecasting problems. MAE is scale-dependent, hence not suitable for comparisons across different domains.
    		\begin{equation} \label{eq:metrics_mae}
    			\text{MAE}(\bm{\hat y}, \bm{y}) = \frac{1}{n}\sum_{i=1}^n|\hat y_i - y_i|
    		\end{equation}
    		
    		\item \textit{Mean Squared Error} (\ac{MSE}): widely applied to assess the regression performance, MSE is a second order measure, hence it incorporate bias and variance of the model. MSE of an unbiased estimator corresponds to its variance. MSE is scale-dependent.
    		\begin{equation} \label{eq:metrics_mse}
	   			\text{MSE}(\bm{\hat y}, \bm{y}) = \frac{1}{n}\sum_{i=1}^n(\hat y_i - y_i)^2
    		\end{equation}
    		
    		\item \textit{Root Mean Squared Error} (\ac{RMSE}): this is simply defined as the square root of MSE.
    		\begin{equation} \label{eq:metrics_rmse}
    			\text{RMSE}(\bm{\hat y}, \bm{y}) = \sqrt{\frac{1}{n}\sum_{i=1}^n(\hat y_i - y_i)^2}
    		\end{equation}

    		\item \textit{Residual Sum of Squares} (\ac{RSS}): this data discrepancy measure is mainly used to measure the performance of least squares methods, as it corresponds to the square loss function.
    		\begin{equation} \label{eq:metrics_rss}
    			\text{RSS}(\bm{\hat y}, \bm{y}) = \sum_{i=1}^n(\hat y_i - y_i)^2
    		\end{equation}
    		
    		\item \textit{Coefficient of determination} (\ac{R2}): this metric measures the proportion of the variation of the output that is correctly predicted.
    		This metric is not scale dependent, the best possible score is $\text{R}^2 = 1.0$, while a constant model predicting the average of the output achieves $\text{R}^2 = 0.0$.
    		\ac{R2} is defined as below, where $\bar y$ is the average output, $\bar y = \frac{1}{n}\sum_{j=1}^n y_j$. 
    		\begin{equation} \label{eq:metrics_r2}
	    		\text{R}^2(\bm{\hat y}, \bm{y}) = 1 - \frac{\sum_{i=1}^n(\hat y_i - y_i)^2}{\sum_{i=1}^n (y_i - \bar y)^2}
    		\end{equation}
    		
    		\item \textit{Explained Variance} (\ac{EV}): similar to \ac{R2}, this metric is not scale-dependent and it can be used to compare the outcome of different regression problems.
    		EV is often expressed as percentage, best possible value is $100\%$, lower is worse.
    		EV is defined as below, where $\text{Var}(\bm{y}) = \frac{1}{n} \sum_{i=1}^n (y_i - \bar y)^2$.
    		\begin{equation} \label{eq:metrics_ev}
	    		\text{EV}(\bm{\hat y}, \bm{y}) = 1 - \frac{\text{Var}(\bm{y} - \bm{\hat y})}{\text{Var}(\bm{y})}
    		\end{equation}
    	\end{enumerate}
    	
    	\item[] \textbf{Classification} Compared to regression, assessing the performance of a classifier seems like a much trivial task. It turns out that this is actually not true; assessing the classification performance by different metrics may let you draw very different conclusions about the quality of $\bm{\hat y}$.
    	What follows is a list of most important classification metrics, where the meaning of True Positives (TP), False Positives (FP), True Negatives (TN) and False Negatives (FN) is illustrated in the prototypical confusion matrix (for binary classification) represented in Table~\ref{tab:tptnfpfn}.
    	
    	\begin{table}[]
    		\centering
    		\caption{A prototypical confusion matrix for a binary classification problem. The two classes are encoded as $\pm 1$, the number of true positive examples is $n_+$, whereas the number of true negative examples is $n_-$; hats denote estimated values. The extension for multiclass problems is straightforward, once a positive class is defined.} \label{tab:tptnfpfn}
		\begin{tabular}{c >{\bfseries}r @{\hspace{0.7em}}c @{\hspace{0.4em}}c @{\hspace{0.7em}}l}
			\multirow{10}{*}{\parbox{1.1cm}{\bfseries\raggedleft True\\ value}} & 
			& \multicolumn{2}{c}{\bfseries Predicted value} & \\
			& & $\bm{\hat n_{+1}}$ & $\bm{\hat n_{-1}}$ \\
			& $\bm{n_{+1}}$ & \MyBox{True}{Positive} & \MyBox{False}{Negative}  \\[2.1em]
			& $\bm{n_{-1}}$ & \MyBox{False}{Positive} & \MyBox{True}{Negative} \\
		\end{tabular}
		\end{table}
    	
    	
	    	\begin{enumerate}[label=(\roman*)]
    			\item \textit{Accuracy}: this is the most intuitive classification metric, it simply consists in the fraction of correct predictions. The accuracy measure can be used for binary as well as multiclass classification tasks. The best possible score is $1.0$, while the expected score of a random classifier is the fraction of the most represented class.
    			\begin{equation} \label{eq:metrics_accuracy}
    			\text{Acc}(\bm{\hat y}, \bm{y}) = \frac{1}{n} \sum_{i=1}^n \mathds{1}(\hat y_i = y_i)
    			\end{equation}
    			
    			\item \textit{Balanced accuracy}: for heavily unbalanced binary classification problems this metric is preferable with respect to the standard classification accuracy, as the balanced accuracy of the random classifier is $0.5$ by construction.
    			\begin{equation} \label{eq:metrics_balanced_accuracy}
    			\text{Bacc}(\bm{\hat y}, \bm{y}) = \frac{1}{2} \cdot \bigg[\frac{\text{TP}}{\text{TP}+\text{TN}} + \frac{\text{TN}}{\text{TN}+\text{FP}}\bigg]
    			\end{equation}
    			
    			\item \textit{Precision}, \aka positive predictive value: this is the fraction of TP over the total number of samples classified as positive. Precision is defined for binary classification, but it can be extended to multiclass problems selecting the positive class.
    			\begin{equation} \label{eq:metrics_precision}
    			\text{Prec} (\bm{\hat y}, \bm{y}) = \frac{\text{TP}}{\text{TP} + \text{FP}}
    			\end{equation}
    			
    			\item \textit{Recall}, \aka sensitivity or True Positive Rate (\ac{TPR}): this is the fraction of TP over the total number of actually positive samples. Recall is defined for binary classification, but it can be extended to multiclass problems selecting the positive class.
    			\begin{equation} \label{eq:metrics_recall}
    			\text{Rec}(\bm{\hat y}, \bm{y}) = \frac{\text{TP}}{\text{TP} + \text{FN}}
    			\end{equation}
    			
    			\item \textit{F\textsubscript{$1$}-score}, \aka F-measure: this is the harmonic mean of precision and recall and it can be used to control both of them at the same time.
    			\begin{equation} \label{eq:metrics_f1_score}
    			\text{F}_1(\bm{\hat y}, \bm{y}) = \frac{2 \text{TP}}{2 \text{TP} + \text{FN} + \text{FP}}
    			\end{equation}

    			\item \textit{Matthews correlation coefficient} (\ac{MCC}), \aka $\varphi$-coefficient: MCC values ranges from $-1$ to $+1$, where $\text{MCC} = 1$ is the optimal classification,  $\text{MCC} = 0$ is the score achieved by the random classifier, even in cases of unbalanced problems, and $\text{MCC} = -1$ is the inverse prediction.
    			MCC is defined for binary classification, but it can be extended to multiclass problems selecting the positive class.
    			\begin{equation} \label{eq:metrics_mcc}
    			\text{MCC}(\bm{\hat y}, \bm{y}) = \frac{TP \cdot TN - FP \cdot FN}{\sqrt{(TP+FP) \cdot (TP + FN) \cdot (TN + FP) \cdot (TN + FN)}}
    			\end{equation}
    			
    			\item \textit{Area Under Receiver Operating Curve} (\ac{AUC}): the Receiver Operating characteristic Curve (\ac{ROC}) is a plot used to assess the performance of a binary classifier as its classification threshold is varied. In ROC analysis, the horizontal axis is the False Positive Rate (\ac{FPR}), \aka fall-out, which is defined as
    			\begin{equation} \label{eq:metrics_fpr}
    			\text{FPR}(\bm{\hat y}, \bm{y}) = \frac{\text{FP}}{\text{FP} + \text{TN}}
    			\end{equation}
				while the vertical axis is the recall, defined in Equation~\eqref{eq:metrics_recall}. An example of ROC analysis is reported in Figure~\ref{fig:roc}. The ROC of a random classifier is simply the bottom-left to top-right diagonal Plots in the upper triangular area indicate \textit{good} classifiers. A fundamental metric for classification problems is the AUC. AUC ranges from $0$ to $1$, where optimal classification leads to $\text{AUC} = 1$ and the random classifier achieves $\text{AUC} = 0.5$.
    		\end{enumerate}
    	
        \begin{figure}[!h]
    		\centering
    		\includegraphics[width=0.8\textwidth]{part1/roc.png}
    		\caption{An example of ROC analysis on the MNIST dataset~\cite{lecun2010mnist}. In this example a linear SVM is trained on only $1\%$ of the dataset, where $200$ additional noisy features were added to make the classification problem harder.} \label{fig:roc}
    	\end{figure}

        \item[] \textbf{Clustering} Quantitatively measuring the output of a clustering algorithm is not a trivial task, in particular when no ground truth on the clusters is provided. We report here a list of the most common clustering performance metrics.
        \begin{enumerate}[label=(\roman*)]
        	\item \textit{Silhouette}: the main characteristic of this metric is that it can be used when no ground truth on the clustering is provided~\cite{rousseeuw1987silhouettes}.
        	The silhouette coefficient ranges from $-1$ to $1$ and it is higher for compact and well-separated clusters. Silhouette score around $0$ are associated to partially overlapping clusters.
        	Let $a(\bm{x}_i)$ be the average dissimilarity of $\bm{x}_i$ with all the other points in the same cluster; let also be $b(\bm{x}_i)$ be the average distance of $\bm{x}_i$ to all the points in the nearest cluster. We can interpret $a(\bm{x}_i)$ as a measure of point-to-cluster assignment, where the higher $a(\bm{x}_i)$ is, the more the point $\bm{x}_i$ is correctly assigned. On the other hand, $b(\bm{x}_i)$ can be seen as a clusters separability index.
        	The silhouette coefficient is defined as follows.
        	\begin{equation} \label{eq:metrics_silhouette}
        	s(\bm{x}_i) = \frac{b(\bm{x}_i) - a(\bm{x}_i)}{\max[a(\bm{x_i}), b(\bm{x}_i)]}
        	\end{equation}
        	The quality of the achieved clustering can be graphically assessed visualizing the silhouette coefficient for each sample, as in Figure~\ref{fig:silhouette}\footnote{ Image created with \ade, see Chapter~\ref{chap:adenine}.}, or it can be globally measured by its average score $\bar s = \frac{1}{n}\sum_{i=1}^n s(\bm{x}_i)$.
        	
        	\item \textit{Homogeneity}: a clustering satisfies the homogeneity property if each cluster contains only members of a single class.
        	In order to evaluate the homogeneity, the clustering ground truth must be provided. In this sense, this can be seen as a supervised measure.
        	
        	\item \textit{Completeness}: a clustering satisfies the completeness property if all members of a given class are assigned to the same cluster.
        	As for the homogeneity, this is a supervised measure.
        	
        	\item \textit{V-measure}: this is simply defined as the harmonic mean of completeness and homogeneity.
        	
        	\item \textit{Adjusted Rand Index} (\ac{ARI}): this index is a chance normalized measure of the similarity between two cluster assignments which is robust with respect to permutations of the cluster identifier. This index ranges from $-1$ and $1$, where $-1$ stands for independent cluster assignments and $1$ is perfect match. As it requires the clusters ground truth, this is a supervised metric. Let $\bm{y}$ and $\bm{\hat y}$ be a ground truth and an estimated cluster assignment, respectively, ARI is defined as follows.
        	\begin{equation} \label{eq:metrics_ari}
        	\text{ARI} (\bm{\hat y}, \bm{y}) = \frac{\text{RI} - E[\text{RI}]}{\max(\text{RI}) - E[\text{RI}]}
        	\end{equation}
        	where $\text{RI}$ (the unadjusted Rand Index) is given by
        	$$
        	\text{RI} = \frac{g+h}{\psi}
        	$$
        	where $g$ is the number of pairs of elements that are in the same set both in $\bm{y}$ and $\bm{\hat y}$, $h$ is the number of pairs of elements that are in different sets in both $\bm{y}$ and $\bm{\hat y}$ and $\psi$ is the total number of pairs (without ordering).
        	
        	\item \textit{Adjusted Mutual Information} (\ac{AMI}): similar to ARI, this is an information theory-based metric to measure the similarity between two cluster assignments. Let $Y = \{\bm{y}_1, \dots, \bm{y}_R \}$ and $\hat Y = \{ \bm{\hat y}_1, \dots, \bm{\hat y}_S \}$ be a ground truth and an estimated clustering partitions, respectively, AMI is given by
        	\begin{equation} \label{eq:metrics_ami}
        	\text{AMI} (\hat Y, Y) = \frac{\text{MI} - E[\text{MI}]}{\max[H(\hat Y), H(Y)]- E[\text{MI}]}
        	\end{equation}
        	where $H$ is the entropy and $\text{MI}$ is the Mutual Information, defined as
        	$$
        	\text{MI}(\hat Y, Y) = \sum_{i=1}^{R} \sum_{j=1}^{S} P(i,j) \log\bigg[ \frac{P(i,j)}{P(i)P'(j)} \bigg]
%        	\text{MI}(\bm{\hat y}, \bm{y}) = \sum_{i=1}^{|\bm{\hat y}|} \sum_{j=1}^{|\bm{y}|} \frac{|\hat y_i \cap y_j|}{n} \log\bigg[ \frac{n\cdot|\hat y_i \cap y_j|}{|\hat y_i||y_j|} \bigg]
        	$$
        	where $P(i)$ is the probability that a randomly picked sample falls in $\bm{y}_i$, likewise for $\bm{\hat y}$ and $P'(j)$. While, $P(i,j)$ is the probability that a random sample falls in both clusters $\bm{y}_i,\bm{\hat y}_j$.
        	
        \end{enumerate}
    
    \end{enumerate}

	\begin{figure}[]
		\centering
		\includegraphics[width=0.8\textwidth]{part1/silhouette.png}
		\caption{An example of Silhouette analysis where a $10$ clusters $k$-means runs on a randomly sampled $80\%$ of the MNIST dataset~\cite{lecun2010mnist}.} \label{fig:silhouette}
	\end{figure}


\section{Computational requirements and implementations} \label{sec:implementation}

ML is currently experiencing a renewed wave of interest. Several new technologies, both software and hardware, are specifically designed for ML applications. The number of ML practitioners and researcher is growing fast and the number of problems that are now tackled with ML is rapidly evolving. Therefore, a description of the current ML technological state of the art is completely pointless as it would become outdated in no time.

Nevertheless, describing the philosophy that the most appreciated ML software solutions are currently adopting is more interesting, as it may last longer. The vast majority of ML and data science software libraries presents two stacks: a high level interface, mainly developed for Python, R, Julia or Torch, and a set of highly optimized algorithms, written in C++ or Fortran, that take care of the heavy computational workloads.

In modern ML algorithms, linear algebra operations are the most common and computational demanding tasks to run. In order to carry them out as efficiently as possible ML software developers are currently following two different strategies:
\begin{enumerate*}[label=(\roman*)]
	\item exploiting the computational power offered by multi-core \ac{CPU}s by using optimized \ac{BLAS} libraries (such as OpenBLAS or Intel\textsuperscript{\textregistered}~\ac{MKL}), or
	\item reducing the computational time by offloading the heavy linear algebra operations on \ac{GPU} cards.
\end{enumerate*}

The first solution is inherited from standard scientific computing and it is adopted from the very beginning of ML diffusion. Instead, the second is a relatively recent solution and it is currently a hot-topic in ML software development. This is mainly due to the diffusion of deep learning applications that started to exponentially rise from $2012$, when AlexNet~\cite{krizhevsky2012imagenet}, a deep CNN implemented in {\sc CUDA}, ranked first in the ImageNet large scale visual recognition challenge achieving a dramatically improved top-5 accuracy with respect to the previous editions.
Since then, the interest of ML researcher toward GPU computing is heavily grown.

Another key aspect that characterize ML applications is the high number of independent tasks that need to be executed. As an example, we can think about hyperparameters optimization via grid-search cross-validation (see Section~\ref{sec:model_selection}), where a large number of independent models is trained on the same set of data.
In large-scale ML research, this computationally heavy workload can be distributed on the computing nodes of HPC architectures.
A popular library in ML that exploit this parallel programming paradigm is Dask, a dynamic task scheduling manager developed for large-scale data intensive applications (see Chapter~\ref{chap:adenine}).

All the data science challenge described in the second part of this PhD thesis are tackled with Python-based software solutions. In particular, the numerical computations are carried out by {\sc NumPy}\footnote{ Source: \url{http://www.numpy.org}.} and {\sc SciPy}\footnote{ Source \url{https://www.scipy.org}.}, while the data management is left to {\sc pandas}\footnote{ Source \url{https://pandas.pydata.org}.}. Moreover, for the vast majority of standard ML models (such as generalized linear models or ensemble learning strategies) I took advantage of \sklearn~\cite{scikit-learn}\footnote{ Source \url{http://scikit-learn.org}.}, which is one of the most popular ML libraries available online.
I also relied on custom solutions, such as {\sc L1L2Py}\footnote{ Soruce: \url{https://github.com/slipguru/l1l2py}} and {\sc minimal}\footnote{ Source \url{https://github.com/samuelefiorini/minimal}}, for ML models that are not available via \sklearn API.

Every figure and every experimental result obtained in this thesis can be easily reproduced by using the Python scripts and {\sc jupyter}  notebooks\footnote{ Source: \url{http://jupyter.org}.} available on a dedicated GitHub repository: \url{https://github.com/samuelefiorini/phdthesis}, which also keeps track of the~\LaTeX~source code of the thesis.