% !TEX root = ./main.tex
\appendix

\chapter{Appendix} \label{appendix:A}
As already pointed out at the beginning of Chapter~\ref{chap:state-of-the-art}, ML is a cross-disciplinary field and the statistical tools used in literature to describe models and algorithms heavily depend on the academic background of the author. This can make the approach to ML fascinating and somewhat cumbersome at the same time.

The goal of this appendix is to shed light on some of the statistical tools and definitions that are typically left unsaid or given for granted by most of the authors. In particular, in the following sections insightful statistical details on the formulation of the supervised learning problem expressed in Equation~\eqref{eq:losspen} will be provided.

%The expected value of the function $g(a,b)$, where $(a,b)$ are two continuous random variables with joint probability distribution $f(a,b)$, can be computed with the \textit{Law of the unconscious statistician} (\ac{LOTUS}), reported in Theorem~\ref{th:lotus}.

\section{Useful theorems and definitions}

\begin{theorem}[Law of the unconscious statistician] \label{th:lotus}
	Given two continuous random variables $(a,b) \in A \times B$ with joint probability distribution $p(a,b)$, the expected value of the function $g(a,b)$ can be stated as follows.
	$$\mathbb{E}[g(a,b)]=\iint_{A \times B}g(a,b)~p(a,b)~dadb$$
\end{theorem}

\begin{theorem}[Bayes rule] \label{th:bayes_rule}
	Given $A$ and $B$ two events with probability $P(A)$ and $P(B) \neq 0$, the conditional probability of observing $A$ given that $B$ is true is
	$$P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$$
	where $P(B|A)$ is the probability of observing $B$ given that $A$ is true.
\end{theorem}

\begin{theorem}[Chain rule] \label{th:chain_rule}
	Given $n$ events $A_1, A_2, \dots, A_n$ with probability $P(A_1), P(A_2), \dots, P(A_n)$ the joint probability $P(A_1, A_2, \dots, A_n)$ can be written as follows.
	$$P(A_1, A_2, \dots, A_n) = P(A_1|A_2, \dots, A_n) \cdot P(A_2|A_3, \dots, A_n) \dots P(A_{n-1}|A_{n}) \cdot P(A_n)$$
\end{theorem}

\begin{definition}[Well-posed problem]
	A problem is \textbf{well-posed} if its solution:
	\begin{enumerate*}[label=(\roman*)]
		\item exists,
		\item is unique,
		\item depends continuously on the data (\eg it is stable).
	\end{enumerate*}
\end{definition}

\begin{definition}[Ill-posed problem] \label{def:ill_posed}
	A problem is \textbf{ill-posed} if it is not well-posed.
\end{definition}

\begin{definition}[Likelihood function] \label{def:likelihood}
	Let $\bm{a}$ be a continuous random variable with probability distribution $p(\bm{a}, \bm{\phi})$ depending on the parameter $\bm{\phi}$. Then the function $\mathcal{L}(\bm{\phi}|\bm{\bar a})=p(\bm{\bar a},\bm{\phi})$ is the likelihood function of $\bm{\phi}$ given that $\bm{\bar a}$ is the outcome of $\bm{a}$.
	% Given the observed values $\bm{\bar a} = [\bar a_1, \bar a_2, \dots, \bar a_n]^T$, the likelihood function
	% % $$\mathcal{L}(\bm{\bar a}, \bm{\phi}) = p(\bm{\bar a}, \bm{\phi})$$
	% $p(\bm{\bar a}, \bm{\phi})$
	% and if the random variables $a_i$ (for $i=1,\dots,n$) are \text{\ac{iid}}, then the likelihood function can be simplified as follows.
	% $$\mathcal{L}(\bm{\bar a}, \bm{\phi}) = \prod_{i=1}^n p(\bar a_i, \bm{\phi})$$
\end{definition}


\section{Empirical risk minimization} \label{sec:erm}
In Section~\ref{subsec:supervised_learning} we introduced the concept of supervised learning as the branch of ML in which predictive models are trained on labeled data. The final goal of supervised learning is to find a function of the input variables $f: \mathcal{X} \rightarrow \mathcal{Y}$ that provides a \textit{good} approximation of the output $y$. In order to measure the adherence between predictions $\hat y = f(\bm{x})$ and actual output $y$ we introduced the concept of \textit{loss function} $L(\hat y, y)$, see Table~\ref{tab:losses}. For a fixed choice of the loss, the ideal estimator, also known as the \textit{target} function, $f^*(\bm{x})$ is the minimizer of the (true) expected risk $\mathcal{E}(f)$ in a rather \textit{large} class of functions $\mathcal{F}$.
%We assume that $f^*(\bm{x})$ minimizes $\mathbb{E}(f)$ in a rather \textit{large} class of functions $\mathcal{F}$.


\begin{equation} \label{eq:fstar}
	f^*(\bm{x}) = \min_{f \in \mathcal{F}}{\mathcal{E}(f)}
\end{equation}

Applying the law of the unconscious statistician, stated in Theorem~\ref{th:lotus}, the expected risk $\mathcal{E}(f)$ can be written as in Equation~\eqref{eq:expected_loss}, where $(\bm{x},y)$ are two random variables with joint probability distribution $p(\bm{x},y)$.

\begin{equation} \label{eq:expected_loss}
	\mathcal{E}(f) = \mathbb{E}[L(f(\bm{x}),y))] = \iint_{\mathcal{X} \times \mathcal{Y}}L(f(\bm{x}),y)~p(\bm{x},y)~d\bm{x}dy
%	\mathcal{E}(f) = \mathbb{E}[L(f(\bm{x}),y))] = \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty} L(f(\bm{x}),y) p(\bm{x},y) d\bm{x}dy
\end{equation}

In real situations, a direct computation of $\mathcal{E}(f)$ is unfeasible as the joint probability distribution $p(\bm{x},y)$ is unknown. Although, we assume to be provided with a collection of input-output pairs $\mathcal{D}=\{(\bm{x}_i,y_i)\}_{i=1}^n$ that are supposed to be sampled \ac{iid} from $\mathcal{X} \times \mathcal{Y}$ according to $p(\bm{x}, y)$.
In statistical learning theory, as introduced by Vapnik~\cite{vapnik2013nature}, the dataset $\mathcal{D}$ can be used to build a stochastic approximation of $\mathcal{E}(f)$ called \textit{empirical risk} $\mathcal{E}(f_{\mathcal{D}})$ and defined in Equation~\eqref{eq:empirical_risk}.

\begin{equation} \label{eq:empirical_risk}
	\mathcal{E}(f_{\mathcal{D}}) = \frac{1}{n} \sum_{i=1}^{n} L(f(\bm{x}_i), y_i)
\end{equation}

As $\mathcal{D}$ is drawn according to the probability distribution $p(\bm{x},y)$, our hope is that the empirical risk can be considered as a proxy for the expected risk, hence $\mathcal{E}(f_{\mathcal{D}}) \approx \mathcal{E}(f)$. The solution of the supervised learning problem is then found by \textit{Empirical Risk Minimization} (ERM), defined in Equation~\eqref{eq:erm}.

\begin{equation} \label{eq:erm}
	\hat f(\bm{x}) = \min_{f \in \mathcal{F}}{\mathcal{E}(f_{\mathcal{D}})} = \min_{f \in \mathcal{F}}{\frac{1}{n} \sum_{i=1}^{n} L(f(\bm{x}_i), y_i)}
\end{equation}

In practice, minimizing $\mathcal{E}(f_{\mathcal{D}})$ instead of $\mathcal{E}(f)$ comes at a price. The central problem is whether the first is a good approximation of the second. For instance, when $p(\bm{x}, y)$ is too \textit{complex}, the number of examples is too small and/or the class of functions $\mathcal{F}$ is too large, $\hat f(\bm{x})$ will be far from the target function $f^*(\bm{x})$, even when its empirical error is $0$. In real circumstances, it is impossible to control the true probability distribution and it is often extremely difficult to collect a very large number of examples. The only element we can control is the class of functions $\mathcal{F}$, and in particular its \textit{size}. Since Tikhonov~\cite{tikhonov1963solution} it is known that, for an arbitrary function space $\mathcal{F}$, the ERM problem is ill-posed (see Definition~\ref{def:ill_posed}). A possible way to ensure well-posedness is to impose a constrain that restricts the function space. Hence, the constrained ERM problem assumes the form in Equation~\eqref{eq:constrained_erm}

\begin{equation} \label{eq:constrained_erm}
	\begin{aligned}
		\min_{f \in \mathcal{F}}{\frac{1}{n} \sum_{i=1}^{n} L(f(\bm{x}_i), y_i)} \\
		\text{subject to}~\mathcal{R}(f) < A
	\end{aligned}
\end{equation}

Applying the \textit{Lagrange multipliers technique}\footnote{for a thorough description of this technique, see Appendix E of Bishop's book~\cite{bishop2006pattern}} Equation~\eqref{eq:constrained_erm} can be finally written as Equation~\eqref{eq:losspen2}.

\begin{equation} \label{eq:losspen2}
	\min_{f \in \mathcal{F}}{\frac{1}{n} \sum_{i=1}^{n} L(f(\bm{x}_i), y_i)} + \lambda \mathcal{R}(f)
\end{equation}

The penalty term $\mathcal{R}(f)$ acts as regularizer \footnote{$\mathcal{R}(f)$ in general, can be thought as $\mathcal{R}(f) = \norm{f}^2_K$ where $\norm{\cdot}^2_K$ is the norm defined by the kernel $K$ in a \textit{Reproducing Kernel Hilbert Space} $\mathcal{H}$~\cite{evgeniou2000regularization}}
and, according to its definition, it can ensure well-posedness of the problem and it can enforce different interesting properties on the achieved solution, see Section~\ref{subsec:regularization_methods}. It can also be shown that the use of appropriate regularizes promotes generalization, hence increases our chance to find a $\hat f(\bm{x})$ \textit{close} to $f^*(\bm{x})$.

 According to the choice made for $L(\cdot)$ and $\mathcal{R}(\cdot)$, the posed minimization problem of Equation~\eqref{eq:losspen2} can have very different properties; it can be convex or non-convex, it can include differentiable as well as non-differentiable terms. A rigorous review of the most common optimization methods for ML is beyond the scope of this thesis and can be found here~\cite{boyd2004convex, bach2012optimization, sra2012optimization, nesterov2013introductory}.


\section{Maximum likelihood  estimation} \label{sec:mle}
In this section we will see a different approach to tackle the supervised learning problem.
Once again, let the training data be made of input-output pairs $\mathcal{D} = \{(\bm{x}_i, y_i)\}_{i=1}^n$, with $(\bm{x},y) \in \mathcal{X} \times \mathcal{Y}$. This approach relies on the expression of the uncertainty over the value of $y$ with a probability distribution $p(y|\bm{x},\bm{\theta})$ parameterized by $\bm{\theta} \in \Theta$.
Applying Definition~\ref{def:likelihood}, and assuming that the samples are drawn \ac{iid}, we can write the likelihood function for $\bm{\theta}$ as in Equation~\eqref{eq:likelihood}.

\begin{equation} \label{eq:likelihood}
	\mathcal{L}(y|\bm{x}, \bm{\theta}) = \prod_{i=1}^{n} p(y_i | \bm{x}_i , \bm{\theta})
\end{equation}

Equation~\eqref{eq:likelihood} can be considered as the probability of observing the outputs $y$, given the input $\bm{x}$ and the parameters $\bm{\theta}$.
This statistical setup suggests a way to obtain an estimate for $\bm{\theta}$ which value, in general, is unknown. The criterion in Equation~\eqref{eq:mle} is known as Maximum Likelihood Estimation (MLE).

\begin{equation} \label{eq:mle}
	\bm{\hat \theta}_{MLE} =\argmax_{\theta~\in~\Theta}\mathcal{L}(y_i|\bm{x}_i, \bm{\theta}) = \argmax_{\theta~\in~\Theta} \prod_{i=1}^{n} p(y_i | \bm{x}_i, \bm{\theta})
\end{equation}

Instead of maximizing the likelihood it is often convenient to minimize the \textit{negative log-likelihood}\footnote{approaching information theory or deep learning papers, the log-likelihood is often referred to as \textit{cross-entropy}}. Equation~\eqref{eq:mle} can then be rewritten as Equation~\eqref{eq:nll}.

\begin{equation} \label{eq:nll}
	\bm{\hat \theta}_{MLE} = \argmin_{\theta~\in~\Theta} - \sum_{i=1}^n \log p(y_i | \bm{x}_i, \bm{\theta})
\end{equation}

Moreover, if some prior knowledge on $\bm{\theta}$ is available, it is possible to incorporate it in the form of a prior distribution $p(\bm{\theta})$. Applying the Bayes rule (Theorem~\ref{th:bayes_rule}) it is possible to write the \textit{posterior distribution} $p(\bm{\theta}|y,\bm{x})$ as in Equation~\eqref{eq:posterior}.

\begin{equation} \label{eq:posterior}
	p(\bm{\theta}|y,\bm{x}) = \frac{p(y|\bm{x}, \bm{\theta}) \cdot p(\bm{\theta})}{p(y|\bm{x})}
\end{equation}

The normalizing constant in Equation~\eqref{eq:posterior} $p(y|\bm{x})$ is independent from $\bm{\theta}$, it is known as the marginal likelihood and it can be estimated as in Equation~\eqref{eq:marginal_likelihood}.

\begin{equation} \label{eq:marginal_likelihood}
	p(y|\bm{x}) = \int p(y|\bm{x}, \bm{\theta}) \cdot p(\bm{\theta}) d\bm{\theta}
\end{equation}

This suggest a new way to achieve an estimate for $\bm{\theta}$ that takes into account the prior distribution. The criterion in Equation~\eqref{eq:map} is known as Maximum A Posteriori (MAP).

\begin{equation} \label{eq:map}
	\bm{\hat \theta}_{MAP} = \argmax_{\theta~\in~\Theta} p(\bm{\theta}|y,\bm{x}) = \argmax_{\theta~\in~\Theta} p(y|\bm{x}, \bm{\theta}) \cdot p(\bm{\theta})
\end{equation}

Once again, assuming the independence of the samples and applying the $\log$ trick, Equation~\eqref{eq:map} can be rewritten as Equation~\eqref{eq:map2}.

\begin{equation} \label{eq:map2}
	\bm{\hat \theta}_{MAP} = \argmin_{\theta~\in~\Theta} - \bigg[\sum_{i=1}^n \log p(y_i|\bm{x}_i,\bm{\theta})  + \log p(\theta)\bigg]
\end{equation}

\todo{check again this section}


% As in the case of ERM, the MLE estimate may easily overfit the data as no regularization strategy is adopted. The standard way to impose some kind of regularization is to assume a prior on the parametric model $p(f_\theta)$. Given a prior and a likelihood, we can apply the Bayes rule (see Theorem~\ref{th:bayes_rule}) to define the so-called posterior distribution $p(f_\theta | y, \bm{x})$ as in Equation~\eqref{eq:posterior}\footnote{the dependency from $\phi$ is omitted for ease of notation}.
%
% \begin{equation} \label{eq:posterior}
% 	p(f_\theta | y, \bm{x}) = \frac{\mathcal{L}(y|\bm{x}; \theta) \cdot p(f_\theta)}{p(y|\bm{x})}
% \end{equation}
%
% The normalization constant is the marginal likelihood and, being independent from the parameters $\theta$, can be ignored. For the sake of achieving a predictive model, the posterior function can be thought as the product between likelihood and prior. We can finally rewrite our optimization problem as in Equation~\eqref{eq:map}.
%
% \begin{equation} \label{eq:map}
% 	[\hat \phi, \hat \theta] = \argmax_{\phi, \theta~\in~\Phi \times \Theta}\mathcal{L}(y_i|\bm{x}_i; \theta, \phi) \cdot p(f_\theta) =
% %	\argmax_{\phi, \theta~\in~\Phi \times \Theta} \prod_{i=1}^{n} p_\phi(f_\theta(\bm{x}_i)) \cdot p(f_\theta) =
% 	\argmin_{\phi, \theta~\in~\Phi \times \Theta} -\bigg[\sum_{i=1}^n \log p_\phi(f_\theta(\bm{x}_i))  + \log p(f_\theta)\bigg]
% \end{equation}
% Equation~\eqref{eq:map} is known as Maximum A Posteriori (MAP) estimate.

%and it is independent from the parameters $\theta$ as it is given by Equation~\eqref{eq:marginal_likelihood}.
%\begin{equation} \label{eq:marginal_likelihood}
%	p(y|\bm{x}) = \int \mathcal{L}(y|\bm{x}; \theta) \cdot p(f_\theta)~d\theta
%\end{equation}



%\section{Maximum a posteriori} \label{sec:map}

\section{ERM vs MLE/MAP} \label{sec:erm-mlemap_connection}
bla bla bla
